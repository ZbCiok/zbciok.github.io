<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-streams/kafka-streams/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">Kafka Streams | Reactive Programming</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://zbciok.github.io/docs/kafka-streams"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Kafka Streams | Reactive Programming"><meta data-rh="true" name="description" content="Kafka Streams"><meta data-rh="true" property="og:description" content="Kafka Streams"><link data-rh="true" rel="icon" href="/img/favicon-black.ico"><link data-rh="true" rel="canonical" href="https://zbciok.github.io/docs/kafka-streams"><link data-rh="true" rel="alternate" href="https://zbciok.github.io/docs/kafka-streams" hreflang="en"><link data-rh="true" rel="alternate" href="https://zbciok.github.io/docs/kafka-streams" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Reactive Programming RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Reactive Programming Atom Feed"><link rel="stylesheet" href="/assets/css/styles.7393f987.css">
<script src="/assets/js/runtime~main.427df3e4.js" defer="defer"></script>
<script src="/assets/js/main.22179b09.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon-black.ico" alt="RP" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon-black.ico" alt="RP" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Home</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://jreact.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">jreact.com<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/docs/streams">Streams</a><button aria-label="Collapse sidebar category &#x27;Streams&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/java-8-streams">Java 8 Streams</a><button aria-label="Expand sidebar category &#x27;Java 8 Streams&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible menu__list-item-collapsible--active"><a class="menu__link menu__link--sublist menu__link--active" aria-current="page" tabindex="0" href="/docs/kafka-streams">Kafka Streams</a><button aria-label="Collapse sidebar category &#x27;Kafka Streams&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/streams/kafka-streams/glossary">Glossary</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/kafka-streams/examples">Examples</a><button aria-label="Expand sidebar category &#x27;Examples&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/streams/kafka-streams/references">References</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/ksqlDB">ksqlDB</a><button aria-label="Expand sidebar category &#x27;ksqlDB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/apache-flink">Apache Flink</a><button aria-label="Expand sidebar category &#x27;Apache Flink&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/apache-spark">Apache Spark</a><button aria-label="Expand sidebar category &#x27;Apache Spark&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/reactive-streams">Reactive Streams</a><button aria-label="Expand sidebar category &#x27;Reactive Streams&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/project-reactor">Project Reactor</a><button aria-label="Expand sidebar category &#x27;Project Reactor&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/smallrye-mutiny">SmallRye Mutiny</a><button aria-label="Expand sidebar category &#x27;SmallRye Mutiny&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/vertx">VERT.X</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/aws">AWS</a><button aria-label="Expand sidebar category &#x27;AWS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/quarkus">Quarkus</a><button aria-label="Expand sidebar category &#x27;Quarkus&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/spring">Spring</a><button aria-label="Expand sidebar category &#x27;Spring&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/messaging-1">Messaging</a><button aria-label="Expand sidebar category &#x27;Messaging&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/auxiliary-software">Auxiliary</a><button aria-label="Expand sidebar category &#x27;Auxiliary&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/streams"><span itemprop="name">Streams</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Kafka Streams</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Kafka Streams</h1></header>
<p><a href="https://kafka.apache.org/0102/documentation/streams/" target="_blank" rel="noopener noreferrer"><em><strong>Kafka Streams</strong></em></a><br>
<a href="https://kafka.apache.org/20/documentation/streams/developer-guide/" target="_blank" rel="noopener noreferrer"><em><strong>Developer Guide for Kafka Streams</strong></em></a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="-for-confluent-platform"><em><strong>... for Confluent Platform</strong></em><a href="#-for-confluent-platform" class="hash-link" aria-label="Direct link to -for-confluent-platform" title="Direct link to -for-confluent-platform">​</a></h2>
<p><a href="https://docs.confluent.io/platform/current/streams/concepts.html" target="_blank" rel="noopener noreferrer"><em><strong>Kafka Streams Basics for Confluent</strong></em></a></p>
<p>In this section we summarize the key concepts of Kafka Streams. For more detailed information refer to <a href="https://docs.confluent.io/platform/current/streams/architecture.html#streams-architecture" target="_blank" rel="noopener noreferrer"><em><strong>Kafka Streams Architecture for Confluent Platform</strong></em></a> and the Kafka Streams Developer Guide for Confluent Platform. You may also be interested in the <a href="https://developer.confluent.io/learn-kafka/kafka-streams/?session_ref=https://www.google.com/&amp;_gl=1*12nf27f*_gcl_au*MTYyNDM0OTAxNS4xNzMwMTIwNTQ0LjE5NDczMDA3NDguMTczMzg2MTk0NS4xNzMzODYxOTQ1*_ga*NTE5ODY0NDI3LjE3MzAxMjA1NDQ.*_ga_D2D3EGKSGD*MTczNDQzNDgzNy42MC4xLjE3MzQ0NDAzMjMuNTguMC4w&amp;_ga=2.223317534.484256090.1734434838-519864427.1730120544" target="_blank" rel="noopener noreferrer"><em><strong>Kafka Streams 101</strong></em></a> course.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="stream">Stream<a href="#stream" class="hash-link" aria-label="Direct link to Stream" title="Direct link to Stream">​</a></h2>
<p>A <strong>stream</strong> is the most important abstraction provided by Kafka Streams: it represents an unbounded, continuously updating data set, where unbounded means “of unknown or of unlimited size”. Just like a topic in Kafka, a stream in the Kafka Streams API consists of one or more stream partitions.</p>
<p>A <strong>stream partition</strong> is an, ordered, replayable, and fault-tolerant sequence of immutable data records, where a data record is defined as a key-value pair.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="stream-processing-application">Stream Processing Application<a href="#stream-processing-application" class="hash-link" aria-label="Direct link to Stream Processing Application" title="Direct link to Stream Processing Application">​</a></h2>
<p>A stream processing application is any program that makes use of the Kafka Streams library. In practice, this means it is probably “your” application. It may define its computational logic through one or more processor topologies.</p>
<p>Your stream processing application doesn’t run inside a broker. Instead, it runs in a separate JVM instance, or in a separate cluster entirely.</p>
<p>An <strong>application instance</strong> is any running instance or “copy” of your application. Application instances are the primary means to elasticly scale and parallelize your application, and they also contribute to making it fault-tolerant. For example, you may need the power of ten machines to handle the incoming data load of your application; here, you could opt to run ten instances of your application, one on each machine, and these instances would automatically collaborate on the data processing – even as new instances/machines are added or existing ones removed during live operation.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="processor-topology">Processor Topology<a href="#processor-topology" class="hash-link" aria-label="Direct link to Processor Topology" title="Direct link to Processor Topology">​</a></h2>
<p>A processor topology or simply topology defines the computational logic of the data processing that needs to be performed by a stream processing application. A topology is a graph of stream processors (nodes) that are connected by streams (edges). Developers can define topologies either via the low-level Processor API or via the Kafka Streams DSL, which builds on top of the former.</p>
<img src="/img/streams/kafka-streams/kafka-streams-processor-topology-01.png" alt="kafka-streams-processor-topology-01.png">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="stream-processor">Stream Processor<a href="#stream-processor" class="hash-link" aria-label="Direct link to Stream Processor" title="Direct link to Stream Processor">​</a></h2>
<p>A stream processor is a node in the processor topology as shown in the diagram of section Processor Topology. It represents a processing step in a topology, i.e. it is used to transform data. Standard operations such as map or filter, joins, and aggregations are examples of stream processors that are available in Kafka Streams out of the box. A stream processor receives one input record at a time from its upstream processors in the topology, applies its operation to it, and may subsequently produce one or more output records to its downstream processors.</p>
<p>Kafka Streams provides two APIs to define stream processors:</p>
<ul>
<li>The <a href="https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl" target="_blank" rel="noopener noreferrer"><em><strong>declarative, functional DSL</strong></em></a> is the recommended API for most users – and notably for starters – because most data processing use cases can be expressed in just a few lines of DSL code. Here, you typically use built-in operations such as map and filter.</li>
<li>The <a href="https://docs.confluent.io/platform/current/streams/developer-guide/processor-api.html#streams-developer-guide-processor-api" target="_blank" rel="noopener noreferrer"><em><strong>imperative, lower-level Processor API</strong></em></a> provides you with even more flexibility than the DSL but at the expense of requiring more manual coding work. Here, you can define and connect custom processors as well as directly interact with <a href="https://docs.confluent.io/platform/current/streams/architecture.html#streams-architecture-state" target="_blank" rel="noopener noreferrer"><em><strong>state stores</strong></em></a>.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="stateful-stream-processing">Stateful Stream Processing<a href="#stateful-stream-processing" class="hash-link" aria-label="Direct link to Stateful Stream Processing" title="Direct link to Stateful Stream Processing">​</a></h2>
<p>Some stream processing applications don’t require state – they are stateless – which means the processing of a message is independent from the processing of other messages. Examples are when you only need to transform one message at a time, or filter out messages based on some condition.</p>
<p>In practice, however, most applications require state – they are stateful – in order to work correctly, and this state must be managed in a fault-tolerant manner. Your application is stateful whenever, for example, it needs to join, aggregate, or window its input data. Kafka Streams provides your application with powerful, elastic, highly scalable, and fault-tolerant stateful processing capabilities.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="duality-of-streams-and-tables">Duality of Streams and Tables<a href="#duality-of-streams-and-tables" class="hash-link" aria-label="Direct link to Duality of Streams and Tables" title="Direct link to Duality of Streams and Tables">​</a></h2>
<p>When implementing stream processing use cases in practice, you typically need both streams and also databases. An example use case that is very common in practice is an e-commerce application that enriches an incoming stream of customer transactions with the latest customer information from a database table. In other words, streams are everywhere, but databases are everywhere, too.</p>
<p>Any stream processing technology must therefore provide first-class support for streams and tables. Kafka’s Streams API provides such functionality through its core abstractions for streams and tables, which we will talk about in a minute. Now, an interesting observation is that there is actually a close relationship between streams and tables, the so-called stream-table duality. And Kafka exploits this duality in many ways: for example, to make your applications elastic, to support fault-tolerant stateful processing, or to run Kafka Streams Interactive Queries for Confluent Platform against your application’s latest processing results. And, beyond its internal usage, the Kafka Streams API also allows developers to exploit this duality in their own applications.</p>
<p>Before we discuss concepts such as aggregations in Kafka Streams we must first introduce tables in more detail, and talk about the aforementioned stream-table duality. Essentially, this duality means that a stream can be viewed as a table, and a table can be viewed as a stream.</p>
<p>The following explanations are kept simple intentionally and skip the discussion of compound keys, multisets, and so on.</p>
<p>A simple form of a table is a collection of key-value pairs, also called a map or associative array. Such a table may look as follows:</p>
<img src="/img/streams/kafka-streams/kafka-streams-a-table-01.png" alt="kafka-streams-a-table-01.png">
<p>The stream-table duality describes the close relationship between streams and tables.</p>
<ul>
<li><strong>Stream as Table:</strong> A stream can be considered a changelog of a table, where each data record in the stream captures a state change of the table. A stream is thus a table in disguise, and it can be easily turned into a “real” table by replaying the changelog from beginning to end to reconstruct the table. Similarly, aggregating data records in a stream will return a table. For example, we could compute the total number of pageviews by user from an input stream of pageview events, and the result would be a table, with the table key being the user and the value being the corresponding pageview count.</li>
<li><strong>Table as Stream:</strong> A table can be considered a snapshot, at a point in time, of the latest value for each key in a stream (a stream’s data records are key-value pairs). A table is thus a stream in disguise, and it can be easily turned into a “real” stream by iterating over each key-value entry in the table.</li>
</ul>
<p>Let’s illustrate this with an example. Imagine a table that tracks the total number of pageviews by user (first column of diagram below). Over time, whenever a new pageview event is processed, the state of the table is updated accordingly. Here, the state changes between different points in time – and different revisions of the table – can be represented as a changelog stream (second column).</p>
<img src="/img/streams/kafka-streams/kafka-streams-talbe-stream-01.png" alt="kafka-streams-talbe-stream-01.png">
<p>Because of the stream-table duality, the same stream can be used to reconstruct the original table (third column):</p>
<img src="/img/streams/kafka-streams/kafka-streams-streams-table-01.png" alt="kafka-streams-streams-table-01.png">
<p>The same mechanism is used, for example, to replicate databases via change data capture (CDC) and, within Kafka Streams, to replicate its so-called state stores across machines for fault tolerance. The stream-table duality is such an important concept for stream processing applications in practice that Kafka Streams models it explicitly via the KStream and KTable abstractions, which we describe in the next sections.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="kstream">KStream<a href="#kstream" class="hash-link" aria-label="Direct link to KStream" title="Direct link to KStream">​</a></h2>
<p>A <strong>KStream</strong> is an abstraction of a record stream, where each data record represents a self-contained datum in the unbounded data set. Using the table analogy, data records in a record stream are always interpreted as an “INSERT” – think: adding more entries to an append-only ledger – because no record replaces an existing row with the same key. Examples are a credit card transaction, a page view event, or a server log entry.</p>
<p>Only the Kafka Streams DSL has the notion of a KStream.</p>
<p>To illustrate, imagine the following two data records are being sent to the stream:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">(&quot;alice&quot;, 1) --&gt; (&quot;alice&quot;, 3)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If your stream processing application were to sum the values per user, it would return 4 for alice. Why? Because the second data record would not be considered an update of the previous record. Compare this behavior of KStream to KTable below, which would return 3 for alice.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ktable">KTable<a href="#ktable" class="hash-link" aria-label="Direct link to KTable" title="Direct link to KTable">​</a></h2>
<p>A <strong>KTable</strong> is an abstraction of a changelog stream, where each data record represents an update. More precisely, the value in a data record is interpreted as an “UPDATE” of the last value for the same record key, if any (if a corresponding key doesn’t exist yet, the update will be considered an INSERT). Using the table analogy, a data record in a changelog stream is interpreted as an UPSERT aka INSERT/UPDATE because any existing row with the same key is overwritten. Also, null values are interpreted in a special way: a record with a null value represents a “DELETE” or tombstone for the record’s key.</p>
<p>Only the Kafka Streams DSL has the notion of a KTable.</p>
<p>To illustrate, let’s imagine the following two data records are being sent to the stream:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">(&quot;alice&quot;, 1) --&gt; (&quot;alice&quot;, 3)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>If your stream processing application were to sum the values per user, it would return 3 for alice. Why? Because the second data record would be considered an update of the previous record. Compare this behavior of KTable with the illustration for KStream above, which would return 4 for alice.</p>
<p>You have already seen an example of a changelog stream in the section Duality of Streams and Tables. Another example are change data capture (CDC) records in the changelog of a relational database, representing which row in a database table was inserted, updated, or deleted.</p>
<p>KTable also provides an ability to look up current values of data records by keys. This table-lookup functionality is available through join operations (see also Joining in the Developer Guide) as well as through Interactive Queries.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="effect-of-kafka-log-compaction">Effect of Kafka log compaction<a href="#effect-of-kafka-log-compaction" class="hash-link" aria-label="Direct link to Effect of Kafka log compaction" title="Direct link to Effect of Kafka log compaction">​</a></h2>
<p>Another way of thinking about KStream and KTable is as follows: If you were to store a KTable into a Kafka topic, you’d probably want to enable Kafka’s log compaction feature to save storage space.</p>
<p>But it wouldn’t be safe to enable log compaction in the case of a KStream, because as soon as <a href="https://docs.confluent.io/kafka/design/log_compaction.html" target="_blank" rel="noopener noreferrer"><em><strong>log compaction</strong></em></a> begins purging older data records of the same key, it would break the semantics of the data. To pick up the illustration example again, you’d suddenly get a 3 for alice, instead of a 4, because log compaction would have removed the (&quot;alice&quot;, 1) data record. this means that log compaction is safe for a KTable (changelog stream) but it is a mistake for a KStream (record stream).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="globalktable">GlobalKTable<a href="#globalktable" class="hash-link" aria-label="Direct link to GlobalKTable" title="Direct link to GlobalKTable">​</a></h2>
<p>Like a KTable, a GlobalKTable is an abstraction of a changelog stream, where each data record represents an update.</p>
<p>Only the Kafka Streams DSL has the notion of a GlobalKTable.</p>
<p>A GlobalKTable differs from a KTable in the data that they are being populated with, i.e. which data from the underlying Kafka topic is being read into the respective table. Slightly simplified, imagine you have an input topic with 5 partitions. In your application, you want to read this topic into a table. Also, you want to run your application across 5 application instances for maximum parallelism.</p>
<ul>
<li>If you read the input topic into a <strong>KTable</strong>, then the “local” KTable instance of each application instance will be populated with data <strong>from only 1 partition</strong> of the topic’s 5 partitions.</li>
<li>If you read the input topic into a <strong>GlobalKTable</strong>, then the local GlobalKTable instance of each application instance will be populated with data <strong>from all partitions of the topic</strong>.</li>
</ul>
<p>GlobalKTable provides the ability to look up current values of data records by keys. This table-lookup functionality is available through join operations (as described in Joining in the Developer Guide) and Kafka Streams Interactive Queries for Confluent Platform.</p>
<p>Benefits of global tables:</p>
<ul>
<li>You can use global tables to “broadcast” information to all running instances of your application.</li>
<li>Global tables enable more convenient and efficient joins.<!-- -->
<ul>
<li>Global tables enable star joins.</li>
<li>Global tables are more efficient when chaining multiple joins.</li>
<li>When joining against a global table, the input data doesn’t need to be co-partitioned.</li>
<li>Global tables support “foreign-key” lookups, which means that you can look up data in the table not just by record key, but also by data in the record values. In this case, the join always uses the table’s primary key, and the “foreign key” refers to the stream records. Unlike a stream-table join that always calculates the join based on the stream-record key, a stream-globalKTable join enables you to extract the join key directly from the stream record’s value.</li>
</ul>
</li>
</ul>
<p>Drawbacks of global tables include:</p>
<ul>
<li>Increased local storage consumption compared to the (partitioned) KTable, because the entire topic is tracked.</li>
<li>Increased network and Kafka broker load compared to the (partitioned) KTable, because the entire topic is read.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="time">Time<a href="#time" class="hash-link" aria-label="Direct link to Time" title="Direct link to Time">​</a></h2>
<p>A critical aspect in stream processing is the notion of time, and how it is modeled and integrated. For example, some operations such as Windowing are defined based on time boundaries.</p>
<p>Kafka Streams supports the following notions of time:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="event-time">event-time<a href="#event-time" class="hash-link" aria-label="Direct link to event-time" title="Direct link to event-time">​</a></h3>
<p>The point in time when an event or data record occurred (that is, was originally created by the source). Achieving event-time semantics typically requires embedding timestamps in the data records at the time a data record is being produced.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="processing-time">processing-time<a href="#processing-time" class="hash-link" aria-label="Direct link to processing-time" title="Direct link to processing-time">​</a></h3>
<p>The point in time when the event or data record happens to be processed by the stream processing application (that is, when the record is being consumed). The processing-time may be milliseconds, hours, days, etc. later than the original event-time.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ingestion-time">ingestion-time<a href="#ingestion-time" class="hash-link" aria-label="Direct link to ingestion-time" title="Direct link to ingestion-time">​</a></h3>
<p>The point in time when an event or data record is stored in a topic partition by a Kafka broker. Ingestion-time is similar to event-time, as a timestamp gets embedded in the data record itself. The difference is that the timestamp is generated when the record is appended to the target topic by the Kafka broker, not when the record is created at the source. Ingestion-time may approximate event-time reasonably well if we assume that the time difference between creation of the record and its ingestion into Kafka is sufficiently small, where “sufficiently” depends on the specific use case. Thus, ingestion-time may be a reasonable alternative for use cases where event-time semantics are not possible, perhaps because the data producers don’t embed timestamps (such as with older versions of Kafka’s Java producer client) or the producer cannot assign timestamps directly (for example, does not have access to a local clock).</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="stream-time">stream-time<a href="#stream-time" class="hash-link" aria-label="Direct link to stream-time" title="Direct link to stream-time">​</a></h3>
<p>The maximum timestamp seen over all processed records so far. Kafka Streams tracks stream-time on a per-task basis.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="timestamps">Timestamps<a href="#timestamps" class="hash-link" aria-label="Direct link to Timestamps" title="Direct link to Timestamps">​</a></h3>
<p>Kafka Streams assigns a <strong>timestamp</strong> to every data record via so-called timestamp extractors. These per-record timestamps describe the progress of a stream with regards to time (although records may be out-of-order within the stream) and are leveraged by time-dependent operations such as joins. We call it the <strong>event-time</strong> of the application to differentiate with the wall-clock-time when this application is actually executing. Event-time is also used to synchronize multiple input streams within the same application.</p>
<p>Concrete implementations of timestamp extractors may retrieve or compute timestamps based on the actual contents of data records such as an embedded timestamp field to provide event-time or ingestion-time semantics, or use any other approach such as returning the current wall-clock time at the time of processing, thereby yielding processing-time semantics to stream processing applications. Developers can thus enforce different notions/semantics of time depending on their business needs.</p>
<p>Finally, whenever a Kafka Streams application writes records to Kafka, then it will also assign timestamps to these new records. The way the timestamps are assigned depends on the context:</p>
<ul>
<li>When new output records are generated via directly processing some input record, output record timestamps are inherited from input record timestamps directly.</li>
<li>When new output records are generated via periodic functions, the output record timestamp is defined as the current internal time of the stream task.</li>
<li>For aggregations, the timestamp of the resulting update record will be that of the latest input record that triggered the update.</li>
</ul>
<p>For aggregations and joins, timestamps are computed using the following rules.</p>
<ul>
<li>For joins (stream-stream, table-table) that have left and right input records, the timestamp of the output record is assigned max(left.ts, right.ts).</li>
<li>For stream-table joins, the output record is assigned the timestamp from the stream record.</li>
<li>For aggregations, Kafka Streams also computes the max timestamp across all records, per key, either globally (for non-windowed) or per-window.</li>
<li>Stateless operations are assigned the timestamp of the input record. For flatMap and siblings that emit multiple records, all output records inherit the timestamp from the corresponding input record.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="assign-timestamps-to-output-records-with-the-processor-api">Assign timestamps to output records with the Processor API<a href="#assign-timestamps-to-output-records-with-the-processor-api" class="hash-link" aria-label="Direct link to Assign timestamps to output records with the Processor API" title="Direct link to Assign timestamps to output records with the Processor API">​</a></h4>
<p>You can change the default behavior in the Processor API by assigning timestamps to output records explicitly when calling #forward().</p>
<p>The forward() method takes two parameters: a key-value pair and a timestamp. The optional timestamp parameter can be used to set the timestamp of the output record explicitly.</p>
<p>The following example shows the explicit assignment of timestamps to output records using the forward() method.</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">public class MyProcessor implements Processor&lt;String, String&gt; {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  private ProcessorContext context;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  @Override</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  public void init(ProcessorContext context) {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    this.context = context;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  @Override</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  public void process(String key, String value) {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    // Extract the timestamp from the input record.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    long inputTimestamp = context.timestamp();</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    // Process the input record.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    String outputValue = processRecord(value);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    // Assign the timestamp to the output record explicitly.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    // You implement the computeOutputTimestamp method for your use case.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    long outputTimestamp = computeOutputTimestamp(inputTimestamp);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    KeyValue&lt;String, String&gt; outputRecord = KeyValue.pair(key, outputValue);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    context.forward(outputRecord, outputTimestamp);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  @Override</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  public void close() {}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In this example, the timestamp is extracted from the input record by using the context.timestamp() method. The computeOutputTimestamp() custom method, which you implement, computes the timestamp for the output record. Finally, a new key-value pair is created for the output record by using KeyValue.pair() and calling context.forward() with this pair and the computed timestamp.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="assign-timestamps-to-output-records-with-the-kafka-streams-api">Assign timestamps to output records with the Kafka Streams API<a href="#assign-timestamps-to-output-records-with-the-kafka-streams-api" class="hash-link" aria-label="Direct link to Assign timestamps to output records with the Kafka Streams API" title="Direct link to Assign timestamps to output records with the Kafka Streams API">​</a></h4>
<p>You can assign timestamps to output records explicitly in Kafka Streams by using the TimestampExtractor interface. Implement this interface to extract a timestamp from each record and use it for processing-time or event-time semantics.</p>
<p>The following example shows the explicit assignment of timestamps to output records using the TimestampExtractor interface.</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">public class CustomTimestampExtractor implements TimestampExtractor {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    @Override</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    public long extract(ConsumerRecord&lt;Object, Object&gt; record, long previousTimestamp) {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        // Extract timestamp from record</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        long timestamp = ...;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return timestamp;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Use the custom timestamp extractor</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">KStream&lt;String, String&gt; stream = builder.stream(&quot;input-topic&quot;, Consumed.with(Serdes.String(), Serdes.String())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        .withTimestampExtractor(new CustomTimestampExtractor()));</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Process records with timestamps</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">stream.map((key, value) -&gt; new KeyValue&lt;&gt;(key, value.toUpperCase()))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        .to(&quot;output-topic&quot;, Produced.with(Serdes.String(), Serdes.String()));</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In this example, a custom TimestampExtractor extracts a timestamp from each record and returns it as a long value. The custom extractor is used when creating a KStream by calling the withTimestampExtractor() method on the Consumed object.</p>
<p>Once you have a stream with timestamps, you can process records with processing-time or event-time semantics by using methods like windowedBy() or groupByKey().</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="aggregations">Aggregations<a href="#aggregations" class="hash-link" aria-label="Direct link to Aggregations" title="Direct link to Aggregations">​</a></h2>
<p>An <strong>aggregation</strong> operation takes one input stream or table, and yields a new table by combining multiple input records into a single output record. Examples of aggregations are computing counts or sum.</p>
<p>In the <a href="https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl" target="_blank" rel="noopener noreferrer"><em><strong>Kafka Streams DSL</strong></em></a>, an input stream of an <strong>aggregation</strong> operation can be a KStream or a KTable, but the output stream will always be a KTable. This allows Kafka Streams to update an aggregate value upon the out-of-order arrival of further records after the value was produced and emitted. When such out-of-order arrival happens, the aggregating KStream or KTable emits a new aggregate value. Because the output is a KTable, the new value is considered to overwrite the old value with the same key in subsequent processing steps. For more information on out-of-order records, see <a href="https://docs.confluent.io/platform/current/streams/concepts.html#streams-concepts-out-out-order-handling" target="_blank" rel="noopener noreferrer"><em><strong>Out-of-Order Handling</strong></em></a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="joins">Joins<a href="#joins" class="hash-link" aria-label="Direct link to Joins" title="Direct link to Joins">​</a></h2>
<p>A <strong>join</strong> operation merges two input streams and/or tables based on the keys of their data records, and yields a new stream/table.</p>
<p>The <a href="https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-joins" target="_blank" rel="noopener noreferrer"><em><strong>join operations</strong></em></a> available in the <a href="https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl" target="_blank" rel="noopener noreferrer"><em><strong>Kafka Streams DSL</strong></em></a> differ based on which kinds of streams and tables are being joined; for example, KStream-KStream joins versus KStream-KTable joins.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="windowing">Windowing<a href="#windowing" class="hash-link" aria-label="Direct link to Windowing" title="Direct link to Windowing">​</a></h2>
<p>Windowing lets you control how to group records that have the same key for stateful operations such as aggregations or joins into so-called windows. Windows are tracked per record key.</p>
<p><a href="https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-windowing" target="_blank" rel="noopener noreferrer"><em><strong>Windowing operations</strong></em></a> are available in the Kafka Streams DSL. When working with windows, you can specify a <strong>grace period</strong> for the window that indicates when window results are final. This grace period controls how long Kafka Streams will wait for out-of-order data records for a window. If a record arrives after the grace period of a window has passed (i.e., record.ts &gt; window-end-time + grace-period), the record is discarded and will not be processed in that window.</p>
<p>Out-of-order records are always possible in the real world and your applications must account for them properly. The system’s time semantics determine how <strong>out-of-order</strong> records are handled. For processing-time, the semantics are “when the record is being processed”, which means that the notion of <strong>out-of-order</strong> records is not applicable. Similarly, for ingestion-time, the broker assigns timestamps in ascending order based on topic append order; the timestamp indicates ingestion-time only. Out-of-order records can only be considered for event-time semantics, where timestamps are set by producers specifically to indicate event-time. If two producers write to the same topic partition, there is no guarantee on the event append order.</p>
<p>Kafka Streams is able to properly handle out-of-order records for the relevant time semantics (event-time).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="interactive-queries">Interactive Queries<a href="#interactive-queries" class="hash-link" aria-label="Direct link to Interactive Queries" title="Direct link to Interactive Queries">​</a></h2>
<p>Interactive Queries allow you to treat the stream processing layer as a lightweight embedded database, and to directly query the latest state of your stream processing application. You can do this without having to first materialize that state to external databases or external storage.</p>
<p>Interactive Queries simplify the architecture and lead to more application-centric architectures.</p>
<p>The following diagram juxtapose two architectures: the first does not use Interactive Queries whereas the second architecture does. It depends on the concrete use case to determine which of these architectures is a better fit – the important takeaway is that Kafka Streams and Interactive Queries give you the flexibility to pick and to compose the right one, rather than limiting you to just a single way.</p>
<p>Here are some use case examples for applications that benefit from Interactive Queries:</p>
<ul>
<li>Real-time monitoring: A front-end dashboard that provides threat intelligence (e.g., web servers currently under attack by cyber criminals) can directly query a Kafka Streams application that continuously generates the relevant information by processing network telemetry data in real-time.</li>
<li>Video gaming: A Kafka Streams application continuously tracks location updates from players in the gaming universe. A mobile companion app can then directly query the Kafka Streams application to show the current location of a player to friends and family, and invite them to come along. Similarly, the game vendor can use the data to identify unusual hotspots of players, which may indicate a bug or an operational issue.</li>
<li>Risk and fraud: A Kafka Streams application continuously analyzes user transactions for anomalies and suspicious behavior. An online banking application can directly query the Kafka Streams application when a user logs in to deny access to those users that have been flagged as suspicious.</li>
<li>Trend detection: A Kafka Streams application continuously computes the latest top charts across music genres based on user listening behavior that is collected in real-time. Mobile or desktop applications of a music store can then interactively query for the latest charts while users are browsing the store.</li>
</ul>
<p>For more information, see the <a href="https://docs.confluent.io/platform/current/streams/developer-guide/interactive-queries.html#streams-developer-guide-interactive-queries" target="_blank" rel="noopener noreferrer"><em><strong>Developer Guide</strong></em></a>.</p>
<hr>
<br>
<br>
<!-- -->
<section class="row"><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/streams/kafka-streams/glossary"><h2 class="text--truncate cardTitle_rnsV" title="Glossary">📄️<!-- --> <!-- -->Glossary</h2><p class="text--truncate cardDescription_PWke" title="SerDes (Data Types and Serialization)">SerDes (Data Types and Serialization)</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/kafka-streams/examples"><h2 class="text--truncate cardTitle_rnsV" title="Examples">🗃️<!-- --> <!-- -->Examples</h2><p class="text--truncate cardDescription_PWke" title="1 item">1 item</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/streams/kafka-streams/references"><h2 class="text--truncate cardTitle_rnsV" title="References">📄️<!-- --> <!-- -->References</h2><p class="text--truncate cardDescription_PWke" title="Writing a Streams Application">Writing a Streams Application</p></a></article></section></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/streams/java-8-streams/examples"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Examples</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/streams/kafka-streams/glossary"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Glossary</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#-for-confluent-platform" class="table-of-contents__link toc-highlight"><em><strong>... for Confluent Platform</strong></em></a></li><li><a href="#stream" class="table-of-contents__link toc-highlight">Stream</a></li><li><a href="#stream-processing-application" class="table-of-contents__link toc-highlight">Stream Processing Application</a></li><li><a href="#processor-topology" class="table-of-contents__link toc-highlight">Processor Topology</a></li><li><a href="#stream-processor" class="table-of-contents__link toc-highlight">Stream Processor</a></li><li><a href="#stateful-stream-processing" class="table-of-contents__link toc-highlight">Stateful Stream Processing</a></li><li><a href="#duality-of-streams-and-tables" class="table-of-contents__link toc-highlight">Duality of Streams and Tables</a></li><li><a href="#kstream" class="table-of-contents__link toc-highlight">KStream</a></li><li><a href="#ktable" class="table-of-contents__link toc-highlight">KTable</a></li><li><a href="#effect-of-kafka-log-compaction" class="table-of-contents__link toc-highlight">Effect of Kafka log compaction</a></li><li><a href="#globalktable" class="table-of-contents__link toc-highlight">GlobalKTable</a></li><li><a href="#time" class="table-of-contents__link toc-highlight">Time</a><ul><li><a href="#event-time" class="table-of-contents__link toc-highlight">event-time</a></li><li><a href="#processing-time" class="table-of-contents__link toc-highlight">processing-time</a></li><li><a href="#ingestion-time" class="table-of-contents__link toc-highlight">ingestion-time</a></li><li><a href="#stream-time" class="table-of-contents__link toc-highlight">stream-time</a></li><li><a href="#timestamps" class="table-of-contents__link toc-highlight">Timestamps</a></li></ul></li><li><a href="#aggregations" class="table-of-contents__link toc-highlight">Aggregations</a></li><li><a href="#joins" class="table-of-contents__link toc-highlight">Joins</a></li><li><a href="#windowing" class="table-of-contents__link toc-highlight">Windowing</a></li><li><a href="#interactive-queries" class="table-of-contents__link toc-highlight">Interactive Queries</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://netty.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Netty Project<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/reactive-streams/reactive-streams-jvm?tab=readme-ov-file" target="_blank" rel="noopener noreferrer" class="footer__link-item">reactive-streams-jvm <svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Tools</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://pages.github.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Pages<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">License</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank"><img src="https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-sa.png" alt="CC BY-SA License" height="40px" width="114px"></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 jreact.com</div></div></div></footer></div>
</body>
</html>