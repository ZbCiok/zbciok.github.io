<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-apache-hadoop/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Apache Hadoop | Machine Learning &amp; Big Data &amp; Reactive Programming</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://zbciok.github.io/docs/apache-hadoop"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Apache Hadoop | Machine Learning &amp; Big Data &amp; Reactive Programming"><meta data-rh="true" name="description" content="Apache Hadoop"><meta data-rh="true" property="og:description" content="Apache Hadoop"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://zbciok.github.io/docs/apache-hadoop"><link data-rh="true" rel="alternate" href="https://zbciok.github.io/docs/apache-hadoop" hreflang="en"><link data-rh="true" rel="alternate" href="https://zbciok.github.io/docs/apache-hadoop" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Machine Learning &amp; Big Data &amp; Reactive Programming RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Machine Learning &amp; Big Data &amp; Reactive Programming Atom Feed"><link rel="stylesheet" href="/assets/css/styles.4b0135be.css">
<script src="/assets/js/runtime~main.91423f6c.js" defer="defer"></script>
<script src="/assets/js/main.f971ea5f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="RP" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="RP" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Home</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://jreact.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">jreact.com<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/streams">Streams</a><button aria-label="Expand sidebar category &#x27;Streams&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/project-reactor">Project Reactor</a><button aria-label="Expand sidebar category &#x27;Project Reactor&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/smallrye-mutiny">SmallRye Mutiny</a><button aria-label="Expand sidebar category &#x27;SmallRye Mutiny&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/vertx">VERT.X</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/aws">AWS</a><button aria-label="Expand sidebar category &#x27;AWS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/quarkus">Quarkus</a><button aria-label="Expand sidebar category &#x27;Quarkus&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/spring">Spring</a><button aria-label="Expand sidebar category &#x27;Spring&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/messaging-1">Messaging</a><button aria-label="Expand sidebar category &#x27;Messaging&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/apache-flink">Apache Flink</a><button aria-label="Expand sidebar category &#x27;Apache Flink&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible menu__list-item-collapsible--active"><a class="menu__link menu__link--sublist menu__link--active" aria-current="page" href="/docs/apache-hadoop">Apache Hadoop</a><button aria-label="Collapse sidebar category &#x27;Apache Hadoop&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/apache-hadoop/install">Setting up a Single Node Cluster</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/apache-hadoop/sqoop">SQOOP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/apache-hadoop/hbase">HBase</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/apache-hadoop/hbase-pseudo-distributed-mode-kerberos">HBase in Pseudo-Distributed Mode with Kerberos Authentication</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/apache-hadoop/hadoop-using-docker-01">Hadoop-3.2.1 using Docker 01</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/apache-hadoop/hadoop-using-docker-02">Hadoop-3.4.1 using Docker 02</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/apache-hadoop/hive/">Hive</a><button aria-label="Expand sidebar category &#x27;Hive&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/docs/apache-hadoop/examples">Examples</a><button aria-label="Expand sidebar category &#x27;Examples&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/apache-hadoop/references">References</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/apache-spark">Apache Spark</a><button aria-label="Expand sidebar category &#x27;Apache Spark&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/artificial-intelligence/">Artificial Intelligence</a><button aria-label="Expand sidebar category &#x27;Artificial Intelligence&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/auxiliary-software">Auxiliary</a><button aria-label="Expand sidebar category &#x27;Auxiliary&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Apache Hadoop</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Apache Hadoop</h1></header>
<p><a href="https://hadoop.apache.org/" target="_blank" rel="noopener noreferrer"><em><strong>Apache Hadoop</strong></em></a> <br>
<em><strong><a href="https://www.tpointtech.com/hadoop-tutorial" target="_blank" rel="noopener noreferrer">https://www.tpointtech.com/hadoop-tutorial</a></strong></em></p>
<p><strong>Apache‚Äôs Hadoop is a leading <a href="/docs/auxiliary-software/big-data"><em><strong>Big Data</strong></em></a> platform used by IT giants Yahoo, Facebook &amp; Google.</strong><br></p>
<p><strong>Apache Hadoop is an open source software framework used to develop data processing applications which are executed in a distributed computing environment.</strong> <em>(A distributed system is simply any environment where multiple computers or devices are working on a variety of tasks and components, all spread across a network. Components within distributed systems split up the work, coordinating efforts to complete a given job more efficiently than if only a single device ran it.)</em></p>
<p>Similar to data residing in a local file system of a personal computer system, in Hadoop, data resides in a distributed file system which is called as a <strong>Hadoop Distributed File System (HDFS)</strong>. The processing model is based on ‚ÄòData Locality‚Äô concept wherein computational logic is sent to cluster nodes (servers) containing data. This computational logic is nothing, but a compiled version of a program written in a high-level language such as Java. Such a program, processes data stored in Hadoop HDFS.</p>
<p>The Hadoop Architecture Mainly consists of 4 components.</p>
<ul>
<li><strong>MapReduce</strong>: This is a framework which helps Java programs to do the parallel computation on data using key value pair. The <strong>Map</strong> task takes input data and converts it into a data set which can be computed in Key value pair. The output of <strong>Map</strong> task is consumed by <strong>Reduce</strong> task and then the out of reducer gives the desired result.</li>
<li><strong>HDFS</strong> (Hadoop Distributed File System): It states that the files will be broken into blocks and stored in nodes over the distributed architecture.</li>
<li><strong>YARN</strong> (Yet Another Resource Negotiator): Yet another Resource Negotiator is used for job scheduling and manage the cluster.</li>
<li><strong>Common Utilities or Hadoop Common</strong>: These Java libraries are used to start Hadoop and are used by other Hadoop modules.</li>
</ul>
<img src="/img/apache-hadoop/hadoop-architecture-and-core-components.png" alt="hadoop-architecture-and-core-components.png">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mapreduce">MapReduce<a href="#mapreduce" class="hash-link" aria-label="Direct link to MapReduce" title="Direct link to MapReduce">‚Äã</a></h2>
<img src="/img/apache-hadoop/map-reduce-diagram.webp" width="600 px" alt="map-reduce-diagram.webp">
<p>A MapReduce system is usually composed of three steps (even though it&#x27;s generalized as the combination of Map and Reduce operations/functions). The MapReduce operations are:</p>
<ul>
<li><strong>Map</strong>: The input data is first split into smaller blocks. The Hadoop framework then decides how many mappers to use, based on the size of the data to be processed and the memory block available on each mapper server. Each block is then assigned to a mapper for processing. Each ‚Äòworker‚Äô node applies the map function to the local data, and writes the output to temporary storage. The primary (master) node ensures that only a single copy of the redundant input data is processed.</li>
<li><strong>Shuffle</strong>, combine and partition: worker nodes redistribute data based on the output keys (produced by the map function), such that all data belonging to one key is located on the same worker node. As an optional process the combiner (a reducer) can run individually on each mapper server to reduce the data on each mapper even further making reducing the data footprint and shuffling and sorting easier. Partition (not optional) is the process that decides how the data has to be presented to the reducer and also assigns it to a particular reducer.</li>
<li><strong>Reduce</strong>: A reducer cannot start while a mapper is still in progress. Worker nodes process each group of <code>&lt;key,value&gt;</code> pairs output data, in parallel to produce <code>&lt;key,value&gt;</code> pairs as output. All the map output values that have the same key are assigned to a single reducer, which then aggregates the values for that key. Unlike the map function which is mandatory to filter and sort the initial data, the reduce function is optional.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="hdfs-hadoop-distributed-file-system">HDFS (Hadoop Distributed File System)<a href="#hdfs-hadoop-distributed-file-system" class="hash-link" aria-label="Direct link to HDFS (Hadoop Distributed File System)" title="Direct link to HDFS (Hadoop Distributed File System)">‚Äã</a></h2>
<p>More detailed description: <a href="https://www.simplilearn.com/tutorials/hadoop-tutorial/hdfs" target="_blank" rel="noopener noreferrer"><em><strong>HDFS Tutorial</strong></em></a></p>
<img src="/img/apache-hadoop/hdfs-architecture-01.png" width="600 px" alt="hdfs-architecture-01.png">
<p><strong>HDFS has three components:</strong></p>
<ul>
<li><strong>NameNode:</strong> There is a single NameNode per cluster. This service stores the file system metadata (e.g., the directory structure), and the file names, and also, the location where the blocks of a file are stored.</li>
<li><strong>DataNodes:</strong> These services usually run on each server that also does the compute. Each DataNode stores blocks from many files. The DataNodes are also responsible for replicating these blocks to other DataNodes to protect files against data loss.</li>
<li><strong>Clients:</strong> The client provides access to the file system and the Hadoop/MapReduce jobs that want to access the data. The client, a piece of software embedded in the Hadoop distribution, communicates with the NameNode to find the file and retrieve the block‚Äôs locations. Then the client reads or writes the blocks directly from the DataNodes.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="read-operation-in-hdfs">Read Operation In HDFS<a href="#read-operation-in-hdfs" class="hash-link" aria-label="Direct link to Read Operation In HDFS" title="Direct link to Read Operation In HDFS">‚Äã</a></h3>
<p><a href="https://www.guru99.com/learn-hdfs-a-beginners-guide.html" target="_blank" rel="noopener noreferrer"><em><strong>Read / Write Operations in HDFS</strong></em></a><br></p>
<p>Data read request is served by HDFS, NameNode, and DataNode. Let‚Äôs call the reader as a ‚Äòclient‚Äô. Below diagram depicts file read operation in Hadoop.</p>
<img src="/img/apache-hadoop/read-hdfs-01.webp" width="600 px" alt="read-hdfs-01.webp">
<ul>
<li>A client initiates read request by calling ‚Äòopen()‚Äô method of FileSystem object; it is an object of type DistributedFileSystem.</li>
<li>This object connects to namenode using RPC and gets metadata information such as the locations of the blocks of the file. Please note that these addresses are of first few blocks of a file.</li>
<li>In response to this metadata request, addresses of the DataNodes having a copy of that block is returned back.</li>
<li>Once addresses of DataNodes are received, an object of type FSDataInputStream is returned to the client. FSDataInputStream contains DFSInputStream which takes care of interactions with DataNode and NameNode. In step 4 shown in the above diagram, a client invokes ‚Äòread()‚Äô method which causes DFSInputStream to establish a connection with the first DataNode with the first block of a file.</li>
<li>Data is read in the form of streams wherein client invokes ‚Äòread()‚Äô method repeatedly. This process of read() operation continues till it reaches the end of block.</li>
<li>Once the end of a block is reached, DFSInputStream closes the connection and moves on to locate the next DataNode for the next block</li>
<li>Once a client has done with the reading, it calls a close() method.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="write-operation-in-hdfs">Write Operation In HDFS<a href="#write-operation-in-hdfs" class="hash-link" aria-label="Direct link to Write Operation In HDFS" title="Direct link to Write Operation In HDFS">‚Äã</a></h3>
<img src="/img/apache-hadoop/write-hdfs-01.webp" width="600 px" alt="write-hdfs-01.webp">
<ul>
<li>A client initiates write operation by calling ‚Äòcreate()‚Äô method of DistributedFileSystem object which creates a new file ‚Äì Step no. 1 in the above diagram.</li>
<li>DistributedFileSystem object connects to the NameNode using RPC call and initiates new file creation. However, this file creates operation does not associate any blocks with the file. It is the responsibility of NameNode to verify that the file (which is being created) does not exist already and a client has correct permissions to create a new file. If a file already exists or client does not have sufficient permission to create a new file, then IOException is thrown to the client. Otherwise, the operation succeeds and a new record for the file is created by the NameNode.</li>
<li>Once a new record in NameNode is created, an object of type FSDataOutputStream is returned to the client. A client uses it to write data into the HDFS. Data write method is invoked (step 3 in the diagram).</li>
<li>FSDataOutputStream contains DFSOutputStream object which looks after communication with DataNodes and NameNode. While the client continues writing data, DFSOutputStream continues creating packets with this data. These packets are enqueued into a queue which is called as DataQueue.</li>
<li>There is one more component called DataStreamer which consumes this DataQueue. DataStreamer also asks NameNode for allocation of new blocks thereby picking desirable DataNodes to be used for replication.</li>
<li>Now, the process of replication starts by creating a pipeline using DataNodes. In our case, we have chosen a replication level of 3 and hence there are 3 DataNodes in the pipeline.</li>
<li>The DataStreamer pours packets into the first DataNode in the pipeline.</li>
<li>Every DataNode in a pipeline stores packet received by it and forwards the same to the second DataNode in a pipeline.</li>
<li>Another queue, ‚ÄòAck Queue‚Äô is maintained by DFSOutputStream to store packets which are waiting for acknowledgment from DataNodes.</li>
<li>Once acknowledgment for a packet in the queue is received from all DataNodes in the pipeline, it is removed from the ‚ÄòAck Queue‚Äô. In the event of any DataNode failure, packets from this queue are used to reinitiate the operation.</li>
<li>After a client is done with the writing data, it calls a close() method (Step 9 in the diagram) Call to close(), results into flushing remaining data packets to the pipeline followed by waiting for acknowledgment.</li>
<li>Once a final acknowledgment is received, NameNode is contacted to tell it that the file write operation is complete.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="yarn-yet-another-resource-negotiator">YARN (Yet Another Resource Negotiator)<a href="#yarn-yet-another-resource-negotiator" class="hash-link" aria-label="Direct link to YARN (Yet Another Resource Negotiator)" title="Direct link to YARN (Yet Another Resource Negotiator)">‚Äã</a></h2>
<p>YARN is a Framework on which MapReduce works. YARN performs 2 operations that are Job scheduling and Resource Management. The Purpose of Job schedular is to divide a big task into small jobs so that each job can be assigned to various slaves in a Hadoop cluster and Processing can be Maximized. Job Scheduler also keeps track of which job is important, which job has more priority, dependencies between the jobs and all the other information like job timing, etc. And the use of Resource Manager is to manage all the resources that are made available for running a Hadoop cluster.</p>
<p>Features of YARN</p>
<ul>
<li>Multi-Tenancy</li>
<li>Scalability</li>
<li>Cluster-Utilization</li>
<li>Compatibility</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="hadoop-common-utilities">Hadoop Common Utilities<a href="#hadoop-common-utilities" class="hash-link" aria-label="Direct link to Hadoop Common Utilities" title="Direct link to Hadoop Common Utilities">‚Äã</a></h2>
<p>Hadoop common or Common utilities are nothing but our java library and java files or we can say the java scripts that we need for all the other components present in a Hadoop cluster. these utilities are used by HDFS, YARN, and MapReduce for running the cluster. Hadoop Common verify that Hardware failure in a Hadoop cluster is common so it needs to be solved automatically in software by Hadoop Framework.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="network-topology-in-hadoop">Network Topology In Hadoop<a href="#network-topology-in-hadoop" class="hash-link" aria-label="Direct link to Network Topology In Hadoop" title="Direct link to Network Topology In Hadoop">‚Äã</a></h2>
<p>Topology (Arrangement) of the network, affects the performance of the Hadoop cluster when the size of the Hadoop cluster grows. In addition to the performance, one also needs to care about the high availability and handling of failures. In order to achieve this Hadoop, cluster formation makes use of network topology.</p>
<img src="/img/apache-hadoop/hadoop-data-center-01.jpg" width="600 px" alt="hadoop-data-center-01.jpg">
<p>Typically, network bandwidth is an important factor to consider while forming any network. However, as measuring bandwidth could be difficult, in Hadoop, a network is represented as a tree and distance between nodes of this tree (number of hops) is considered as an important factor in the formation of Hadoop cluster. Here, the distance between two nodes is equal to sum of their distance to their closest common ancestor.</p>
<p>Hadoop cluster consists of a data center, the rack and the node which actually executes jobs. Here, data center consists of racks and rack consists of nodes. Network bandwidth available to processes varies depending upon the location of the processes. That is, the bandwidth available becomes lesser as we go away from-</p>
<ul>
<li>Processes on the same node</li>
<li>Different nodes on the same rack</li>
<li>Nodes on different racks of the same data center</li>
<li>Nodes in different data centers</li>
</ul>
<hr>
<br>
<br>
<!-- -->
<section class="row"><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/apache-hadoop/install"><h2 class="text--truncate cardTitle_rnsV" title="Setting up a Single Node Cluster">üìÑÔ∏è<!-- --> <!-- -->Setting up a Single Node Cluster</h2><p class="text--truncate cardDescription_PWke" title="How to Install Hadoop on Ubuntu">How to Install Hadoop on Ubuntu</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/apache-hadoop/sqoop"><h2 class="text--truncate cardTitle_rnsV" title="SQOOP">üìÑÔ∏è<!-- --> <!-- -->SQOOP</h2><p class="text--truncate cardDescription_PWke" title="Apache Sqoop">Apache Sqoop</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/apache-hadoop/hbase"><h2 class="text--truncate cardTitle_rnsV" title="HBase">üìÑÔ∏è<!-- --> <!-- -->HBase</h2><p class="text--truncate cardDescription_PWke" title="https://hbase.apache.org/">https://hbase.apache.org/</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/apache-hadoop/hbase-pseudo-distributed-mode-kerberos"><h2 class="text--truncate cardTitle_rnsV" title="HBase in Pseudo-Distributed Mode with Kerberos Authentication">üìÑÔ∏è<!-- --> <!-- -->HBase in Pseudo-Distributed Mode with Kerberos Authentication</h2><p class="text--truncate cardDescription_PWke" title="Prerequisites">Prerequisites</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/apache-hadoop/hadoop-using-docker-01"><h2 class="text--truncate cardTitle_rnsV" title="Hadoop-3.2.1 using Docker 01">üìÑÔ∏è<!-- --> <!-- -->Hadoop-3.2.1 using Docker 01</h2><p class="text--truncate cardDescription_PWke" title="Description">Description</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/apache-hadoop/hadoop-using-docker-02"><h2 class="text--truncate cardTitle_rnsV" title="Hadoop-3.4.1 using Docker 02">üìÑÔ∏è<!-- --> <!-- -->Hadoop-3.4.1 using Docker 02</h2><p class="text--truncate cardDescription_PWke" title="Description">Description</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/apache-hadoop/hive/"><h2 class="text--truncate cardTitle_rnsV" title="Hive">üóÉÔ∏è<!-- --> <!-- -->Hive</h2><p class="text--truncate cardDescription_PWke" title="1 item">1 item</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/apache-hadoop/examples"><h2 class="text--truncate cardTitle_rnsV" title="Examples">üóÉÔ∏è<!-- --> <!-- -->Examples</h2><p class="text--truncate cardDescription_PWke" title="7 items">7 items</p></a></article><article class="col col--6 margin-bottom--lg"><a class="card padding--lg cardContainer_fWXF" href="/docs/apache-hadoop/references"><h2 class="text--truncate cardTitle_rnsV" title="References">üìÑÔ∏è<!-- --> <!-- -->References</h2><p class="text--truncate cardDescription_PWke" title="Hadoop &amp; Mapreduce Examples: Create First Program in Java">Hadoop &amp; Mapreduce Examples: Create First Program in Java</p></a></article></section></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/apache-flink/references"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">References</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/apache-hadoop/install"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Setting up a Single Node Cluster</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#mapreduce" class="table-of-contents__link toc-highlight">MapReduce</a></li><li><a href="#hdfs-hadoop-distributed-file-system" class="table-of-contents__link toc-highlight">HDFS (Hadoop Distributed File System)</a><ul><li><a href="#read-operation-in-hdfs" class="table-of-contents__link toc-highlight">Read Operation In HDFS</a></li><li><a href="#write-operation-in-hdfs" class="table-of-contents__link toc-highlight">Write Operation In HDFS</a></li></ul></li><li><a href="#yarn-yet-another-resource-negotiator" class="table-of-contents__link toc-highlight">YARN (Yet Another Resource Negotiator)</a></li><li><a href="#hadoop-common-utilities" class="table-of-contents__link toc-highlight">Hadoop Common Utilities</a></li><li><a href="#network-topology-in-hadoop" class="table-of-contents__link toc-highlight">Network Topology In Hadoop</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://netty.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Netty Project<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/reactive-streams/reactive-streams-jvm?tab=readme-ov-file" target="_blank" rel="noopener noreferrer" class="footer__link-item">reactive-streams-jvm <svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Tools</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://pages.github.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Pages<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">License</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank"><img src="https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-sa.png" alt="CC BY-SA License" height="40px" width="114px"></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2025 jreact.com</div></div></div></footer></div>
</body>
</html>