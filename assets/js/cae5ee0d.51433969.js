"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[1203],{355:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>l,frontMatter:()=>i,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"apache-hadoop/Examples/mapreduce-first-program","title":"MapReduce: the First Program","description":"Start Hadoop: Single Node Cluster","source":"@site/docs/apache-hadoop/Examples/mapreduce-first-program.mdx","sourceDirName":"apache-hadoop/Examples","slug":"/apache-hadoop/Examples/mapreduce-first-program","permalink":"/docs/apache-hadoop/Examples/mapreduce-first-program","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":100,"frontMatter":{"sidebar_position":100},"sidebar":"tutorialSidebar","previous":{"title":"Examples","permalink":"/docs/apache-hadoop/examples"},"next":{"title":"MapReduce: the Second Program","permalink":"/docs/apache-hadoop/Examples/mapreduce-second-program"}}');var r=o(4848),a=o(8453);const i={sidebar_position:100},c="MapReduce: the First Program",d={},p=[{value:"Start Hadoop: <em><strong>Single Node Cluster</strong></em>",id:"start-hadoop-single-node-cluster",level:4},{value:"Based on <em><strong>https://www.h2kinfosys.com/blog/hadoop-mapreduce-examples/</strong></em>",id:"based-on-httpswwwh2kinfosyscombloghadoop-mapreduce-examples",level:4},{value:"Description",id:"description",level:2},{value:"MapReduce: Word Counter:",id:"mapreduce-word-counter",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Start Programming",id:"start-programming",level:2},{value:"Create <code>wordcount/input</code> in /home/hadoop",id:"create-wordcountinput-in-homehadoop",level:4},{value:"Create two input files (local)",id:"create-two-input-files-local",level:4},{value:"Create HDFS input folder:",id:"create-hdfs-input-folder",level:4},{value:"Copy local files to HDFS file system",id:"copy-local-files-to-hdfs-file-system",level:4},{value:"Check if both files have been copied",id:"check-if-both-files-have-been-copied",level:4},{value:"Java Program",id:"java-program",level:2},{value:"pom.xml",id:"pomxml",level:4},{value:"Map.java",id:"mapjava",level:4},{value:"WordCount",id:"wordcount",level:4},{value:"Reduce.java",id:"reducejava",level:4},{value:"Run",id:"run",level:2},{value:"Copy",id:"copy",level:4},{value:"Run the MapReduce using the command given below.",id:"run-the-mapreduce-using-the-command-given-below",level:4},{value:"Output:",id:"output",level:4},{value:"Output:",id:"output-1",level:4}];function s(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"mapreduce-the-first-program",children:"MapReduce: the First Program"})}),"\n",(0,r.jsxs)(n.h4,{id:"start-hadoop-single-node-cluster",children:["Start Hadoop: ",(0,r.jsx)(n.a,{href:"/docs/apache-hadoop/install",children:(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"Single Node Cluster"})})})]}),"\n",(0,r.jsxs)(n.h4,{id:"based-on-httpswwwh2kinfosyscombloghadoop-mapreduce-examples",children:["Based on ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://www.h2kinfosys.com/blog/hadoop-mapreduce-examples/",children:"https://www.h2kinfosys.com/blog/hadoop-mapreduce-examples/"})})})]}),"\n",(0,r.jsx)(n.h2,{id:"description",children:"Description"}),"\n",(0,r.jsx)(n.h3,{id:"mapreduce-word-counter",children:"MapReduce: Word Counter:"}),"\n",(0,r.jsx)("img",{src:"/img/apache-hadoop/example-01-img-01.png",alt:"example-01-img-01.png"}),"\n",(0,r.jsx)(n.p,{children:"We'll implement the above."}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Linux"}),"\n",(0,r.jsx)(n.li,{children:"JDK 11"}),"\n",(0,r.jsx)(n.li,{children:"Maven"}),"\n",(0,r.jsx)(n.li,{children:"Hadoop 3.4.1"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"start-programming",children:"Start Programming"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"su - hadoop\nstart-dfs.sh\nstart-yarn.sh\n"})}),"\n",(0,r.jsxs)(n.h4,{id:"create-wordcountinput-in-homehadoop",children:["Create ",(0,r.jsx)(n.code,{children:"wordcount/input"})," in /home/hadoop"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"mkdir wordcount\nmkdir wordcount/input\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"chmod -R 777 /home/hadoop/wordcount\n"})}),"\n",(0,r.jsx)(n.h4,{id:"create-two-input-files-local",children:"Create two input files (local)"}),"\n",(0,r.jsxs)(n.p,{children:["in  ",(0,r.jsx)(n.code,{children:"wordcount/input"}),(0,r.jsx)("br",{}),"\n",(0,r.jsx)(n.strong,{children:"input_one"})," with content:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Hello to the World\n"})}),"\n",(0,r.jsxs)(n.p,{children:["and ",(0,r.jsx)("br",{}),"\n",(0,r.jsx)(n.strong,{children:"input_two"})," with content:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Hello Hadoop Hello World\n"})}),"\n",(0,r.jsx)("img",{src:"/img/apache-hadoop/example-01-img-02.png",alt:"example-01-img-02.png"}),"\n",(0,r.jsx)(n.h4,{id:"create-hdfs-input-folder",children:"Create HDFS input folder:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hdfs dfs -mkdir /wordcount\nhdfs dfs -mkdir /wordcount/input\n"})}),"\n",(0,r.jsx)(n.h4,{id:"copy-local-files-to-hdfs-file-system",children:"Copy local files to HDFS file system"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hdfs dfs -copyFromLocal /home/hadoop/wordcount/input/input_one /wordcount/input/\nhdfs dfs -copyFromLocal /home/hadoop/wordcount/input/input_two /wordcount/input/\n"})}),"\n",(0,r.jsx)(n.h4,{id:"check-if-both-files-have-been-copied",children:"Check if both files have been copied"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hdfs dfs -ls /wordcount/input/\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hadoop@LL01:~/hadoop-3.4.1/bin$ ./hdfs dfs -ls /wordcount/input/\nFound 2 items\n-rw-r--r--   1 hadoop supergroup         19 2025-03-11 11:00 /wordcount/input/input_one\n-rw-r--r--   1 hadoop supergroup         25 2025-03-11 11:04 /wordcount/input/input_two\n"})}),"\n",(0,r.jsx)(n.h2,{id:"java-program",children:"Java Program"}),"\n",(0,r.jsx)(n.h4,{id:"pomxml",children:"pom.xml"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>org.example</groupId>\n    <artifactId>mapreduce-first-program</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <target.java.version>11</target.java.version>\n        <maven.compiler.source>${target.java.version}</maven.compiler.source>\n        <maven.compiler.target>${target.java.version}</maven.compiler.target>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    </properties>\n\n    <dependencies>\n        \x3c!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core --\x3e\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-mapreduce-client-core</artifactId>\n            <version>3.4.1</version>\n        </dependency>\n\n        \x3c!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core --\x3e\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-core</artifactId>\n            <version>1.2.1</version>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n\n            \x3c!-- Java Compiler --\x3e\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.1</version>\n                <configuration>\n                    <source>${target.java.version}</source>\n                    <target>${target.java.version}</target>\n                </configuration>\n            </plugin>\n\n            \x3c!-- We use the maven-shade plugin to create a fat jar that contains all necessary dependencies. --\x3e\n            \x3c!-- Change the value of <mainClass>...</mainClass> if your program entry point changes. --\x3e\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>3.0.0</version>\n                <executions>\n                    \x3c!-- Run shade goal on package phase --\x3e\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <artifactSet>\n                                <excludes>\n                                    <exclude>org.apache.flink:flink-shaded-force-shading</exclude>\n                                    <exclude>com.google.code.findbugs:jsr305</exclude>\n                                    <exclude>org.slf4j:*</exclude>\n                                    <exclude>org.apache.logging.log4j:*</exclude>\n                                </excludes>\n                            </artifactSet>\n                            <filters>\n                                <filter>\n                                    \x3c!-- Do not copy the signatures in the META-INF folder.\n                                    Otherwise, this might cause SecurityExceptions when using the JAR. --\x3e\n                                    <artifact>*:*</artifact>\n                                    <excludes>\n                                        <exclude>META-INF/*.SF</exclude>\n                                        <exclude>META-INF/*.DSA</exclude>\n                                        <exclude>META-INF/*.RSA</exclude>\n                                    </excludes>\n                                </filter>\n                            </filters>\n                            <transformers>\n                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">\n                                    <mainClass>WordCount.WordCount</mainClass>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n\n    </build>\n</project>\n'})}),"\n",(0,r.jsx)(n.h4,{id:"mapjava",children:"Map.java"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"package WordCount;\n\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapred.MapReduceBase;\nimport org.apache.hadoop.mapred.Mapper;\nimport org.apache.hadoop.mapred.OutputCollector;\nimport org.apache.hadoop.mapred.Reporter;\npublic class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable>\n{\n\tprivate final static IntWritable one = new IntWritable(1);\n\n\tprivate Text word = new Text();\n\n\tpublic void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter)\n\t\t\tthrows IOException\n\t{\n\t\tString line = value.toString();\n\t\tStringTokenizer tokenizer = new StringTokenizer(line);\n\t\twhile (tokenizer.hasMoreTokens())\n\t\t{\n\t\t\tword.set(tokenizer.nextToken());\n\t\t\toutput.collect(word, one);\n\t\t}\n\t}\n}\n"})}),"\n",(0,r.jsx)(n.h4,{id:"wordcount",children:"WordCount"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'\npackage WordCount;\n\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapred.FileInputFormat;\nimport org.apache.hadoop.mapred.FileOutputFormat;\nimport org.apache.hadoop.mapred.JobClient;\nimport org.apache.hadoop.mapred.JobConf;\nimport org.apache.hadoop.mapred.TextInputFormat;\nimport org.apache.hadoop.mapred.TextOutputFormat;\n\npublic class WordCount\n{\n\tpublic static void main(String[] args) throws Exception\n\t{\n\t\tJobConf conf = new JobConf(WordCount.class);\n\t\tconf.setJobName("wordcount");\n\n\t\tconf.setOutputKeyClass(Text.class);\n\t\tconf.setOutputValueClass(IntWritable.class);\n\n\t\tconf.setMapperClass(Map.class);\n\t\tconf.setCombinerClass(Reduce.class);\n\t\tconf.setReducerClass(Reduce.class);\n\n\t\tconf.setInputFormat(TextInputFormat.class);\n\t\tconf.setOutputFormat(TextOutputFormat.class);\n\n\t\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\n\n\t\tJobClient.runJob(conf);\n\n\t}\n}\n'})}),"\n",(0,r.jsx)(n.h4,{id:"reducejava",children:"Reduce.java"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"package WordCount;\n\nimport java.io.IOException;\nimport java.util.Iterator;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapred.MapReduceBase;\nimport org.apache.hadoop.mapred.OutputCollector;\nimport org.apache.hadoop.mapred.Reducer;\nimport org.apache.hadoop.mapred.Reporter;\n\npublic class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable>\n{\n\tpublic void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output,\n\t\t\t\t\t   Reporter reporter) throws IOException\n\t{\n\t\tint sum = 0;\n\t\twhile (values.hasNext())\n\t\t{\n\t\t\tsum += values.next().get();\n\t\t}\n\t\toutput.collect(key, new IntWritable(sum));\n\t}\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"run",children:"Run"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"mvn clean package\n"})}),"\n",(0,r.jsx)(n.h4,{id:"copy",children:"Copy"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"mapreduce-first-program-1.0-SNAPSHOT.jar"})," ",(0,r.jsx)("br",{}),"\nto ",(0,r.jsx)("br",{}),"\n",(0,r.jsx)(n.code,{children:"/home/hadoop/wordcount/"})," ."]}),"\n",(0,r.jsx)(n.h4,{id:"run-the-mapreduce-using-the-command-given-below",children:"Run the MapReduce using the command given below."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hadoop jar /home/hadoop/wordcount/mapreduce-first-program-1.0-SNAPSHOT.jar /wordcount/input /wordcount/output\n"})}),"\n",(0,r.jsx)(n.h4,{id:"output",children:"Output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\n./hadoop jar /home/hadoop/wordcount/mapreduce-first-program-1.0-SNAPSHOT.jar /wordcount/input /wordcount/output\n2025-03-11 13:11:54,545 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /127.0.0.1:8032\n2025-03-11 13:11:54,616 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /127.0.0.1:8032\n...\n...\n\t\tPeak Map Physical memory (bytes)=375726080\n\t\tPeak Map Virtual memory (bytes)=2788450304\n\t\tPeak Reduce Physical memory (bytes)=260698112\n\t\tPeak Reduce Virtual memory (bytes)=2783264768\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters\n\t\tBytes Read=47\n\tFile Output Format Counters\n\t\tBytes Written=36\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hadoop dfs -cat /wordcount/output/part-00000\n"})}),"\n",(0,r.jsx)(n.h4,{id:"output-1",children:"Output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Hadoop\t1\nHello\t3\nWorld\t2\nthe\t1\nto\t1\n"})}),"\n",(0,r.jsxs)(n.p,{children:["or ",(0,r.jsx)("br",{})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"http://localhost:9870\n"})}),"\n",(0,r.jsx)("img",{src:"/img/apache-hadoop/example-01-img-03.png",alt:"example-01-img-03.png"}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Source of the java program:"})," ",(0,r.jsx)("br",{}),"\n",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://github.com/ZbCiok/zjc-examples/tree/main/streams/hadoop/mapreduce-first-program",children:"https://github.com/ZbCiok/zjc-examples/tree/main/streams/hadoop/mapreduce-first-program"})})})]})]})}function l(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(s,{...e})}):s(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>i,x:()=>c});var t=o(6540);const r={},a=t.createContext(r);function i(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);