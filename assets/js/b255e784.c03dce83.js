"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[8101],{317:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>i,contentTitle:()=>a,default:()=>h,frontMatter:()=>d,metadata:()=>r,toc:()=>p});const r=JSON.parse('{"id":"apache-hadoop/hadoop-using-docker-02","title":"Hadoop-3.4.1 using Docker 02","description":"Description","source":"@site/docs/apache-hadoop/hadoop-using-docker-02.mdx","sourceDirName":"apache-hadoop","slug":"/apache-hadoop/hadoop-using-docker-02","permalink":"/docs/apache-hadoop/hadoop-using-docker-02","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":600,"frontMatter":{"sidebar_position":600},"sidebar":"tutorialSidebar","previous":{"title":"Hadoop-3.2.1 using Docker 01","permalink":"/docs/apache-hadoop/hadoop-using-docker-01"},"next":{"title":"Examples","permalink":"/docs/apache-hadoop/examples"}}');var t=n(4848),s=n(8453);const d={sidebar_position:600},a="Hadoop-3.4.1 using Docker 02",i={},p=[{value:"Description",id:"description",level:2},{value:"Objectives",id:"objectives",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Set up Cluster",id:"set-up-cluster",level:2},{value:"Explore the hadoop environment",id:"explore-the-hadoop-environment",level:2},{value:"Examples Provided <em><strong>(some selected programs)</strong></em>",id:"examples-provided-some-selected-programs",level:2},{value:"Run <em><strong>hadoop-mapreduce-examples-3.4.1.jar wordcount</strong></em>",id:"run-hadoop-mapreduce-examples-341jar-wordcount",level:2},{value:"Stops containers and removes containers, networks, and volumes created by up.",id:"stops-containers-and-removes-containers-networks-and-volumes-created-by-up",level:2}];function c(e){const o={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(o.header,{children:(0,t.jsx)(o.h1,{id:"hadoop-341-using-docker-02",children:"Hadoop-3.4.1 using Docker 02"})}),"\n",(0,t.jsx)(o.h2,{id:"description",children:"Description"}),"\n",(0,t.jsxs)(o.p,{children:["We will create an instance of Hadoop Cluster within a Docker container.",(0,t.jsx)("br",{})]}),"\n",(0,t.jsx)(o.h3,{id:"objectives",children:"Objectives"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsx)(o.li,{children:"Run hadoop instance (3.4.1)"}),"\n",(0,t.jsx)(o.li,{children:"Run provided example"}),"\n"]}),"\n",(0,t.jsx)(o.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsx)(o.li,{children:"Ubuntu"}),"\n",(0,t.jsx)(o.li,{children:"Docker"}),"\n"]}),"\n",(0,t.jsx)(o.h2,{id:"set-up-cluster",children:"Set up Cluster"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.strong,{children:"Clone the repository"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"git clone https://github.com/hibuz/hadoop-docker.git\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.strong,{children:"Go to"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"cd hadoop-docker\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.strong,{children:"Compose the docker application"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"docker compose up hadoop-dev --no-build\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.strong,{children:"Run the namenode as a mounted drive on bash"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"docker exec -it hadoop bash\n"})}),"\n",(0,t.jsx)(o.strong,{children:"Output"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"hadoop@dfc392f64adc:~/hadoop-3.4.1$\n\n# dfc392f64adc - container id\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(o.h2,{id:"explore-the-hadoop-environment",children:"Explore the hadoop environment"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.em,{children:(0,t.jsx)(o.strong,{children:"hadoop-3.4.1 ls"})}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"hadoop@dfc392f64adc:~/hadoop-3.4.1$ ls -1\nbin\netc\ninclude\nlib\nlibexec\nLICENSE-binary\nlicenses-binary\nLICENSE.txt\nlogs\nNOTICE-binary\nNOTICE.txt\nREADME.txt\nsbin\nshare\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.em,{children:(0,t.jsx)(o.strong,{children:"$HADOOP_HOME"})}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"hadoop@dfc392f64adc:~/hadoop-3.4.1$ echo $HADOOP_HOME\n/home/hadoop/hadoop-3.4.1\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.em,{children:(0,t.jsx)(o.strong,{children:"[configuration].xml"})}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"\nhadoop@dfc392f64adc:~/hadoop-3.4.1$ ls $HADOOP_HOME/etc/hadoop/*.xml\n/home/hadoop/hadoop-3.4.1/etc/hadoop/capacity-scheduler.xml  /home/hadoop/hadoop-3.4.1/etc/hadoop/httpfs-site.xml\n/home/hadoop/hadoop-3.4.1/etc/hadoop/core-site.xml\t     /home/hadoop/hadoop-3.4.1/etc/hadoop/kms-acls.xml\n/home/hadoop/hadoop-3.4.1/etc/hadoop/hadoop-policy.xml\t     /home/hadoop/hadoop-3.4.1/etc/hadoop/kms-site.xml\n/home/hadoop/hadoop-3.4.1/etc/hadoop/hdfs-rbf-site.xml\t     /home/hadoop/hadoop-3.4.1/etc/hadoop/mapred-site.xml\n/home/hadoop/hadoop-3.4.1/etc/hadoop/hdfs-site.xml\t     /home/hadoop/hadoop-3.4.1/etc/hadoop/yarn-site.xml\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.em,{children:(0,t.jsx)(o.strong,{children:"sbin ls"})}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"hadoop@dfc392f64adc:~/hadoop-3.4.1/sbin$ ls\ndistribute-exclude.sh  kms.sh\t\t\tstart-balancer.sh    start-yarn.sh     stop-dfs.sh\t   yarn-daemon.sh\nFederationStateStore   mr-jobhistory-daemon.sh\tstart-dfs.cmd\t     stop-all.cmd      stop-secure-dns.sh  yarn-daemons.sh\nhadoop-daemon.sh       refresh-namenodes.sh\tstart-dfs.sh\t     stop-all.sh       stop-yarn.cmd\nhadoop-daemons.sh      start-all.cmd\t\tstart-secure-dns.sh  stop-balancer.sh  stop-yarn.sh\nhttpfs.sh\t       start-all.sh\t\tstart-yarn.cmd\t     stop-dfs.cmd      workers.sh\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.em,{children:(0,t.jsx)(o.strong,{children:"start dfs"})}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"hadoop@dfc392f64adc:~/hadoop-3.4.1/sbin$ start-dfs.sh\nStarting namenodes on [localhost]\nlocalhost: namenode is running as process 165.  Stop it first and ensure /tmp/hadoop-hadoop-namenode.pid file is empty before retry.\nStarting datanodes\nlocalhost: datanode is running as process 391.  Stop it first and ensure /tmp/hadoop-hadoop-datanode.pid file is empty before retry.\nStarting secondary namenodes [dfc392f64adc]\ndfc392f64adc: secondarynamenode is running as process 647.  Stop it first and ensure /tmp/hadoop-hadoop-secondarynamenode.pid file is empty before retry.\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.em,{children:(0,t.jsx)(o.strong,{children:"start yarn"})}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"hadoop@dfc392f64adc:~/hadoop-3.4.1/sbin$ start-yarn.sh\nStarting resourcemanager\nStarting nodemanagers\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.em,{children:(0,t.jsx)(o.strong,{children:"jps"})}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"hadoop@dfc392f64adc:~/hadoop-3.4.1/sbin$ jps\n1922 ResourceManager\n2277 NodeManager\n165 NameNode\n647 SecondaryNameNode\n391 DataNode\n2510 Jps\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(o.h2,{id:"examples-provided-some-selected-programs",children:["Examples Provided ",(0,t.jsx)(o.em,{children:(0,t.jsx)(o.strong,{children:"(some selected programs)"})})]}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.strong,{children:"Mapreduce"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"hadoop@dfc392f64adc:~/hadoop-3.4.1/share/hadoop/mapreduce$ ls\nhadoop-mapreduce-client-app-3.4.1.jar\t  hadoop-mapreduce-client-hs-3.4.1.jar\t\thadoop-mapreduce-client-jobclient-3.4.1-tests.jar  hadoop-mapreduce-client-uploader-3.4.1.jar  sources\nhadoop-mapreduce-client-common-3.4.1.jar  hadoop-mapreduce-client-hs-plugins-3.4.1.jar\thadoop-mapreduce-client-nativetask-3.4.1.jar\t   hadoop-mapreduce-examples-3.4.1.jar\nhadoop-mapreduce-client-core-3.4.1.jar\t  hadoop-mapreduce-client-jobclient-3.4.1.jar\thadoop-mapreduce-client-shuffle-3.4.1.jar\t   jdiff\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsxs)(o.strong,{children:[(0,t.jsx)(o.em,{children:(0,t.jsx)(o.strong,{children:"hadoop-mapreduce-examples-3.4.1.jar"})})," - valid program names:"]}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n  dbcount: An example job that count the pageview counts from a database.\n  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n  grep: A map/reduce program that counts the matches of a regex in the input.\n  join: A job that effects a join over sorted, equally partitioned datasets\n  multifilewc: A job that counts words from several files.\n  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n  randomwriter: A map/reduce program that writes 10GB of random data per node.\n  secondarysort: An example defining a secondary sort to the reduce.\n  sort: A map/reduce program that sorts the data written by the random writer.\n  sudoku: A sudoku solver.\n  teragen: Generate data for the terasort\n  terasort: Run the terasort\n  teravalidate: Checking results of terasort\n  wordcount: A map/reduce program that counts the words in the input files.\n  wordmean: A map/reduce program that counts the average length of the words in the input files.\n  wordmedian: A map/reduce program that counts the median length of the words in the input files.\n  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(o.h2,{id:"run-hadoop-mapreduce-examples-341jar-wordcount",children:["Run ",(0,t.jsx)(o.em,{children:(0,t.jsx)(o.strong,{children:"hadoop-mapreduce-examples-3.4.1.jar wordcount"})})]}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.strong,{children:"Make the HDFS directories"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"hdfs dfs -mkdir -p /user/hadoop/input\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.strong,{children:"Copy the input files"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"hdfs dfs -put ./etc/hadoop/*.xml input\n"})}),"\n",(0,t.jsx)(o.strong,{children:"verify"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"hdfs dfs -ls /user/hadoop/input\nFound 10 items\n-rw-r--r--   1 hadoop supergroup       9213 2025-04-25 10:49 /user/hadoop/input/capacity-scheduler.xml\n-rw-r--r--   1 hadoop supergroup        856 2025-04-25 10:49 /user/hadoop/input/core-site.xml\n-rw-r--r--   1 hadoop supergroup      14007 2025-04-25 10:49 /user/hadoop/input/hadoop-policy.xml\n-rw-r--r--   1 hadoop supergroup        683 2025-04-25 10:49 /user/hadoop/input/hdfs-rbf-site.xml\n-rw-r--r--   1 hadoop supergroup        840 2025-04-25 10:49 /user/hadoop/input/hdfs-site.xml\n-rw-r--r--   1 hadoop supergroup        620 2025-04-25 10:49 /user/hadoop/input/httpfs-site.xml\n-rw-r--r--   1 hadoop supergroup       3518 2025-04-25 10:49 /user/hadoop/input/kms-acls.xml\n-rw-r--r--   1 hadoop supergroup        682 2025-04-25 10:49 /user/hadoop/input/kms-site.xml\n-rw-r--r--   1 hadoop supergroup        836 2025-04-25 10:49 /user/hadoop/input/mapred-site.xml\n-rw-r--r--   1 hadoop supergroup        990 2025-04-25 10:49 /user/hadoop/input/yarn-site.xml\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.strong,{children:"Run Examples"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"\nhadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar  wordcount input output\n"})}),"\n",(0,t.jsx)(o.strong,{children:"Output"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"...\n...\n2025-04-25 10:59:39,423 INFO mapred.LocalJobRunner: Finishing task: attempt_local1918393452_0001_r_000000_0\n2025-04-25 10:59:39,423 INFO mapred.LocalJobRunner: reduce task executor complete.\n2025-04-25 10:59:39,940 INFO mapreduce.Job: Job job_local1918393452_0001 running in uber mode : false\n2025-04-25 10:59:39,942 INFO mapreduce.Job:  map 100% reduce 100%\n2025-04-25 10:59:39,943 INFO mapreduce.Job: Job job_local1918393452_0001 completed successfully\n2025-04-25 10:59:39,964 INFO mapreduce.Job: Counters: 36\n\tFile System Counters\n\t\tFILE: Number of bytes read=3214979\n\t\tFILE: Number of bytes written=11244233\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=307019\n\t\tHDFS: Number of bytes written=11086\n\t\tHDFS: Number of read operations=168\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=13\n\t\tHDFS: Number of bytes read erasure-coded=0\n\tMap-Reduce Framework\n\t\tMap input records=842\n\t\tMap output records=3551\n\t\tMap output bytes=43805\n\t\tMap output materialized bytes=23062\n\t\tInput split bytes=1199\n\t\tCombine input records=3551\n\t\tCombine output records=1341\n\t\tReduce input groups=634\n\t\tReduce shuffle bytes=23062\n\t\tReduce input records=1341\n\t\tReduce output records=634\n\t\tSpilled Records=2682\n\t\tShuffled Maps =10\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=10\n\t\tGC time elapsed (ms)=15\n\t\tTotal committed heap usage (bytes)=7495221248\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters\n\t\tBytes Read=32245\n\tFile Output Format Counters\n\t\tBytes Written=11086\n"})}),"\n"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.strong,{children:"See results in browser"}),"\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/02-hadoop-using-docker-01.png",alt:"02-hadoop-using-docker-01.png"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(o.h2,{id:"stops-containers-and-removes-containers-networks-and-volumes-created-by-up",children:"Stops containers and removes containers, networks, and volumes created by up."}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"docker compose down -v\n"})}),"\n",(0,t.jsx)(o.hr,{}),"\n",(0,t.jsxs)(o.p,{children:["Based on ",(0,t.jsx)(o.em,{children:(0,t.jsx)(o.strong,{children:(0,t.jsx)(o.a,{href:"https://github.com/hibuz/hadoop-docker",children:"https://github.com/hibuz/hadoop-docker"})})})]})]})}function h(e={}){const{wrapper:o}={...(0,s.R)(),...e.components};return o?(0,t.jsx)(o,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,o,n)=>{n.d(o,{R:()=>d,x:()=>a});var r=n(6540);const t={},s=r.createContext(t);function d(e){const o=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(o):{...o,...e}}),[o,e])}function a(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:d(e.components),r.createElement(s.Provider,{value:o},e.children)}}}]);