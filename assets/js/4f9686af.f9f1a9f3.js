"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[2915],{2713:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>p,default:()=>c,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"apache-spark/examples/python-spark-sql-01","title":"Python Spark SQL 01","description":"Spark SQL","source":"@site/docs/apache-spark/examples/python-spark-sql-01.mdx","sourceDirName":"apache-spark/examples","slug":"/apache-spark/examples/python-spark-sql-01","permalink":"/docs/apache-spark/examples/python-spark-sql-01","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":140,"frontMatter":{"sidebar_position":140},"sidebar":"tutorialSidebar","previous":{"title":"Python Spark DataFrame","permalink":"/docs/apache-spark/examples/python-spark-DataFrame"},"next":{"title":"Python Spark SQL 02","permalink":"/docs/apache-spark/examples/python-spark-sql-02"}}');var a=s(4848),r=s(8453);const i={sidebar_position:140},p="Python Spark SQL 01",o={},l=[{value:"Description",id:"description",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"python-spark-sql-01.py",id:"python-spark-sql-01py",level:2},{value:"Run",id:"run",level:2},{value:"Output",id:"output",level:4}];function h(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"python-spark-sql-01",children:"Python Spark SQL 01"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html",children:(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"Spark SQL"})})})," ",(0,a.jsx)("br",{})]}),"\n",(0,a.jsx)(n.h2,{id:"description",children:"Description"}),"\n",(0,a.jsx)("img",{src:"/img/streams/spark/07-sql-dataframe-picture.png",width:"300 px",alt:"07-sql-dataframe-picture.png"}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsx)(n.p,{children:"Spark lets you use the programmatic API, the SQL API, or a combination of both. This flexibility makes Spark accessible to a variety of users and powerfully expressive."}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Python"}),"\n",(0,a.jsx)(n.li,{children:"Spark"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Go to ",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"spark/my-examples"})})," and create:"]}),"\n",(0,a.jsx)(n.h2,{id:"python-spark-sql-01py",children:"python-spark-sql-01.py"}),"\n",(0,a.jsxs)(n.p,{children:["Let\u2019s persist the DataFrame in a named ",(0,a.jsx)(n.a,{href:"https://parquet.apache.org/",children:(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"Parquet"})})})," table that is easily accessible via the SQL API. (",(0,a.jsx)(n.em,{children:'df1.write.saveAsTable("some_people")'})," - ",(0,a.jsx)(n.em,{children:"python-spark-sql-01.py"}),")"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'print("Create a Spark DataFrame")\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName("demo").getOrCreate()\n\nfrom pyspark.sql.functions import col, when\n\ndf = spark.createDataFrame(\n    [\n        ("sue", 32),\n        ("li", 3),\n        ("bob", 75),\n        ("heo", 13),\n    ],\n    ["first_name", "age"],\n)\n\ndf1 = df.withColumn(\n    "life_stage",\n    when(col("age") < 13, "child")\n    .when(col("age").between(13, 19), "teenager")\n    .otherwise("adult"),\n)\ndf1.show()\n\nprint("Let\u2019s persist the DataFrame")\ndf1.write.saveAsTable("some_people")\nprint("...\\n...")\n\nprint("\\n\\nMake sure that the table is accessible via the table name \'select * from some_people\':")\nspark.sql("select * from some_people").show()\n\nprint("Now, let\u2019s use SQL to insert a few more rows of data into the table:")\nspark.sql("INSERT INTO some_people VALUES (\'frank\', 4, \'child\')")\nprint("...\\n...")\n\nprint("Inspect the table contents to confirm the row was inserted:")\nspark.sql("select * from some_people").show()\n\nprint("Run a query that returns the teenagers:")\nspark.sql("select * from some_people where life_stage=\'teenager\'").show()\n\nprint("\\n\\nstop the spark session")\nspark.stop()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"run",children:"Run"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"./bin/spark-submit --master local[4] ./my-examples/python-spark-sql-01.py\n"})}),"\n",(0,a.jsx)(n.h4,{id:"output",children:"Output"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Create a Spark DataFrame\n+----------+---+----------+\n|first_name|age|life_stage|\n+----------+---+----------+\n|       sue| 32|     adult|\n|        li|  3|     child|\n|       bob| 75|     adult|\n|       heo| 13|  teenager|\n+----------+---+----------+\n\nLet\u2019s persist the DataFrame\n...\n...\n\n\nMake sure that the table is accessible via the table name 'select * from some_people':\n+----------+---+----------+\n|first_name|age|life_stage|\n+----------+---+----------+\n|       heo| 13|  teenager|\n|       bob| 75|     adult|\n|       sue| 32|     adult|\n|        li|  3|     child|\n+----------+---+----------+\n\nNow, let\u2019s use SQL to insert a few more rows of data into the table:\n...\n...\nInspect the table contents to confirm the row was inserted:\n+----------+---+----------+\n|first_name|age|life_stage|\n+----------+---+----------+\n|       heo| 13|  teenager|\n|     frank|  4|     child|\n|       bob| 75|     adult|\n|       sue| 32|     adult|\n|        li|  3|     child|\n+----------+---+----------+\n\nRun a query that returns the teenagers:\n+----------+---+----------+\n|first_name|age|life_stage|\n+----------+---+----------+\n|       heo| 13|  teenager|\n+----------+---+----------+\n\n\n\nstop the spark session\n"})})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>p});var t=s(6540);const a={},r=t.createContext(a);function i(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function p(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);