"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[9526],{7834:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var s=i(4848),t=i(8453);const r={sidebar_position:120},a="K-Nearest Neighbor(KNN)",o={id:"auxiliary-software/artificial-intelligence/machine-learning/classification/knn",title:"K-Nearest Neighbor(KNN)",description:":",source:"@site/docs/auxiliary-software/artificial-intelligence/machine-learning/classification/knn.mdx",sourceDirName:"auxiliary-software/artificial-intelligence/machine-learning/classification",slug:"/auxiliary-software/artificial-intelligence/machine-learning/classification/knn",permalink:"/docs/auxiliary-software/artificial-intelligence/machine-learning/classification/knn",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:120,frontMatter:{sidebar_position:120},sidebar:"tutorialSidebar",previous:{title:"Binary Classification",permalink:"/docs/auxiliary-software/artificial-intelligence/machine-learning/classification/binary-classification"},next:{title:"Clustering",permalink:"/docs/auxiliary-software/artificial-intelligence/machine-learning/clustering/"}},l={},c=[{value:"<strong>How KNN Works</strong>",id:"how-knn-works",level:3},{value:"<strong>Distance Metrics Used</strong>",id:"distance-metrics-used",level:3},{value:"<strong>Pros and Cons of KNN</strong>",id:"pros-and-cons-of-knn",level:3},{value:"<strong>Choosing the Best K</strong>",id:"choosing-the-best-k",level:3},{value:"<strong>Example in Python (Using Scikit-Learn)</strong>",id:"example-in-python-using-scikit-learn",level:3},{value:"K-Nearest Neighbor(KNN) Algorithm",id:"k-nearest-neighborknn-algorithm",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"k-nearest-neighborknn",children:"K-Nearest Neighbor(KNN)"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)("img",{src:"/img/ai/ai-001.png",width:"24 px",alt:"ai-001.png"}),":",(0,s.jsx)("br",{}),"\nK-Nearest Neighbors (KNN) is a simple and widely used machine learning algorithm for classification and regression tasks. It is a ",(0,s.jsx)(n.strong,{children:"lazy learning"})," algorithm, meaning it does not learn a model during training but rather memorizes the dataset and makes predictions based on similarity."]}),"\n",(0,s.jsx)(n.h3,{id:"how-knn-works",children:(0,s.jsx)(n.strong,{children:"How KNN Works"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Choose K"}),": Select the number of nearest neighbors (K)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Calculate Distance"}),": Compute the distance between the new data point and all points in the dataset (commonly using Euclidean distance)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Find Nearest Neighbors"}),": Select the K closest data points."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Make Prediction"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Classification"}),": Assign the most common class among the K neighbors."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Regression"}),": Compute the average of the K neighbors' values."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"distance-metrics-used",children:(0,s.jsx)(n.strong,{children:"Distance Metrics Used"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Euclidean Distance"}),": ",(0,s.jsx)(n.code,{children:"( d = \\sqrt{\\sum (x_i - y_i)^2} \\)"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manhattan Distance"}),": ",(0,s.jsx)(n.code,{children:"( d = \\sum |x_i - y_i| \\)"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Minkowski Distance"}),": A generalized form of both Euclidean and Manhattan distances."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"pros-and-cons-of-knn",children:(0,s.jsx)(n.strong,{children:"Pros and Cons of KNN"})}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Advantages:"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Simple and easy to implement."}),"\n",(0,s.jsx)(n.li,{children:"No need for training (instance-based learning)."}),"\n",(0,s.jsx)(n.li,{children:"Works well with small datasets."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u274c ",(0,s.jsx)(n.strong,{children:"Disadvantages:"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Computationally expensive for large datasets."}),"\n",(0,s.jsx)(n.li,{children:"Performance depends on the choice of K."}),"\n",(0,s.jsx)(n.li,{children:"Sensitive to irrelevant or redundant features."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"choosing-the-best-k",children:(0,s.jsx)(n.strong,{children:"Choosing the Best K"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A small K (e.g., 1 or 3) makes the model sensitive to noise."}),"\n",(0,s.jsx)(n.li,{children:"A large K smooths decision boundaries but may ignore patterns."}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cross-validation"})," helps find the optimal K."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-in-python-using-scikit-learn",children:(0,s.jsx)(n.strong,{children:"Example in Python (Using Scikit-Learn)"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\n\n# Load dataset\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\n# Train KNN model\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\n# Make predictions\ny_pred = knn.predict(X_test)\n\n# Evaluate model\nfrom sklearn.metrics import accuracy_score\nprint("Accuracy:", accuracy_score(y_test, y_pred))\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.admonition,{title:"citation",type:"note",children:[(0,s.jsx)(n.h3,{id:"k-nearest-neighborknn-algorithm",children:"K-Nearest Neighbor(KNN) Algorithm"}),(0,s.jsx)(n.p,{children:"K-Nearest Neighbors (KNN) is a simple way to classify things by looking at what\u2019s nearby. Imagine a streaming service wants to predict if a new user is likely to cancel their subscription (churn) based on their age. They checks the ages of its existing users and whether they churned or stayed. If most of the \u201cK\u201d closest users in age of new user canceled their subscription KNN will predict the new user might churn too. The key idea is that users with similar ages tend to have similar behaviors and KNN uses this closeness to make decisions."}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"https://www.geeksforgeeks.org/k-nearest-neighbours/",children:"https://www.geeksforgeeks.org/k-nearest-neighbours/"})})})})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);