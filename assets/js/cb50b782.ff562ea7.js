"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[2592],{3824:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"apache-hadoop/Examples/mapreduce-inverted-index","title":"MapReduce: Inverted Index","description":"Based on...","source":"@site/docs/apache-hadoop/Examples/mapreduce-inverted-index.mdx","sourceDirName":"apache-hadoop/Examples","slug":"/apache-hadoop/Examples/mapreduce-inverted-index","permalink":"/docs/apache-hadoop/Examples/mapreduce-inverted-index","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":300,"frontMatter":{"sidebar_position":300},"sidebar":"tutorialSidebar","previous":{"title":"MapReduce: the Second Program","permalink":"/docs/apache-hadoop/Examples/mapreduce-second-program"},"next":{"title":"MapReduce: Matrix-Vector Multiplication","permalink":"/docs/apache-hadoop/Examples/mapreduce-matrix-vector-multiplication"}}');var a=t(4848),i=t(8453);const o={sidebar_position:300},s="MapReduce: Inverted Index",d={},l=[{value:"Description",id:"description",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"HDFS",id:"hdfs",level:2},{value:"Create files in HDFS",id:"create-files-in-hdfs",level:3},{value:"Create local:",id:"create-local",level:4},{value:"Copy the above local files to HDFS file system",id:"copy-the-above-local-files-to-hdfs-file-system",level:4},{value:"Java Program",id:"java-program",level:2},{value:"pom.xml",id:"pomxml",level:4},{value:"InvertedIndex.java",id:"invertedindexjava",level:4},{value:"Driver.java",id:"driverjava",level:4},{value:"Build jar",id:"build-jar",level:4},{value:"Run jar",id:"run-jar",level:3},{value:"Verify result",id:"verify-result",level:4},{value:"Output:",id:"output",level:4},{value:"Display content",id:"display-content",level:4},{value:"Output:",id:"output-1",level:4}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"mapreduce-inverted-index",children:"MapReduce: Inverted Index"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://github.com/caizkun/mapreduce-examples/tree/master/InvertedIndex",children:(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"Based on..."})})})}),"\n",(0,a.jsx)(n.h2,{id:"description",children:"Description"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Inverted index"})," maps terms (words) to their locations in a set of documents. This type of index allows for fast full-text searches, making it ideal for applications like search engines and document retrieval systems."]}),"\n",(0,a.jsx)(n.p,{children:"This Example is created for looking up the words in files."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Workflow:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Mapper"}),": fetch the file name of each record and split the record into words\ninput: ",(0,a.jsx)(n.code,{children:"<offset, line>"}),"\noutput: ",(0,a.jsx)(n.code,{children:"<word, fileName>"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Reducer:"})," sum up the count for each word\ninput: ",(0,a.jsx)(n.code,{children:"<word, (file1, file2, file1, ...)>"}),"\noutput: ",(0,a.jsx)(n.code,{children:"<word, (file1=count1, file2=count2, ...)>"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Linux"}),"\n",(0,a.jsx)(n.li,{children:"JDK 11"}),"\n",(0,a.jsx)(n.li,{children:"Maven"}),"\n",(0,a.jsx)(n.li,{children:"Hadoop 3.4.1"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"hdfs",children:"HDFS"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"/docs/apache-hadoop/Examples/mapreduce-first-program",children:(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"Explanations"})})})}),"\n",(0,a.jsx)(n.h3,{id:"create-files-in-hdfs",children:"Create files in HDFS"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"su - hadoop\nstart-dfs.sh\nstart-yarn.sh\n"})}),"\n",(0,a.jsx)(n.h4,{id:"create-local",children:"Create local:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"/home/hadoop/examples/InvertedIndex/input/sample1.txt\n\nwith data:\n\nI love big data\nFinally AI\n"})}),"\n",(0,a.jsx)(n.p,{children:"and"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"/home/hadoop/examples/InvertedIndex/input/sample2.txt\n\nwith data:\n\nI love hello world\nFinally AI\n"})}),"\n",(0,a.jsx)(n.h4,{id:"copy-the-above-local-files-to-hdfs-file-system",children:"Copy the above local files to HDFS file system"}),"\n",(0,a.jsxs)(n.p,{children:["and",(0,a.jsx)("br",{}),"\ncheck if both files have been copied:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"hdfs dfs -ls /examples/InvertedIndex/input/\n"})}),"\n",(0,a.jsx)(n.h2,{id:"java-program",children:"Java Program"}),"\n",(0,a.jsx)(n.h4,{id:"pomxml",children:"pom.xml"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>org.example</groupId>\n    <artifactId>InvertedIndex</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <target.java.version>11</target.java.version>\n        <maven.compiler.source>${target.java.version}</maven.compiler.source>\n        <maven.compiler.target>${target.java.version}</maven.compiler.target>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    </properties>\n\n    <dependencies>\n        \x3c!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core --\x3e\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-mapreduce-client-core</artifactId>\n            <version>3.4.1</version>\n        </dependency>\n\n        \x3c!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core --\x3e\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-core</artifactId>\n            <version>1.2.1</version>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n\n            \x3c!-- Java Compiler --\x3e\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.1</version>\n                <configuration>\n                    <source>${target.java.version}</source>\n                    <target>${target.java.version}</target>\n                </configuration>\n            </plugin>\n\n            \x3c!-- We use the maven-shade plugin to create a fat jar that contains all necessary dependencies. --\x3e\n            \x3c!-- Change the value of <mainClass>...</mainClass> if your program entry point changes. --\x3e\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>3.0.0</version>\n                <executions>\n                    \x3c!-- Run shade goal on package phase --\x3e\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <artifactSet>\n                                <excludes>\n                                    <exclude>org.apache.flink:flink-shaded-force-shading</exclude>\n                                    <exclude>com.google.code.findbugs:jsr305</exclude>\n                                    <exclude>org.slf4j:*</exclude>\n                                    <exclude>org.apache.logging.log4j:*</exclude>\n                                </excludes>\n                            </artifactSet>\n                            <filters>\n                                <filter>\n                                    \x3c!-- Do not copy the signatures in the META-INF folder.\n                                    Otherwise, this might cause SecurityExceptions when using the JAR. --\x3e\n                                    <artifact>*:*</artifact>\n                                    <excludes>\n                                        <exclude>META-INF/*.SF</exclude>\n                                        <exclude>META-INF/*.DSA</exclude>\n                                        <exclude>META-INF/*.RSA</exclude>\n                                    </excludes>\n                                </filter>\n                            </filters>\n                            <transformers>\n                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">\n                                    <mainClass>Driver</mainClass>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n\n    </build>\n</project>\n'})}),"\n",(0,a.jsx)(n.h4,{id:"invertedindexjava",children:"InvertedIndex.java"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'import org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileSplit;\n\nimport java.io.IOException;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class InvertedIndex {\n\n    public static class InvertedIndexMapper extends Mapper<Object, Text, Text, Text> {\n\n        @Override\n        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n            // input: <offset, line>\n            // output: <word, fileName>\n\n            // split this record into words\n            String[] words = value.toString().trim().split("\\\\s+");\n\n            // fetch the file name of this record\n            String fileName = ((FileSplit) context.getInputSplit()).getPath().getName();\n\n            for (String word : words) {\n                context.write(new Text(word.toLowerCase()), new Text(fileName));\n            }\n        }\n    }\n\n    public static class InvertedIndexReducer extends Reducer<Text, Text, Text, Text> {\n\n        @Override\n        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {\n            // input: <word, (file1, file2, file1, ...)>\n            // output: <word, {file1=count1, file2=count2, ...)>\n\n            Map<String, Integer> hist = new HashMap<String, Integer>();\n\n            for (Text value : values) {\n                String fileName = value.toString();\n\n                if (hist.containsKey(fileName)) {\n                    hist.put(fileName, hist.get(fileName) + 1);\n                } else {\n                    hist.put(fileName, 1);\n                }\n            }\n\n            context.write(key, new Text(hist.toString()));\n        }\n    }\n\n}\n'})}),"\n",(0,a.jsx)(n.h4,{id:"driverjava",children:"Driver.java"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'import org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\n\n/*\n * This driver implements the Tool interface (which calls GenericOptionsParser)\n * to help interpret common Hadoop command-line options.\n *\n * All implementations of Tool need to implement Configurable (since Tool extends it),\n * and the easiest way to do this is subclassing Configured.\n *\n * ToolRunner is a utility to help run Tool. It can be used to run classes implementing Tool interface.\n * It works in conjunction with GenericOptionsParser to parse the generic hadoop command line\n * arguments and modifies the Configuration of the Tool. The application-specific options are passed along\n * without being modified.\n *\n *\n * Although it is unnecessary in this example, this driver format is very useful for customizing configuration\n * at run time, not compile time!\n *\n */\n\npublic class Driver extends Configured implements Tool {\n\n    @Override\n    public int run(String[] args) throws Exception {\n        // check the run parameters\n        if (args.length != 2) {\n            System.err.printf("Usage: %s [generic options] <input> <output>\\n", getClass().getSimpleName());\n            ToolRunner.printGenericCommandUsage(System.err);\n            return -1;\n        }\n\n        // create a configuration\n        Configuration conf = getConf();\n\n        // instantiate a job\n        Job job = Job.getInstance(conf, "Inverted Index");\n\n        job.setJarByClass(InvertedIndex.class);\n        job.setMapperClass(InvertedIndex.InvertedIndexMapper.class);\n        job.setReducerClass(InvertedIndex.InvertedIndexReducer.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n\n        // specify io\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        return job.waitForCompletion(true)? 0 : 1;\n    }\n\n    public static void main(String[] args) throws Exception {\n        // use ToolRunner to run Tool\n        int exitCode = ToolRunner.run(new Driver(), args);\n        System.exit(exitCode);\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h4,{id:"build-jar",children:"Build jar"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"mvn clean package\n"})}),"\n",(0,a.jsx)(n.h3,{id:"run-jar",children:"Run jar"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"\nhadoop jar /home/hadoop/examples/InvertedIndex/InvertedIndex-1.0-SNAPSHOT.jar /examples/InvertedIndex/input/ /examples/InvertedIndex/output/\n"})}),"\n",(0,a.jsx)(n.h4,{id:"verify-result",children:"Verify result"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"hdfs dfs -ls /examples/InvertedIndex/output/\n"})}),"\n",(0,a.jsx)(n.h4,{id:"output",children:"Output:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"\n-rw-r--r--   1 hadoop supergroup          0 2025-03-13 13:52 /examples/InvertedIndex/output/_SUCCESS\n-rw-r--r--   1 hadoop supergroup        227 2025-03-13 13:52 /examples/InvertedIndex/output/part-r-00000\n"})}),"\n",(0,a.jsx)(n.h4,{id:"display-content",children:"Display content"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"hdfs dfs -cat /examples/InvertedIndex/output/part-r-00000\n"})}),"\n",(0,a.jsx)(n.h4,{id:"output-1",children:"Output:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"\nhadoop@LL01:~/hadoop-3.4.1/bin$ ./hdfs dfs -cat /examples/InvertedIndex/output/part-r-00000\nai\t{sample2.txt=1, sample1.txt=1}\nbig\t{sample1.txt=1}\ndata\t{sample1.txt=1}\nfinally\t{sample2.txt=1, sample1.txt=1}\nhello\t{sample2.txt=1}\ni\t{sample2.txt=1, sample1.txt=1}\nlove\t{sample2.txt=1, sample1.txt=1}\nworld\t{sample2.txt=1}\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsxs)(n.p,{children:["Source of the java program:",(0,a.jsx)("br",{}),"\n",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"https://github.com/ZbCiok/zjc-examples/tree/main/streams/hadoop/InvertedIndex",children:"https://github.com/ZbCiok/zjc-examples/tree/main/streams/hadoop/InvertedIndex"})})})]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var r=t(6540);const a={},i=r.createContext(a);function o(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);