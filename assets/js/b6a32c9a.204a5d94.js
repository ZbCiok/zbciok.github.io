"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[5284],{7718:(e,a,s)=>{s.r(a),s.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"apache-hadoop/hbase","title":"HBase","description":"https://hbase.apache.org/","source":"@site/docs/apache-hadoop/hbase.mdx","sourceDirName":"apache-hadoop","slug":"/apache-hadoop/hbase","permalink":"/docs/apache-hadoop/hbase","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":300,"frontMatter":{"sidebar_position":300},"sidebar":"tutorialSidebar","previous":{"title":"SQOOP","permalink":"/docs/apache-hadoop/sqoop"},"next":{"title":"HBase in Pseudo-Distributed Mode with Kerberos Authentication","permalink":"/docs/apache-hadoop/hbase-pseudo-distributed-mode-kerberos"}}');var t=s(4848),i=s(8453);const r={sidebar_position:300},o="HBase",l={},d=[{value:"Applications of Apache HBase:",id:"applications-of-apache-hbase",level:4},{value:"Features of HBase \u2013",id:"features-of-hbase-",level:4},{value:"Advantages Of Apache HBase:",id:"advantages-of-apache-hbase",level:4},{value:"Disadvantages Of Apache HBase:",id:"disadvantages-of-apache-hbase",level:4},{value:"HBase and HDFS",id:"hbase-and-hdfs",level:2},{value:"HBase Data Model",id:"hbase-data-model",level:2},{value:"Column-Oriented Databases",id:"column-oriented-databases",level:3},{value:"Table Example",id:"table-example",level:3},{value:"hbase:001:0&gt; scan &#39;customers-100&#39;:",id:"hbase0010-scan-customers-100",level:4},{value:"HBase Components",id:"hbase-components",level:2},{value:"- HMaster",id:"--hmaster",level:3},{value:"- Region Server",id:"--region-server",level:3},{value:"- Zookeeper",id:"--zookeeper",level:3},{value:"- How the Components Work Together",id:"--how-the-components-work-together",level:3},{value:"Install HBase",id:"install-hbase",level:2},{value:"Standalone Mode / Pseudo-Distributed Mode:",id:"standalone-mode--pseudo-distributed-mode",level:2},{value:"Start Sequence:",id:"start-sequence",level:3},{value:"Create a table",id:"create-a-table",level:2},{value:"put data into your table",id:"put-data-into-your-table",level:3},{value:"Get a single row of data.",id:"get-a-single-row-of-data",level:4},{value:"Ending Sequence:",id:"ending-sequence",level:3},{value:"Pseudo-Distributed Mode:",id:"pseudo-distributed-mode",level:2},{value:"Output",id:"output",level:4},{value:"Master Status:",id:"master-status",level:2}];function h(e){const a={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.header,{children:(0,t.jsx)(a.h1,{id:"hbase",children:"HBase"})}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:(0,t.jsx)(a.a,{href:"https://hbase.apache.org/",children:"https://hbase.apache.org/"})})}),(0,t.jsx)("br",{}),"\n",(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:(0,t.jsx)(a.a,{href:"https://hbase.apache.org/book.html",children:"https://hbase.apache.org/book.html"})})})," ",(0,t.jsx)("br",{}),"\n",(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:(0,t.jsx)(a.a,{href:"https://hbase.apache.org/book.html#quickstart_pseudo",children:"https://hbase.apache.org/book.html#quickstart_pseudo"})})})," ",(0,t.jsx)("br",{}),"\n",(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:(0,t.jsx)(a.a,{href:"https://www.tutorialspoint.com/hbase/hbase_overview.htm",children:"https://www.tutorialspoint.com/hbase/hbase_overview.htm"})})}),(0,t.jsx)("br",{}),"\n",(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:(0,t.jsx)(a.a,{href:"https://www.cloudduggu.com/hbase/introduction/",children:"https://www.cloudduggu.com/hbase/introduction/"})})}),(0,t.jsx)("br",{}),"\n",(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:(0,t.jsx)(a.a,{href:"https://www.edureka.co/blog/hbase-architecture/",children:"https://www.edureka.co/blog/hbase-architecture/"})})})," ",(0,t.jsx)("br",{}),"\n",(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:(0,t.jsx)(a.a,{href:"https://nag-9-s.gitbook.io/hbase/hbase-architecture",children:"https://nag-9-s.gitbook.io/hbase/hbase-architecture"})})})," ",(0,t.jsx)("br",{}),"\n",(0,t.jsx)(a.a,{href:"https://www.xenonstack.com/insights/apache-hbase",children:(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:"Apache Hbase Security with Kerberos | Complete Guide"})})})," ",(0,t.jsx)("br",{}),"\n",(0,t.jsx)(a.a,{href:"http://39.129.231.25:7180/static/help/topics/cdh_ig_hbase_pseudo_configure.html",children:(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:"Enabling Servers for Pseudo-distributed Operation"})})})," ",(0,t.jsx)("br",{})]}),"\n",(0,t.jsxs)(a.admonition,{type:"note",children:[(0,t.jsx)(a.mdxAdmonitionTitle,{children:(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:(0,t.jsx)(a.a,{href:"https://www.linkedin.com/in/rohit-singh-5a497648/",children:"https://www.linkedin.com/in/rohit-singh-5a497648/"})})})}),(0,t.jsx)(a.p,{children:"Apache HBase is an open-source, NoSQL, distributed big data store. It enables random, strictly consistent, real-time access to petabytes of data. HBase is very effective for handling large, sparse datasets. HBase is a data model that is similar to Google\u2019s big table. It is an open source, distributed database developed by Apache software foundation written in Java. HBase is an essential part of our Hadoop ecosystem. HBase runs on top of HDFS (Hadoop Distributed File System). It can store massive amounts of data from terabytes to petabytes. It is column oriented and horizontally scalable."}),(0,t.jsx)(a.h4,{id:"applications-of-apache-hbase",children:"Applications of Apache HBase:"}),(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"Real-time analytics:"})," HBase is an excellent choice for real-time analytics applications that require low-latency data access. It provides fast read and write performance and can handle large amounts of data, making it suitable for real-time data analysis."]}),(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"Social media applications:"})," HBase is an ideal database for social media applications that require high scalability and performance. It can handle the large volume of data generated by social media platforms and provide real-time analytics capabilities."]}),(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"IoT applications:"})," HBase can be used for Internet of Things (IoT) applications that require storing and processing large volumes of sensor data. HBase\u2019s scalable architecture and fast write performance make it a suitable choice for IoT applications that require low-latency data processing."]}),(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"Online transaction processing:"})," HBase can be used as an online transaction processing (OLTP) database, providing high availability, consistency, and low-latency data access. HBase\u2019s distributed architecture and automatic failover capabilities make it a good fit for OLTP applications that require high availability."]}),(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"Ad serving and clickstream analysis:"})," HBase can be used to store and process large volumes of clickstream data for ad serving and clickstream analysis. HBase\u2019s column-oriented data storage and indexing capabilities make it a good fit for these types of applications."]}),(0,t.jsx)(a.h4,{id:"features-of-hbase-",children:"Features of HBase \u2013"}),(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"It is linearly scalable across various nodes as well as modularly scalable, as it divided across various nodes."}),"\n",(0,t.jsx)(a.li,{children:"HBase provides consistent read and writes."}),"\n",(0,t.jsx)(a.li,{children:"It provides atomic read and write means during one read or write process, all other processes are prevented from performing any read or write operations."}),"\n",(0,t.jsx)(a.li,{children:"It provides easy to use Java API for client access."}),"\n",(0,t.jsx)(a.li,{children:"It supports Thrift and REST API for non-Java front ends which supports XML, Protobuf and binary data encoding options."}),"\n",(0,t.jsx)(a.li,{children:"It supports a Block Cache and Bloom Filters for real-time queries and for high volume query optimization."}),"\n",(0,t.jsx)(a.li,{children:"HBase provides automatic failure support between Region Servers."}),"\n",(0,t.jsx)(a.li,{children:"It support for exporting metrics with the Hadoop metrics subsystem to files."}),"\n",(0,t.jsx)(a.li,{children:"It doesn\u2019t enforce relationship within your data."}),"\n",(0,t.jsx)(a.li,{children:"It is a platform for storing and retrieving data with random access."}),"\n"]}),(0,t.jsx)(a.h4,{id:"advantages-of-apache-hbase",children:"Advantages Of Apache HBase:"}),(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"Scalability: HBase can handle extremely large datasets that can be distributed across a cluster of machines. It is designed to scale horizontally by adding more nodes to the cluster, which allows it to handle increasingly larger amounts of data."}),"\n",(0,t.jsx)(a.li,{children:"High-performance: HBase is optimized for low-latency, high-throughput access to data. It uses a distributed architecture that allows it to process large amounts of data in parallel, which can result in faster query response times."}),"\n",(0,t.jsx)(a.li,{children:"Flexible data model: HBase\u2019s column-oriented data model allows for flexible schema design and supports sparse datasets. This can make it easier to work with data that has a variable or evolving schema."}),"\n",(0,t.jsx)(a.li,{children:"Fault tolerance: HBase is designed to be fault-tolerant by replicating data across multiple nodes in the cluster. This helps ensure that data is not lost in the event of a hardware or network failure."}),"\n"]}),(0,t.jsx)(a.h4,{id:"disadvantages-of-apache-hbase",children:"Disadvantages Of Apache HBase:"}),(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"Complexity: HBase can be complex to set up and manage. It requires knowledge of the Hadoop ecosystem and distributed systems concepts, which can be a steep learning curve for some users."}),"\n",(0,t.jsx)(a.li,{children:"Limited query language: HBase\u2019s query language, HBase Shell, is not as feature-rich as SQL. This can make it difficult to perform complex queries and analyses.\n-No support for transactions: HBase does not support transactions, which can make it difficult to maintain data consistency in some use cases."}),"\n",(0,t.jsx)(a.li,{children:"Not suitable for all use cases: HBase is best suited for use cases where high throughput and low-latency access to large datasets is required. It may not be the best choice for applications that require real-time processing or strong consistency guarantees."}),"\n"]})]}),"\n",(0,t.jsx)(a.h2,{id:"hbase-and-hdfs",children:"HBase and HDFS"}),"\n",(0,t.jsx)(a.p,{children:"In most cases, HBase stores its data in Apache HDFS. This includes the HFiles containing the data, as well as the write-ahead logs (WALs) which store data before it is written to the HFiles and protect against RegionServer crashes. HDFS provides reliability and protection to data in HBase because it is distributed. To operate with the most efficiency, HBase needs data to be available locally. Therefore, it is a good practice to run an HDFS DataNode on each RegionServer."}),"\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/hbase-hdfs-01.png",alt:"hbase-hdfs-01.png"}),"\n",(0,t.jsx)(a.h2,{id:"hbase-data-model",children:"HBase Data Model"}),"\n",(0,t.jsx)(a.p,{children:"HBase is a column-oriented NoSQL database. Although it looks similar to a relational database which contains rows and columns, but it is not a relational database. Relational databases are row oriented while HBase is column-oriented."}),"\n",(0,t.jsx)(a.h3,{id:"column-oriented-databases",children:"Column-Oriented Databases"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"When the amount of data is very huge, like in terms of petabytes or exabytes, we use column-oriented approach, because the data of a single column is stored together and can be accessed faster."}),"\n",(0,t.jsx)(a.li,{children:"While row-oriented approach comparatively handles less number of rows and columns efficiently, as row-oriented database stores data is a structured format."}),"\n",(0,t.jsx)(a.li,{children:"When we need to process and analyze a large set of semi-structured or unstructured data, we use column oriented approach. Such as applications dealing with Online Analytical Processing like data mining, data warehousing, applications including analytics, etc."}),"\n",(0,t.jsx)(a.li,{children:"Whereas, Online Transactional Processing such as banking and finance domains which handle structured data and require transactional properties (ACID properties) use row-oriented approach."}),"\n"]}),"\n",(0,t.jsx)(a.p,{children:"HBase tables has following components, shown in the image below:"}),"\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/hbase-column-oriented-db-01.png",alt:"hbase-column-oriented-db-01.png"}),"\n",(0,t.jsx)(a.h3,{id:"table-example",children:"Table Example"}),"\n",(0,t.jsx)(a.h4,{id:"hbase0010-scan-customers-100",children:"hbase:001:0> scan 'customers-100':"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"hbase:001:0> scan 'customers-100'\nROW                                                   COLUMN+CELL\n 1                                                    column=City:, timestamp=2025-03-22T16:08:00.164, value=East Leonard\n 1                                                    column=Company:, timestamp=2025-03-22T16:08:00.164, value=Rasmussen Group\n 1                                                    column=Country:, timestamp=2025-03-22T16:08:00.164, value=Chile\n 1                                                    column=Customer_Id:, timestamp=2025-03-22T16:08:00.164, value=DD37Cf93aecA6Dc\n 1                                                    column=Email:, timestamp=2025-03-22T16:08:00.164, value=zunigavanessa@smith.info\n 1                                                    column=First_Name:, timestamp=2025-03-22T16:08:00.164, value=Sheryl\n 1                                                    column=Last_Name:, timestamp=2025-03-22T16:08:00.164, value=Baxter\n 1                                                    column=Phone_1:, timestamp=2025-03-22T16:08:00.164, value=229.077.5154\n 1                                                    column=Phone_2:, timestamp=2025-03-22T16:08:00.164, value=397.884.0519x718\n 1                                                    column=Subscription_Date:, timestamp=2025-03-22T16:08:00.164, value=2020-08-24\n 1                                                    column=Website:, timestamp=2025-03-22T16:08:00.164, value=http://www.stephenson.com/\n 10                                                   column=City:, timestamp=2025-03-22T16:08:00.164, value=Elaineberg\n 10                                                   column=Company:, timestamp=2025-03-22T16:08:00.164, value=Beck-Hendrix\n 10                                                   column=Country:, timestamp=2025-03-22T16:08:00.164, value=Timor-Leste\n\n ...\n ...\n"})}),"\n",(0,t.jsx)(a.h2,{id:"hbase-components",children:"HBase Components"}),"\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/hbase-components-01.png",alt:"hbase-components-01.png"}),"\n",(0,t.jsx)(a.h3,{id:"--hmaster",children:"- HMaster"}),"\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/hbase-hmaster-01.png",alt:"hbase-hmaster-01.png"}),"\n",(0,t.jsx)(a.p,{children:"The implementation of Master Server in HBase is HMaster. It is a process in which regions are assigned to region server as well as DDL (create, delete table) operations. It monitor all Region Server instances present in the cluster. In a distributed environment, Master runs several background threads. HMaster has many features like controlling load balancing, failover etc."}),"\n",(0,t.jsx)(a.h3,{id:"--region-server",children:"- Region Server"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Regions"})," - HBase Tables are divided horizontally by row key range into \u201cRegions.\u201d","\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"A region contains all rows in the table between the region\u2019s start key and end key."}),"\n",(0,t.jsx)(a.li,{children:"Regions are assigned to the nodes in the cluster, called \u201cRegion Servers,\u201d and these serve data for reads and writes."}),"\n",(0,t.jsx)(a.li,{children:"A region server can serve about 1,000 regions."}),"\n",(0,t.jsx)(a.li,{children:"Each region is 1GB in size (default)"}),"\n"]}),"\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/hbase-region-servers-01.png",alt:"hbase-region-servers-01.png"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsxs)(a.li,{children:["The basic unit of scalability and load balancing in HBase is called a region. These are essentially contiguous ranges of rows stored together. They are dynamically split by the system when they become too large. Alternatively, they may also be merged to reduce their number and required storage files. An HBase system ma y have more than one region servers.","\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"Initially there is only one region for a table and as we start adding data to it, the system is monitoring to ensure that you do not exceed a configured maximum size. If you exceed the limit, the region is split into two at the middle key middle of the region, creating two roughly equal halves."}),"\n",(0,t.jsx)(a.li,{children:"Each region is served by exactly one region server, the row key in the and each of these servers can serve many regions at any time."}),"\n",(0,t.jsx)(a.li,{children:"Rows are grouped in regions and may be served by different servers"}),"\n"]}),"\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/hbase-region-server-01.png",alt:"hbase-region-server-01.png"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"HFile"})," - HFile represents the real data storage file. The files contain a variable number of data blocks and a fixed number of file info blocks and trailer blocks."]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"WAL(HLog)"})," -","\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"In the case of writing the data, when the client calls HTable.put(Put), the data is  frst written to the write-ahead log fle (which contains actual data and sequence  numbers together represented by the HLogKey class) and also written in MemStore."}),"\n",(0,t.jsx)(a.li,{children:"Writing data directly into MemStrore can be dangerous as it is a volatile in-memory  buffer and always open to the risk of losing data in case of a server failure."}),"\n",(0,t.jsx)(a.li,{children:"Once MemStore is full, the contents of the MemStore are \ufb02ushed to the disk by creating a new HFile on the HDFS."}),"\n",(0,t.jsx)(a.li,{children:"If there is a server failure, the WAL can effectively retrieve the log to get everything up to where the server was prior to the crash failure. Hence, the WAL guarantees that the data is never lost."}),"\n",(0,t.jsx)(a.li,{children:"Also, as another level of assurance, the actual write-ahead log resides on the HDFS, which is a replicated filesystem. Any other server having a replicated copy can open the log."}),"\n",(0,t.jsx)(a.li,{children:"The HLog class represents the WAL. When an HRegion object is instantiated, the single HLog instance is passed on as a parameter to the constructor of HRegion. In the case of an update operation, it saves the data directly to the shared WAL and also keeps track of the changes by incrementing the sequence numbers for each edit."}),"\n",(0,t.jsx)(a.li,{children:"WAL uses a Hadoop SequenceFile, which stores records as sets of key-value pairs."}),"\n",(0,t.jsx)(a.li,{children:"Here, the HLogKey instance represents the key, and the key-value represents the rowkey, column family, column qualifier, timestamp, type, and value along with the region and table name where data needs to be stored."}),"\n",(0,t.jsx)(a.li,{children:"Also, the structure starts with two fixed-length numbers that indicate the size and value of the key."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(a.h3,{id:"--zookeeper",children:"- Zookeeper"}),"\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/hbase-zookeeper-01.png",alt:"hbase-zookeeper-01.png"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"Zookeeper acts like a coordinator inside HBase distributed environment. It helps in maintaining server state inside the cluster by communicating through sessions."}),"\n",(0,t.jsx)(a.li,{children:"Every Region Server along with HMaster Server sends continuous heartbeat at regular interval to Zookeeper and it checks which server is alive and available as mentioned in above image. It also provides server failure notifications so that, recovery measures can be executed."}),"\n",(0,t.jsx)(a.li,{children:"Referring from the above image you can see, there is an inactive server, which acts as a backup for active server. If the active server fails, it comes for the rescue."}),"\n",(0,t.jsx)(a.li,{children:"The active HMaster sends heartbeats to the Zookeeper while the inactive HMaster listens for the notification send by active HMaster. If the active HMaster fails to send a heartbeat the session is deleted and the inactive HMaster becomes active."}),"\n",(0,t.jsx)(a.li,{children:"While if a Region Server fails to send a heartbeat, the session is expired and all listeners are notified about it. Then HMaster performs suitable recovery actions which we will discuss later in this blog."}),"\n",(0,t.jsx)(a.li,{children:"Zookeeper also maintains the .META Server\u2019s path, which helps any client in searching for any region. The Client first has to check with .META Server in which Region Server a region belongs, and it gets the path of that Region Server."}),"\n"]}),"\n",(0,t.jsx)(a.h3,{id:"--how-the-components-work-together",children:"- How the Components Work Together"}),"\n",(0,t.jsx)(a.p,{children:"Zookeeper is used to coordinate shared state information for members of distributed systems. Region servers and the active HMaster connect with a session to ZooKeeper. The ZooKeeper maintains ephemeral nodes for active sessions via heartbeats."}),"\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/hbase-components-work-together-01.png",alt:"hbase-components-work-together-01.png"}),"\n",(0,t.jsx)(a.p,{children:"Each Region Server creates an ephemeral node. The HMaster monitors these nodes to discover available region servers, and it also monitors these nodes for server failures. HMasters vie to create an ephemeral node. Zookeeper determines the first one and uses it to make sure that only one master is active. The active HMaster sends heartbeats to Zookeeper, and the inactive HMaster listens for notifications of the active HMaster failure."}),"\n",(0,t.jsx)(a.p,{children:"If a region server or the active HMaster fails to send a heartbeat, the session is expired and the corresponding ephemeral node is deleted. Listeners for updates will be notified of the deleted nodes. The active HMaster listens for region servers, and will recover region servers on failure. The Inactive HMaster listens for active HMaster failure, and if an active HMaster fails, the inactive HMaster becomes active."}),"\n",(0,t.jsx)(a.hr,{}),"\n",(0,t.jsx)(a.h2,{id:"install-hbase",children:"Install HBase"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.a,{href:"/docs/apache-hadoop/install",children:(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:"Introductory Information"})})})," ",(0,t.jsx)("br",{})]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"su - hadoop\n"})}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:"/home/hadoop"})}),": ",(0,t.jsx)("br",{}),"\nDownload and untar ",(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:"hbase-2.5.11-bin.tar.gz"})})," . ",(0,t.jsx)("br",{})]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"wget http://apache.mirror.gtcomm.net/hbase/stable/hbase-2.5.11-bin.tar.gz\ntar xvf hbase-2.5.11-bin.tar.gz\n"})}),"\n",(0,t.jsxs)(a.p,{children:["Rename ",(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:"hbase-2.5.11"})})," --\x3e ",(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:"hbase"})})," ",(0,t.jsx)("br",{})]}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:".bashrc:"})})}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:'...\n...\nexport HADOOP_HOME=/home/hadoop/hadoop-3.4.1\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nexport HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"\n\nexport SQOOP_HOME=/home/hadoop/sqoop-1.4.7.bin__hadoop-2.6.0\nexport PATH=$PATH:$SQOOP_HOME/bin\n\nexport HBASE_HOME=/home/hadoop/hbase\nexport PATH=$PATH:$HBASE_HOME/bin\n'})}),"\n",(0,t.jsx)(a.hr,{}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"nano $HBASE_HOME/conf/hbase-env.sh\n"})}),"\n",(0,t.jsxs)(a.p,{children:["add:",(0,t.jsx)("br",{})]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"export JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64\n#\n# pseudo-distributed mode:\n# export HBASE_REGIONSERVERS=${HBASE_HOME}/conf/regionservers\n# export HBASE_MANAGES_ZK=true\n"})}),"\n",(0,t.jsx)(a.hr,{}),"\n",(0,t.jsx)(a.h2,{id:"standalone-mode--pseudo-distributed-mode",children:"Standalone Mode / Pseudo-Distributed Mode:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"nano $HBASE_HOME/conf/hbase-site.xml\n"})}),"\n",(0,t.jsxs)(a.p,{children:["change ",(0,t.jsx)(a.code,{children:"<configuration>...</configuration>"})," to:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:'<configuration>\n  \x3c!--\n    The following properties are set for running HBase as a single process on a\n    developer workstation. With this configuration, HBase is running in\n    "stand-alone" mode and without a distributed file system. In this mode, and\n    without further configuration, HBase and ZooKeeper data are stored on the\n    local filesystem, in a path under the value configured for `hbase.tmp.dir`.\n    This value is overridden from its default value of `/tmp` because many\n    systems clean `/tmp` on a regular basis. Instead, it points to a path within\n    this HBase installation directory.\n\n    Running against the `LocalFileSystem`, as opposed to a distributed\n    filesystem, runs the risk of data integrity issues and data loss. Normally\n    HBase will refuse to run in such an environment. Setting\n    `hbase.unsafe.stream.capability.enforce` to `false` overrides this behavior,\n    permitting operation. This configuration is for the developer workstation\n    only and __should not be used in production!__\n\n    See also https://hbase.apache.org/book.html#standalone_dist\n\n  <property>\n    <name>hbase.cluster.distributed</name>\n    <value>false</value>\n  </property>\n  <property>\n    <name>hbase.tmp.dir</name>\n    <value>./tmp</value>\n  </property>\n  <property>\n    <name>hbase.unsafe.stream.capability.enforce</name>\n    <value>false</value>\n  </property>\n--\x3e\n  \x3c!-- Standalone Mode --\x3e\n  <property>\n    <name>hbase.rootdir</name>\n    <value>file:///home/hadoop/hbase/hbaseFiles</value>\n  </property>\n  <property>\n    <name>hbase.zookeeper.property.dataDir</name>\n    <value>/home/hadoop/hadoop-3.4.1/zookeeper</value>\n  </property>\n  <property>\n    <name>hbase.unsafe.stream.capability.enforce</name>\n    <value>false</value>\n  </property>\n\n  \x3c!-- Pseudo-Distributed Mode --\x3e\n  \x3c!--\n    <property>\n      <name>hbase.rootdir</name>\n      <value>hdfs://127.0.0.1:9000/hbase</value>\n      <description>\n        hadoop: core-site.xml\n      </description>\n    </property>\n\n    <property>\n      <name>hbase.cluster.distributed</name>\n      <value>true</value>\n    </property>\n\n    <property>\n      <name>hbase.zookeeper.property.clientPort</name>\n      <value>2181</value>\n      <description>\n          default\n      </description>\n    </property>\n\n    <property>\n      <name>hbase.zookeeper.property.dataDir</name>\n      <value>/home/hadoop/hadoop-3.4.1/zookeeper</value>\n    </property>\n\n    <property>\n        <name>zookeeper.registry.async.get.timeout</name>\n        <value>30000</value>\n    </property>\n\n    <property>\n        <name>hbase.wal.provider</name>\n        <value>filesystem</value>\n    </property>\n  --\x3e\n</configuration>\n'})}),"\n",(0,t.jsx)(a.hr,{}),"\n",(0,t.jsx)(a.h3,{id:"start-sequence",children:"Start Sequence:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"$HADOOP_HOME/sbin/start-dfs.sh\n$HADOOP_HOME/sbin/start-yarn.sh\n$HBASE_HOME/bin/start-hbase.sh\n$HBASE_HOME/bin/hbase shell\n"})}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"Jps\n"})}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"5635 NodeManager\n6915 Jps\n5302 ResourceManager\n4790 DataNode\n4604 NameNode\n6077 HMaster\n5006 SecondaryNameNode\n"})}),"\n",(0,t.jsx)(a.h2,{id:"create-a-table",children:"Create a table"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"create 'test', 'cf'\n"})}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"list 'test'\n"})}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"describe 'test'\n"})}),"\n",(0,t.jsx)(a.h3,{id:"put-data-into-your-table",children:"put data into your table"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"put 'test', 'row1', 'cf:a', 'value1'\nput 'test', 'row2', 'cf:b', 'value2'\nput 'test', 'row3', 'cf:b', 'value3'\n"})}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"scan 'test'\n"})}),"\n",(0,t.jsx)(a.h4,{id:"get-a-single-row-of-data",children:"Get a single row of data."}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"get 'test', 'row1'\n"})}),"\n",(0,t.jsx)(a.h3,{id:"ending-sequence",children:"Ending Sequence:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"exit # shell\n$HBASE_HOME/bin/stop-hbase.sh\n$HADOOP_HOME/sbin/stop-yarn.sh\n$HADOOP_HOME/sbin/stop-dfs.sh\n"})}),"\n",(0,t.jsx)(a.h2,{id:"pseudo-distributed-mode",children:"Pseudo-Distributed Mode:"}),"\n",(0,t.jsxs)(a.p,{children:["See: ",(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:"hbase-site.xml"})})," and ",(0,t.jsx)(a.em,{children:(0,t.jsx)(a.strong,{children:"hbase-env.sh"})})]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"jps\n"})}),"\n",(0,t.jsx)(a.h4,{id:"output",children:"Output"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"14050 HMaster\n23507 NodeManager\n22821 SecondaryNameNode\n22421 NameNode\n22601 DataNode\n24203 Jps\n13899 HQuorumPeer\n14270 HRegionServer\n23119 ResourceManager\n"})}),"\n",(0,t.jsx)(a.h2,{id:"master-status",children:"Master Status:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{children:"http://localhost:16010/master-status\n"})})]})}function c(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},8453:(e,a,s)=>{s.d(a,{R:()=>r,x:()=>o});var n=s(6540);const t={},i=n.createContext(t);function r(e){const a=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),n.createElement(i.Provider,{value:a},e.children)}}}]);