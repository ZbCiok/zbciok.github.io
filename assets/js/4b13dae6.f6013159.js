"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[7873],{1181:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>d,metadata:()=>o,toc:()=>i});const o=JSON.parse('{"id":"apache-hadoop/install","title":"Setting up a Single Node Cluster","description":"How to Install Hadoop on Ubuntu","source":"@site/docs/apache-hadoop/install.mdx","sourceDirName":"apache-hadoop","slug":"/apache-hadoop/install","permalink":"/docs/apache-hadoop/install","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":100,"frontMatter":{"sidebar_position":100},"sidebar":"tutorialSidebar","previous":{"title":"Apache Hadoop","permalink":"/docs/apache-hadoop"},"next":{"title":"SQOOP","permalink":"/docs/apache-hadoop/sqoop"}}');var r=a(4848),s=a(8453);const d={sidebar_position:100},t="Setting up a Single Node Cluster",l={},i=[{value:"Supported Platforms",id:"supported-platforms",level:3},{value:"Required Software",id:"required-software",level:3},{value:"Download and Install",id:"download-and-install",level:2},{value:"At the moment, Apache Hadoop 3.x fully supports Java 8 and 11",id:"at-the-moment-apache-hadoop-3x-fully-supports-java-8-and-11",level:3},{value:"In our case:",id:"in-our-case",level:4},{value:"Set Up Hadoop User and Configure SSH",id:"set-up-hadoop-user-and-configure-ssh",level:3},{value:"Create Hadoop User",id:"create-hadoop-user",level:4},{value:"Install OpenSSH",id:"install-openssh",level:4},{value:"Switch to the newly created user",id:"switch-to-the-newly-created-user",level:4},{value:"Enable Passwordless SSH for Hadoop User",id:"enable-passwordless-ssh-for-hadoop-user",level:4},{value:"Download and Install Hadoop on Ubuntu",id:"download-and-install-hadoop-on-ubuntu",level:3},{value:"Configure Single Node Hadoop",id:"configure-single-node-hadoop",level:3},{value:".bashrc",id:"bashrc",level:4},{value:"hadoop-env.sh",id:"hadoop-envsh",level:4},{value:"core-site.xml",id:"core-sitexml",level:4},{value:"hdfs-site.xml",id:"hdfs-sitexml",level:4},{value:"mapred-site.xml",id:"mapred-sitexml",level:4},{value:"yarn-site.xml",id:"yarn-sitexml",level:4},{value:"Format HDFS NameNode",id:"format-hdfs-namenode",level:3},{value:"Start Hadoop Cluster",id:"start-hadoop-cluster",level:3},{value:"Start YARN",id:"start-yarn",level:3},{value:"Run the following command to check if all the daemons are active and running as Java processes:",id:"run-the-following-command-to-check-if-all-the-daemons-are-active-and-running-as-java-processes",level:3},{value:"Access Hadoop from Browser",id:"access-hadoop-from-browser",level:3},{value:"Start / Stop Commands",id:"start--stop-commands",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"setting-up-a-single-node-cluster",children:"Setting up a Single Node Cluster"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://phoenixnap.com/kb/install-hadoop-ubuntu",children:(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"How to Install Hadoop on Ubuntu"})})})," ",(0,r.jsx)("br",{}),"\n",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://hadoop.apache.org/",children:"https://hadoop.apache.org/"})})})," ",(0,r.jsx)("br",{}),"\n",(0,r.jsx)(n.a,{href:"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Purpose",children:(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"Hadoop: Setting up a Single Node Cluster."})})})," ",(0,r.jsx)("br",{}),"\n",(0,r.jsx)(n.a,{href:"https://hadoop.apache.org/docs/r3.4.1/hadoop-project-dist/hadoop-common/SecureMode.html",children:(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"Hadoop in Secure Mode"})})})," ",(0,r.jsx)("br",{})]}),"\n",(0,r.jsx)(n.admonition,{type:"important",children:(0,r.jsx)(n.p,{children:"All production Hadoop clusters use Kerberos to authenticate callers and secure access to HDFS data as well as restriction access to computation services (YARN etc.)."})}),"\n",(0,r.jsx)(n.h3,{id:"supported-platforms",children:"Supported Platforms"}),"\n",(0,r.jsx)(n.p,{children:"GNU/Linux is supported as a development and production platform. Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes."}),"\n",(0,r.jsx)(n.h3,{id:"required-software",children:"Required Software"}),"\n",(0,r.jsx)(n.p,{children:"Required software for Linux include:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Java\u2122 must be installed. Recommended Java versions are described at ",(0,r.jsx)(n.a,{href:"https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions",children:(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"HadoopJavaVersions."})})})]}),"\n",(0,r.jsx)(n.li,{children:"ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons if the optional start and stop scripts are to be used. Additionally, it is recommmended that pdsh also be installed for better ssh resource management."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"download-and-install",children:"Download and Install"}),"\n",(0,r.jsx)(n.h3,{id:"at-the-moment-apache-hadoop-3x-fully-supports-java-8-and-11",children:"At the moment, Apache Hadoop 3.x fully supports Java 8 and 11"}),"\n",(0,r.jsx)(n.h4,{id:"in-our-case",children:"In our case:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"sudo update-java-alternatives --list\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"java-1.11.0-openjdk-amd64      1111       /usr/lib/jvm/java-1.11.0-openjdk-amd64\njava-1.17.0-openjdk-amd64      1711       /usr/lib/jvm/java-1.17.0-openjdk-amd64\njava-1.21.0-openjdk-amd64      2111       /usr/lib/jvm/java-1.21.0-openjdk-amd64\njava-1.8.0-openjdk-amd64       1081       /usr/lib/jvm/java-1.8.0-openjdk-amd64\n"})}),"\n",(0,r.jsxs)(n.p,{children:["We set ",(0,r.jsx)(n.code,{children:"java-1.11.0-openjdk-amd64"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"sudo update-java-alternatives --set java-1.11.0-openjdk-amd64\n"})}),"\n",(0,r.jsx)(n.p,{children:"Verify:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"java -version\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'openjdk version "11.0.26" 2025-01-21\nOpenJDK Runtime Environment (build 11.0.26+4-post-Ubuntu-1ubuntu122.04)\nOpenJDK 64-Bit Server VM (build 11.0.26+4-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"set-up-hadoop-user-and-configure-ssh",children:"Set Up Hadoop User and Configure SSH"}),"\n",(0,r.jsx)(n.h4,{id:"create-hadoop-user",children:"Create Hadoop User"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"sudo adduser hadoop\n"})}),"\n",(0,r.jsx)(n.h4,{id:"install-openssh",children:"Install OpenSSH"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"sudo apt install openssh-server openssh-client -y\n"})}),"\n",(0,r.jsx)(n.h4,{id:"switch-to-the-newly-created-user",children:"Switch to the newly created user"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"su - hadoop\n"})}),"\n",(0,r.jsx)(n.h4,{id:"enable-passwordless-ssh-for-hadoop-user",children:"Enable Passwordless SSH for Hadoop User"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"chmod 0600 ~/.ssh/authorized_keys\n"})}),"\n",(0,r.jsx)(n.p,{children:"The new user can now SSH without entering a password every time. Verify everything is set up correctly by using the hadoop user to SSH to localhost:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"ssh localhost\n"})}),"\n",(0,r.jsx)(n.h3,{id:"download-and-install-hadoop-on-ubuntu",children:"Download and Install Hadoop on Ubuntu"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"wget https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"tar xzf hadoop-3.4.1.tar.gz\n"})}),"\n",(0,r.jsx)(n.h3,{id:"configure-single-node-hadoop",children:"Configure Single Node Hadoop"}),"\n",(0,r.jsx)(n.h4,{id:"bashrc",children:".bashrc"}),"\n",(0,r.jsxs)(n.p,{children:["Go to ",(0,r.jsx)(n.code,{children:"/home/hadoop"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"nano .bashrc\n"})}),"\n",(0,r.jsx)(n.p,{children:"and add:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'export HADOOP_HOME=/home/hadoop/hadoop-3.4.1\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nexport HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Run the command below to apply the changes to the current running environment (",(0,r.jsx)(n.code,{children:"/home/hadoop/"}),"):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"source ./.bashrc\n"})}),"\n",(0,r.jsx)(n.h4,{id:"hadoop-envsh",children:"hadoop-env.sh"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n"})}),"\n",(0,r.jsxs)(n.p,{children:["add",(0,r.jsx)("br",{}),"\n",(0,r.jsx)(n.code,{children:"export JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64"})," - in our case.",(0,r.jsx)("br",{})]}),"\n",(0,r.jsx)(n.h4,{id:"core-sitexml",children:"core-site.xml"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"nano $HADOOP_HOME/etc/hadoop/core-site.xml\n"})}),"\n",(0,r.jsxs)(n.p,{children:["add",(0,r.jsx)("br",{})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"<configuration>\n<property>\n  <name>hadoop.tmp.dir</name>\n  <value>/home/hadoop/tmpdata</value>\n</property>\n<property>\n  <name>fs.default.name</name>\n  <value>hdfs://127.0.0.1:9000</value>\n</property>\n</configuration>\n"})}),"\n",(0,r.jsx)(n.h4,{id:"hdfs-sitexml",children:"hdfs-site.xml"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml\n"})}),"\n",(0,r.jsxs)(n.p,{children:["add",(0,r.jsx)("br",{})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"<configuration>\n<property>\n  <name>dfs.data.dir</name>\n  <value>/home/hadoop/dfsdata/namenode</value>\n</property>\n<property>\n  <name>dfs.data.dir</name>\n  <value>/home/hadoop/dfsdata/datanode</value>\n</property>\n<property>\n  <name>dfs.replication</name>\n  <value>1</value>\n</property>\n</configuration>\n"})}),"\n",(0,r.jsx)(n.h4,{id:"mapred-sitexml",children:"mapred-site.xml"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"nano $HADOOP_HOME/etc/hadoop/mapred-site.xml\n"})}),"\n",(0,r.jsxs)(n.p,{children:["add",(0,r.jsx)("br",{})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"<configuration>\n<property>\n  <name>mapreduce.framework.name</name>\n  <value>yarn</value>\n</property>\n</configuration>\n"})}),"\n",(0,r.jsx)(n.h4,{id:"yarn-sitexml",children:"yarn-site.xml"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"nano $HADOOP_HOME/etc/hadoop/yarn-site.xml\n"})}),"\n",(0,r.jsxs)(n.p,{children:["add",(0,r.jsx)("br",{})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"<configuration>\n<property>\n  <name>yarn.nodemanager.aux-services</name>\n  <value>mapreduce_shuffle</value>\n</property>\n<property>\n  <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n  <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n</property>\n<property>\n  <name>yarn.resourcemanager.hostname</name>\n  <value>127.0.0.1</value>\n</property>\n<property>\n  <name>yarn.acl.enable</name>\n  <value>0</value>\n</property>\n<property>\n  <name>yarn.nodemanager.env-whitelist</name>\n  <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\n</property>\n</configuration>\n"})}),"\n",(0,r.jsx)(n.h3,{id:"format-hdfs-namenode",children:"Format HDFS NameNode"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hdfs namenode -format\n"})}),"\n",(0,r.jsx)(n.h3,{id:"start-hadoop-cluster",children:"Start Hadoop Cluster"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"start-dfs.sh\n"})}),"\n",(0,r.jsx)(n.p,{children:"Output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Starting namenodes on [localhost]\nStarting datanodes\nStarting secondary namenodes\n"})}),"\n",(0,r.jsx)(n.h3,{id:"start-yarn",children:"Start YARN"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"start-yarn.sh\n"})}),"\n",(0,r.jsx)(n.p,{children:"Output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Starting resourcemanager\nStarting nodemanagers\n"})}),"\n",(0,r.jsx)(n.h3,{id:"run-the-following-command-to-check-if-all-the-daemons-are-active-and-running-as-java-processes",children:"Run the following command to check if all the daemons are active and running as Java processes:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"jps\n"})}),"\n",(0,r.jsx)(n.p,{children:"Sample output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"15584 Jps\n14916 ResourceManager\n15252 NodeManager\n"})}),"\n",(0,r.jsx)(n.h3,{id:"access-hadoop-from-browser",children:"Access Hadoop from Browser"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"NameNode web interface: http://localhost:9870\nDataNode web interface: http://localhost:9864\nhttp://localhost:8088\n"})}),"\n",(0,r.jsx)(n.h3,{id:"start--stop-commands",children:"Start / Stop Commands"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"$HADOOP_HOME/sbin/start-dfs.sh\n$HADOOP_HOME/sbin/start-yarn.sh\n\n$HADOOP_HOME/sbin/stop-yarn.sh\n$HADOOP_HOME/sbin/stop-dfs.sh\n"})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>d,x:()=>t});var o=a(6540);const r={},s=o.createContext(r);function d(e){const n=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:d(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);