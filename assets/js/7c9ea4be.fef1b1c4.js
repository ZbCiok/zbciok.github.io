"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[8435],{2662:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>x,frontMatter:()=>l,metadata:()=>s,toc:()=>h});const s=JSON.parse('{"id":"apache-hadoop/hive/index","title":"Hive","description":"https://hive.apache.org/","source":"@site/docs/apache-hadoop/hive/index.mdx","sourceDirName":"apache-hadoop/hive","slug":"/apache-hadoop/hive/","permalink":"/docs/apache-hadoop/hive/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":100,"frontMatter":{"sidebar_position":100},"sidebar":"tutorialSidebar","previous":{"title":"Hadoop-3.4.1 using Docker 02","permalink":"/docs/apache-hadoop/hadoop-using-docker-02"},"next":{"title":"Hive-4.0.0 Docker 01","permalink":"/docs/apache-hadoop/hive/Examples/hive-4.0.0-docer-01"}}');var t=i(4848),r=i(8453),a=i(7170),o=i(3304);const l={sidebar_position:100},c="Hive",d={},h=[{value:"Hive Architecture",id:"hive-architecture",level:2},{value:"Hive Clients",id:"hive-clients",level:3},{value:"Hive Services",id:"hive-services",level:3},{value:"Hive Storage",id:"hive-storage",level:3},{value:"Data Flow in Hive",id:"data-flow-in-hive",level:2},{value:"Hive Data Modeling (Data Units)",id:"hive-data-modeling-data-units",level:2},{value:"Hive Data Types",id:"hive-data-types",level:2},{value:"HiveServer2",id:"hiveserver2",level:2},{value:"HiveServer2 Functions",id:"hiveserver2-functions",level:3},{value:"Execution Engines",id:"execution-engines",level:2},{value:"The execution engine is responsible for:",id:"the-execution-engine-is-responsible-for",level:4}];function u(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"hive",children:"Hive"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://hive.apache.org/",children:"https://hive.apache.org/"})})}),(0,t.jsx)("br",{}),"\n",(0,t.jsx)(n.a,{href:"https://hive.apache.org/docs/latest/manual-installation_283118363/",children:(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"Apache Hive : Manual Installation"})})})]}),"\n",(0,t.jsxs)(n.admonition,{type:"note",children:[(0,t.jsxs)(n.mdxAdmonitionTitle,{children:["Downloads: ",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"http://www.apache.org/dyn/closer.cgi/hive/",children:"http://www.apache.org/dyn/closer.cgi/hive/"})})})]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"08 October 2024: EOL for release 3.x line"}),"\n",(0,t.jsx)(n.p,{children:"The Apache Hive Community has voted to declare the 3.x release line as End of Life (EOL). This means no further updates or releases will be made for this series.\nWe urge all Hive 3.x users to upgrade to the latest versions promptly to benefit from new features and ongoing support."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"2 October 2024: release 4.0.1 available"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["As from ",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"Hive 4.x"})}),", we encourage all users to move the workload to ",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"Tez"})})," to benefit from performance gain and support."]}),"\n",(0,t.jsxs)(n.li,{children:["This release works with ",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"Hadoop 3.3.6"})}),", ",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"Tez 0.10.4"})}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://hive.apache.org/development/quickstart/",children:(0,t.jsx)(n.strong,{children:"DOCKER_QUICKSTART"})})}),"\n"]}),"\n"]})]}),"\n",(0,t.jsx)(n.p,{children:"Apache Hive is open-source data warehouse software designed to read, write, and manage large datasets extracted from the Apache Hadoop Distributed File System.\nIt facilitates reading, writing and handling wide datasets that stored in distributed storage and queried by Structure Query Language (SQL) syntax.\nIt provides an SQL-like interface to query and analyze large datasets stored in Hadoop\u2019s distributed file system (HDFS) or other compatible storage systems."}),"\n",(0,t.jsx)(n.h2,{id:"hive-architecture",children:"Hive Architecture"}),"\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/hive/01-hive-architecture.png",alt:"01-hive-architecture.png"}),"\n",(0,t.jsx)("br",{}),"\n",(0,t.jsx)(n.p,{children:"Hive Consists of Mainly 3 core parts"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Hive Clients"}),"\n",(0,t.jsx)(n.li,{children:"Hive Services"}),"\n",(0,t.jsx)(n.li,{children:"Hive Storage"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"hive-clients",children:"Hive Clients"}),"\n",(0,t.jsx)(n.p,{children:"Hive allows writing applications in various languages, including Java, Python, and C++. It supports different types of clients such as:-"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Thrift Server"})," - It is a cross-language service provider platform that serves the request from all those programming languages that supports Thrift."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"JDBC Driver"})," - It is used to establish a connection between hive and Java applications. The JDBC Driver is present in the class org.apache.hadoop.hive.jdbc.HiveDriver."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ODBC Driver"})," - It allows the applications that support the ODBC protocol to connect to Hive."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"hive-services",children:"Hive Services"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"User Interface (UI)"})," \u2013 As the name describes User interface provide an interface between user and hive. It enables user to submit queries and other operations to the system. Hive web UI, Hive command line, and Hive HD Insight (In windows server) are supported by the user interface."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hive Server"})," \u2013 It is referred to as Apache Thrift Server. It accepts the request from different clients and provides it to Hive Driver."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hive CLI"})," \u2014 The Hive CLI (Command Line Interface) is a shell where we can execute Hive queries and commands."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Driver"})," \u2013 Queries of the user after the interface are received by the driver within the Hive. Concept of session handles is implemented by driver. Execution and Fetching of APIs modelled on JDBC/ODBC interfaces is provided by the user."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Metastore"})," \u2013 All the structured data or information of the different tables and partition in the warehouse containing attributes and attributes level information are stored in the metastore. Sequences or de-sequences necessary to read and write data and the corresponding HDFS files where the data is stored. Hive selects corresponding database servers to stock the schema or Metadata of databases, tables, attributes in a table, data types of databases, and HDFS mapping."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hive Compiler"})," - The purpose of the compiler is to parse the query and perform semantic analysis on the different query blocks and expressions. It converts HiveQL statements into MapReduce jobs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hive Execution Engine"})," - Optimizer generates the logical plan in the form of DAG of map-reduce tasks and HDFS tasks. In the end, the execution engine executes the incoming tasks in the order of their dependencies."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"hive-storage",children:"Hive Storage"}),"\n",(0,t.jsx)(n.p,{children:"Hive itself doesn't store data. Instead, it provides a schema and query engine to work with data stored in Hadoop-compatible file systems."}),"\n",(0,t.jsx)(n.h2,{id:"data-flow-in-hive",children:"Data Flow in Hive"}),"\n",(0,t.jsx)(n.p,{children:"Data flow in the Hive contains the Hive and Hadoop system. Underneath the user interface, we have driver, compiler, execution engine, and metastore. All of that goes into the MapReduce and the Hadoop file system."}),"\n",(0,t.jsx)(n.p,{children:"The data flow in the following sequence:"}),"\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/hive/02-dataflow_in_hive1.avif",alt:"02-dataflow_in_hive1.avif"}),"\n",(0,t.jsx)("br",{}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"We execute a query, which goes into the driver"}),"\n",(0,t.jsx)(n.li,{children:"Then the driver asks for the plan, which refers to the query execution"}),"\n",(0,t.jsx)(n.li,{children:"After this, the compiler gets the metadata from the metastore"}),"\n",(0,t.jsx)(n.li,{children:"The metastore responds with the metadata"}),"\n",(0,t.jsx)(n.li,{children:"The compiler gathers this information and sends the plan back to the driver"}),"\n",(0,t.jsx)(n.li,{children:"Now, the driver sends the execution plan to the execution engine"}),"\n",(0,t.jsx)(n.li,{children:"The execution engine acts as a bridge between the Hive and Hadoop to process the query"}),"\n",(0,t.jsx)(n.li,{children:"In addition to this, the execution engine also communicates bidirectionally with the metastore to perform various operations, such as create and drop tables"}),"\n",(0,t.jsx)(n.li,{children:"Finally, we have a bidirectional communication to fetch and send results back to the client"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hive-data-modeling-data-units",children:"Hive Data Modeling (Data Units)"}),"\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/hive/04-hive-data-model-01.png",alt:"04-hive-data-model-01.png"}),"\n",(0,t.jsx)("br",{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Databases"})," - In Hive, a database is a ",(0,t.jsx)(n.strong,{children:"namespace"})," that organizes tables and other database objects (like views, functions). It helps prevent name conflicts and manages data in a logical group.","\n",(0,t.jsx)("img",{src:"/img/apache-hadoop/hive/05-hive-databases-01.png",alt:"05-hive-databases-01.png"}),"\n",(0,t.jsx)("br",{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Syntax:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"CREATE DATABASE IF NOT EXISTS my_database;"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"USE my_database;"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"SHOW DATABASES;"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hive Database Storage Location"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["By ",(0,t.jsx)(n.strong,{children:"default"}),", Hive stores each database as a directory in HDFS","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"[hive version path]/warehouse/my_database.db/"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Custom location"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"CREATE DATABASE my_database LOCATION '/my/custom/path';"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tables"})," - Tables in Hive are created the same way it is done in RDBMS"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Partitions"})," - Here, tables are organized into partitions for grouping similar types of data based on the partition key"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Buckets"})," - Data present in partitions can be further divided into buckets for efficient querying"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hive-data-types",children:"Hive Data Types"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Primitive Data Types:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Numeric Data types - Data types like integral, float, decimal"}),"\n",(0,t.jsx)(n.li,{children:"String Data type - Data types like char, string"}),"\n",(0,t.jsx)(n.li,{children:"Date/ Time Data type - Data types like timestamp, date, interval"}),"\n",(0,t.jsx)(n.li,{children:"Miscellaneous Data type - Data types like Boolean and binary"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Complex Data Types:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Arrays - A collection of the same entities. The syntax is: ",(0,t.jsx)(n.code,{children:"array<data_type>"})]}),"\n",(0,t.jsxs)(n.li,{children:["Maps - A collection of key-value pairs and the syntax is ",(0,t.jsx)(n.code,{children:"map<primitive_type, data_type>"})]}),"\n",(0,t.jsxs)(n.li,{children:["Structs - A collection of complex data with comments. Syntax: ",(0,t.jsx)(n.code,{children:"struct<col_name : data_type [COMMENT col_comment],\u2026..>"})]}),"\n",(0,t.jsxs)(n.li,{children:["Units - A collection of heterogeneous data types. Syntax: ",(0,t.jsx)(n.code,{children:"uniontype<data_type, data_type,..>"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hiveserver2",children:"HiveServer2"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"HiveServer"})," is an optional service that allows a remote client to submit requests to Hive, using a variety of programming languages, and retrieve results. ",(0,t.jsx)(n.strong,{children:"HiveServer"})," is built on ",(0,t.jsx)(n.strong,{children:"Apache ThriftTM"})," ",(0,t.jsx)(n.em,{children:(0,t.jsxs)(n.strong,{children:["(",(0,t.jsx)(n.a,{href:"http://thrift.apache.org/",children:"http://thrift.apache.org/"}),")"]})}),", therefore it is sometimes called the ",(0,t.jsx)(n.strong,{children:"Thrift server"})," although this can lead to confusion because a newer service named ",(0,t.jsx)(n.strong,{children:"HiveServer2"})," is also built on ",(0,t.jsx)(n.strong,{children:"Thrift"}),". Since the introduction of ",(0,t.jsx)(n.strong,{children:"HiveServer2"}),", ",(0,t.jsx)(n.strong,{children:"HiveServer"})," has also been called ",(0,t.jsx)(n.strong,{children:"HiveServer1"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"HiveServer"})," was removed from Hive releases starting in Hive 1.0.0 (formerly called 0.14.1). Please switch over to ",(0,t.jsx)(n.strong,{children:"HiveServer2"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"HiveServer (HS2)"})," supports multi-client concurrency and authentication. It is designed to provide better support for open API clients like JDBC and ODBC."]}),"\n",(0,t.jsx)(n.h3,{id:"hiveserver2-functions",children:"HiveServer2 Functions"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Query Execution:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Client Interaction:"})," Clients (like Beeline, JDBC/ODBC clients, or Hive CLI) connect to HiveServer2. Typically JDBC or ODBC. Authentication: Can include Kerberos, etc."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Query Parsing:"})," HiveServer2 receives a query and sends it to the Hive Compiler"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Query Planning:"})," Plan generation: Breaks query into stages of MapReduce/Tez/Spark jobs depending on execution engine."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Query Execution:"})," The Execution Engine (MapReduce / Tez / Spark) runs the physical plan."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Result Handling:"})," HiveServer2 collects the final output:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"In-memory results for small queries."}),"\n",(0,t.jsx)(n.li,{children:"HDFS or temporary locations for large queries."}),"\n",(0,t.jsx)(n.li,{children:"Sends results back to the client over JDBC/ODBC."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-client Concurrency:"})," Refers to the ability of Apache HiveServer2 (HS2) to handle multiple client connections and queries concurrently. This is a major improvement over the original ",(0,t.jsx)(n.strong,{children:"HiveServer"})," (now deprecated), which supported only a single client connection at a time."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Authentication:"})," HS2 supports several authentication methods, which are configured in ",(0,t.jsx)(n.em,{children:(0,t.jsx)(n.strong,{children:"hive-site.xml"})}),". Authentication Methods:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"None"}),"\n",(0,t.jsx)(n.li,{children:"Kerberos"}),"\n",(0,t.jsx)(n.li,{children:"LDAP"}),"\n",(0,t.jsx)(n.li,{children:"PAM (Pluggable Authentication Modules)"}),"\n",(0,t.jsx)(n.li,{children:"Custom (via Java Class)"}),"\n",(0,t.jsx)(n.li,{children:"Delegation Token (used in secure environments)"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Open API Support"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"HiveServer2's relation to other Hive components:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"HiveClient"}),"\n",(0,t.jsx)(n.li,{children:"Thrift Protocol"}),"\n",(0,t.jsx)(n.li,{children:"Apache Hadoop"}),"\n",(0,t.jsx)(n.li,{children:"Metastore"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"execution-engines",children:"Execution Engines"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tez Execution Engine"}),"\n",(0,t.jsx)(n.em,{children:(0,t.jsxs)(n.strong,{children:["(",(0,t.jsx)(n.a,{href:"https://tez.apache.org/",children:"https://tez.apache.org/"}),")"]})}),(0,t.jsx)("br",{}),"\nApache Hive 4 uses Apache Tez as its default execution engine to improve query performance."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MapReduce"})," - \tThe original engine; converts queries to MapReduce jobs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Spark"})," - Allows Hive to run on Apache Spark, providing better in-memory performance for iterative queries."]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"the-execution-engine-is-responsible-for",children:"The execution engine is responsible for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Parsing the Query"})," - The HiveQL query is parsed and converted into an Abstract Syntax Tree (AST)."]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Physical Plan Generation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Job Execution"})}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)("br",{}),"\n",(0,t.jsx)("br",{}),"\n","\n",(0,t.jsx)(a.A,{items:(0,o.$S)().items})]})}function x(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}},4365:(e,n,i)=>{i.d(n,{W:()=>c});var s=i(6540),t=i(502);const r=["zero","one","two","few","many","other"];function a(e){return r.filter((n=>e.includes(n)))}const o={locale:"en",pluralForms:a(["one","other"]),select:e=>1===e?"one":"other"};function l(){const{i18n:{currentLocale:e}}=(0,t.A)();return(0,s.useMemo)((()=>{try{return function(e){const n=new Intl.PluralRules(e);return{locale:e,pluralForms:a(n.resolvedOptions().pluralCategories),select:e=>n.select(e)}}(e)}catch(n){return console.error(`Failed to use Intl.PluralRules for locale "${e}".\nDocusaurus will fallback to the default (English) implementation.\nError: ${n.message}\n`),o}}),[e])}function c(){const e=l();return{selectMessage:(n,i)=>function(e,n,i){const s=e.split("|");if(1===s.length)return s[0];s.length>i.pluralForms.length&&console.error(`For locale=${i.locale}, a maximum of ${i.pluralForms.length} plural forms are expected (${i.pluralForms.join(",")}), but the message contains ${s.length}: ${e}`);const t=i.select(n),r=i.pluralForms.indexOf(t);return s[Math.min(r,s.length-1)]}(i,n,e)}}},7170:(e,n,i)=>{i.d(n,{A:()=>g});i(6540);var s=i(4164),t=i(7936),r=i(7634),a=i(4365),o=i(5242),l=i(9979),c=i(4861);const d={cardContainer:"cardContainer_fWXF",cardTitle:"cardTitle_rnsV",cardDescription:"cardDescription_PWke"};var h=i(4848);function u(e){let{href:n,children:i}=e;return(0,h.jsx)(r.A,{href:n,className:(0,s.A)("card padding--lg",d.cardContainer),children:i})}function x(e){let{href:n,icon:i,title:t,description:r}=e;return(0,h.jsxs)(u,{href:n,children:[(0,h.jsxs)(c.A,{as:"h2",className:(0,s.A)("text--truncate",d.cardTitle),title:t,children:[i," ",t]}),r&&(0,h.jsx)("p",{className:(0,s.A)("text--truncate",d.cardDescription),title:r,children:r})]})}function p(e){let{item:n}=e;const i=(0,t.Nr)(n),s=function(){const{selectMessage:e}=(0,a.W)();return n=>e(n,(0,l.T)({message:"1 item|{count} items",id:"theme.docs.DocCard.categoryDescription.plurals",description:"The default description for a category card in the generated index about how many items this category includes"},{count:n}))}();return i?(0,h.jsx)(x,{href:i,icon:"\ud83d\uddc3\ufe0f",title:n.label,description:n.description??s(n.items.length)}):null}function v(e){let{item:n}=e;const i=(0,o.A)(n.href)?"\ud83d\udcc4\ufe0f":"\ud83d\udd17",s=(0,t.cC)(n.docId??void 0);return(0,h.jsx)(x,{href:n.href,icon:i,title:n.label,description:n.description??s?.description})}function j(e){let{item:n}=e;switch(n.type){case"link":return(0,h.jsx)(v,{item:n});case"category":return(0,h.jsx)(p,{item:n});default:throw new Error(`unknown item type ${JSON.stringify(n)}`)}}function m(e){let{className:n}=e;const i=(0,t.$S)();return(0,h.jsx)(g,{items:i.items,className:n})}function g(e){const{items:n,className:i}=e;if(!n)return(0,h.jsx)(m,{...e});const r=(0,t.d1)(n);return(0,h.jsx)("section",{className:(0,s.A)("row",i),children:r.map(((e,n)=>(0,h.jsx)("article",{className:"col col--6 margin-bottom--lg",children:(0,h.jsx)(j,{item:e})},n)))})}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);