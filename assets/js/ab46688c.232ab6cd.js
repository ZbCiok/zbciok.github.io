"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[669],{8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>l});var s=n(6540);const r={},a=s.createContext(r);function i(e){const t=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(a.Provider,{value:t},e.children)}},8977:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"streams/apache-spark/interactive-analysis-with-spark-shell","title":"Interactive Analysis with the Spark Shell","description":"Basics","source":"@site/docs/streams/apache-spark/interactive-analysis-with-spark-shell.mdx","sourceDirName":"streams/apache-spark","slug":"/streams/apache-spark/interactive-analysis-with-spark-shell","permalink":"/docs/streams/apache-spark/interactive-analysis-with-spark-shell","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":200,"frontMatter":{"sidebar_position":200},"sidebar":"tutorialSidebar","previous":{"title":"Spark Architecture","permalink":"/docs/streams/apache-spark/spark-architecture"},"next":{"title":"Examples","permalink":"/docs/streams/apache-string/examples"}}');var r=n(4848),a=n(8453);const i={sidebar_position:200},l="Interactive Analysis with the Spark Shell",o={},c=[{value:"Basics",id:"basics",level:2},{value:"Output",id:"output",level:4},{value:"Output",id:"output-1",level:4},{value:"Output",id:"output-2",level:4},{value:"Output",id:"output-3",level:4},{value:"Output",id:"output-4",level:4},{value:"More on Dataset Operations",id:"more-on-dataset-operations",level:2},{value:"Output",id:"output-5",level:4},{value:"Output",id:"output-6",level:4},{value:"Output",id:"output-7",level:4},{value:"Caching",id:"caching",level:2},{value:"Output",id:"output-8",level:4},{value:"Output",id:"output-9",level:4}];function d(e){const t={code:"code",em:"em",h1:"h1",h2:"h2",h4:"h4",header:"header",hr:"hr",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"interactive-analysis-with-the-spark-shell",children:"Interactive Analysis with the Spark Shell"})}),"\n",(0,r.jsx)(t.h2,{id:"basics",children:"Basics"}),"\n",(0,r.jsx)(t.p,{children:"Spark\u2019s primary abstraction is a distributed collection of items called a Dataset. Datasets can be created from Hadoop InputFormats (such as HDFS files) or by transforming other Datasets. Due to Python\u2019s dynamic nature, we don\u2019t need the Dataset to be strongly-typed in Python. As a result, all Datasets in Python are Dataset[Row], and we call it DataFrame to be consistent with the data frame concept in Pandas and R. Let\u2019s make a new DataFrame from the text of the README file in the Spark source directory:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"pyspark\n"})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:'>>> textFile = spark.read.text("README.md")\n'})}),"\n",(0,r.jsx)(t.h4,{id:"output",children:"Output"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.em,{children:"no output"})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"textFile.count()  # Number of rows in this DataFrame\n"})}),"\n",(0,r.jsx)(t.h4,{id:"output-1",children:"Output"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"125\n"})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"textFile.first()  # First row in this DataFrame\n"})}),"\n",(0,r.jsx)(t.h4,{id:"output-2",children:"Output"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"Row(value='# Apache Spark')\n"})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:'linesWithSpark = textFile.filter(textFile.value.contains("Spark"))\n'})}),"\n",(0,r.jsx)(t.h4,{id:"output-3",children:"Output"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.em,{children:"no output"})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:'textFile.filter(textFile.value.contains("Spark")).count()  # How many lines contain "Spark"?\n'})}),"\n",(0,r.jsx)(t.h4,{id:"output-4",children:"Output"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"20\n"})}),"\n",(0,r.jsx)(t.h2,{id:"more-on-dataset-operations",children:"More on Dataset Operations"}),"\n",(0,r.jsx)(t.p,{children:"Dataset actions and transformations can be used for more complex computations. Let\u2019s say we want to find the line with the most words:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"from pyspark.sql import functions as sf\n"})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:'\ntextFile.select(sf.size(sf.split(textFile.value, "\\s+")).name("numWords")).agg(sf.max(sf.col("numWords"))).collect()\n'})}),"\n",(0,r.jsx)(t.h4,{id:"output-5",children:"Output"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"[Row(max(numWords)=16)]\n"})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.p,{children:"Dataset actions and transformations can be used for more complex computations. Let\u2019s say we want to find the line with the most words:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"from pyspark.sql import functions as sf\n"})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:'\ntextFile.select(sf.size(sf.split(textFile.value, "\\s+")).name("numWords")).agg(sf.max(sf.col("numWords"))).collect()\n'})}),"\n",(0,r.jsx)(t.h4,{id:"output-6",children:"Output"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"[Row(max(numWords)=16)]\n"})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:'wordCounts = textFile.select(sf.explode(sf.split(textFile.value, "\\s+")).alias("word")).groupBy("word").count()\n'})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"wordCounts.collect()\n"})}),"\n",(0,r.jsx)(t.h4,{id:"output-7",children:"Output"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"\n[Row(word='[![PySpark', count=1), Row(word='online', count=1), Row(word='graphs', ...\n"})}),"\n",(0,r.jsx)(t.h2,{id:"caching",children:"Caching"}),"\n",(0,r.jsx)(t.p,{children:"Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small \u201chot\u201d dataset or when running an iterative algorithm like PageRank. As a simple example, let\u2019s mark our linesWithSpark dataset to be cached:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"linesWithSpark.cache()\n"})}),"\n",(0,r.jsx)(t.h4,{id:"output-8",children:"Output"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"DataFrame[value: string]\n"})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"linesWithSpark.count()\n"})}),"\n",(0,r.jsx)(t.h4,{id:"output-9",children:"Output"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{children:"20\n"})})]})}function u(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);