"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[6529],{4365:(e,t,a)=>{a.d(t,{W:()=>l});var s=a(6540),n=a(502);const r=["zero","one","two","few","many","other"];function o(e){return r.filter((t=>e.includes(t)))}const i={locale:"en",pluralForms:o(["one","other"]),select:e=>1===e?"one":"other"};function c(){const{i18n:{currentLocale:e}}=(0,n.A)();return(0,s.useMemo)((()=>{try{return function(e){const t=new Intl.PluralRules(e);return{locale:e,pluralForms:o(t.resolvedOptions().pluralCategories),select:e=>t.select(e)}}(e)}catch(t){return console.error(`Failed to use Intl.PluralRules for locale "${e}".\nDocusaurus will fallback to the default (English) implementation.\nError: ${t.message}\n`),i}}),[e])}function l(){const e=c();return{selectMessage:(t,a)=>function(e,t,a){const s=e.split("|");if(1===s.length)return s[0];s.length>a.pluralForms.length&&console.error(`For locale=${a.locale}, a maximum of ${a.pluralForms.length} plural forms are expected (${a.pluralForms.join(",")}), but the message contains ${s.length}: ${e}`);const n=a.select(t),r=a.pluralForms.indexOf(n);return s[Math.min(r,s.length-1)]}(a,t,e)}}},4456:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>l,default:()=>p,frontMatter:()=>c,metadata:()=>s,toc:()=>h});const s=JSON.parse('{"id":"apache-hadoop/index","title":"Apache Hadoop","description":"Hadoop Architecture","source":"@site/docs/apache-hadoop/index.mdx","sourceDirName":"apache-hadoop","slug":"/apache-hadoop","permalink":"/docs/apache-hadoop","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"slug":"/apache-hadoop"},"sidebar":"tutorialSidebar","previous":{"title":"References","permalink":"/docs/apache-flink/references"},"next":{"title":"Setting up a Single Node Cluster","permalink":"/docs/apache-hadoop/install"}}');var n=a(4848),r=a(8453),o=a(7170),i=a(3304);const c={slug:"/apache-hadoop"},l="Apache Hadoop",d={},h=[{value:"MapReduce",id:"mapreduce",level:2},{value:"HDFS (Hadoop Distributed File System)",id:"hdfs-hadoop-distributed-file-system",level:2},{value:"YARN (Yet Another Resource Negotiator)",id:"yarn-yet-another-resource-negotiator",level:2},{value:"Hadoop Common Utilities",id:"hadoop-common-utilities",level:2}];function u(e){const t={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"apache-hadoop",children:"Apache Hadoop"})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.a,{href:"https://www.geeksforgeeks.org/hadoop-architecture/",children:(0,n.jsx)(t.em,{children:(0,n.jsx)(t.strong,{children:"Hadoop Architecture"})})})," ",(0,n.jsx)("br",{}),"\n",(0,n.jsx)(t.a,{href:"https://www.tpointtech.com/what-is-hadoop",children:(0,n.jsx)(t.em,{children:(0,n.jsx)(t.strong,{children:"Hadoop Tutorial"})})})," ",(0,n.jsx)("br",{}),"\n",(0,n.jsx)(t.a,{href:"https://documents.uow.edu.au/~jrg/312/slides/03hadooparchitecture/03hadooparchitecture.pdf",children:(0,n.jsx)(t.em,{children:(0,n.jsx)(t.strong,{children:"Hadoop Architecture"})})})," ",(0,n.jsx)("br",{})]}),"\n",(0,n.jsx)(t.p,{children:"The Hadoop Architecture Mainly consists of 4 components."}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"MapReduce"}),": This is a framework which helps Java programs to do the parallel computation on data using key value pair. The Map task takes input data and converts it into a data set which can be computed in Key value pair. The output of Map task is consumed by reduce task and then the out of reducer gives the desired result."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"HDFS"})," (Hadoop Distributed File System): Hadoop Distributed File System. Google published its paper GFS and on the basis of that HDFS was developed. It states that the files will be broken into blocks and stored in nodes over the distributed architecture."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"YARN"})," (Yet Another Resource Negotiator): Yet another Resource Negotiator is used for job scheduling and manage the cluster."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Common Utilities or Hadoop Common"}),": These Java libraries are used to start Hadoop and are used by other Hadoop modules."]}),"\n"]}),"\n",(0,n.jsx)("img",{src:"/img/apache-hadoop/hadoop-architecture-and-core-components.png",alt:"hadoop-architecture-and-core-components.png"}),"\n",(0,n.jsx)(t.h2,{id:"mapreduce",children:"MapReduce"}),"\n",(0,n.jsx)("img",{src:"/img/apache-hadoop/map-reduce-diagram.webp",width:"600 px",alt:"map-reduce-diagram.webp"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Generally MapReduce paradigm is based on sending the computer to where the data resides!"}),"\n",(0,n.jsxs)(t.li,{children:["MapReduce program executes in three stages, namely map stage, shuffle stage, and reduce stage.","\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Map stage \u2212 The map or mapper\u2019s job is to process the input data. Generally the input data is in the form of file or directory and is stored in the Hadoop file system (HDFS). The input file is passed to the mapper function line by line. The mapper processes the data and creates several small chunks of data."}),"\n",(0,n.jsx)(t.li,{children:"Reduce stage \u2212 This stage is the combination of the Shuffle stage and the Reduce stage. The Reducer\u2019s job is to process the data that comes from the mapper. After processing, it produces a new set of output, which will be stored in the HDFS."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(t.li,{children:"During a MapReduce job, Hadoop sends the Map and Reduce tasks to the appropriate servers in the cluster."}),"\n",(0,n.jsx)(t.li,{children:"The framework manages all the details of data-passing such as issuing tasks, verifying task completion, and copying data around the cluster between the nodes."}),"\n",(0,n.jsx)(t.li,{children:"Most of the computing takes place on nodes with data on local disks that reduces the network traffic."}),"\n",(0,n.jsx)(t.li,{children:"After completion of the given tasks, the cluster collects and reduces the data to form an appropriate result, and sends it back to the Hadoop server."}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"hdfs-hadoop-distributed-file-system",children:"HDFS (Hadoop Distributed File System)"}),"\n",(0,n.jsx)("img",{src:"/img/apache-hadoop/hdfs-architecture-01.png",width:"600 px",alt:"hdfs-architecture-01.png"}),"\n",(0,n.jsx)(t.p,{children:"HDFS is designed to be highly scalable, reliable, and efficient, enabling the storage and processing of massive datasets. Its architecture consists of several key components:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"NameNode"}),"\n",(0,n.jsx)(t.li,{children:"DataNode"}),"\n",(0,n.jsx)(t.li,{children:"Secondary NameNode"}),"\n",(0,n.jsx)(t.li,{children:"HDFS Client"}),"\n",(0,n.jsx)(t.li,{children:"Block Structure"}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.strong,{children:"The following describes how HDFS works:"})}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"With HDFS, data is written on the server once and read and reused numerous times."}),"\n",(0,n.jsx)(t.li,{children:"HDFS has a primary NameNode, which keeps track of where file data is kept in the cluster."}),"\n",(0,n.jsxs)(t.li,{children:["HDFS has multiple DataNodes on a commodity hardware cluster -- typically one per node in a cluster. The DataNodes are generally organized within the same rack in the ",(0,n.jsx)(t.a,{href:"https://www.techtarget.com/searchdatacenter/definition/data-center",children:(0,n.jsx)(t.em,{children:(0,n.jsx)(t.strong,{children:"data center"})})}),". Data is broken down into separate blocks and distributed among the various DataNodes for storage. Blocks are also replicated across nodes, enabling highly efficient ",(0,n.jsx)(t.a,{href:"https://www.techtarget.com/searchdatacenter/definition/parallel-processing",children:(0,n.jsx)(t.em,{children:(0,n.jsx)(t.strong,{children:"parallel processing"})})}),"."]}),"\n",(0,n.jsx)(t.li,{children:"The NameNode knows which DataNode contains which blocks and where the DataNodes reside within the machine cluster. The NameNode also manages access to the files, including reads, writes, creates, deletes and the data block replication across the DataNodes."}),"\n",(0,n.jsx)(t.li,{children:"The NameNode operates together with the DataNodes. As a result, the cluster can dynamically adapt to server capacity demands in real time by adding or subtracting nodes as necessary."}),"\n",(0,n.jsx)(t.li,{children:"The DataNodes are in constant communication with the NameNode to determine if the DataNodes need to complete specific tasks. Consequently, the NameNode is always aware of the status of each DataNode. If the NameNode realizes that one DataNode isn't working properly, it can immediately reassign that DataNode's task to a different node containing the same data block. DataNodes also communicate with each other, which enables them to cooperate during normal file operations."}),"\n",(0,n.jsxs)(t.li,{children:["The HDFS is designed to be highly fault tolerant. The file system replicates -- or copies -- each piece of data multiple times and distributes the copies to individual nodes, placing at least one copy on a different ",(0,n.jsx)(t.a,{href:"https://www.techtarget.com/whatis/definition/rack-server-rack-mounted-server",children:(0,n.jsx)(t.em,{children:(0,n.jsx)(t.strong,{children:"server rack"})})})," than the other copies."]}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"yarn-yet-another-resource-negotiator",children:"YARN (Yet Another Resource Negotiator)"}),"\n",(0,n.jsx)(t.p,{children:"YARN is a Framework on which MapReduce works. YARN performs 2 operations that are Job scheduling and Resource Management. The Purpose of Job schedular is to divide a big task into small jobs so that each job can be assigned to various slaves in a Hadoop cluster and Processing can be Maximized. Job Scheduler also keeps track of which job is important, which job has more priority, dependencies between the jobs and all the other information like job timing, etc. And the use of Resource Manager is to manage all the resources that are made available for running a Hadoop cluster."}),"\n",(0,n.jsx)(t.p,{children:"Features of YARN"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Multi-Tenancy"}),"\n",(0,n.jsx)(t.li,{children:"Scalability"}),"\n",(0,n.jsx)(t.li,{children:"Cluster-Utilization"}),"\n",(0,n.jsx)(t.li,{children:"Compatibility"}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"hadoop-common-utilities",children:"Hadoop Common Utilities"}),"\n",(0,n.jsx)(t.p,{children:"Hadoop common or Common utilities are nothing but our java library and java files or we can say the java scripts that we need for all the other components present in a Hadoop cluster. these utilities are used by HDFS, YARN, and MapReduce for running the cluster. Hadoop Common verify that Hardware failure in a Hadoop cluster is common so it needs to be solved automatically in software by Hadoop Framework."}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)("br",{}),"\n",(0,n.jsx)("br",{}),"\n","\n",(0,n.jsx)(o.A,{items:(0,i.$S)().items})]})}function p(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(u,{...e})}):u(e)}},7170:(e,t,a)=>{a.d(t,{A:()=>j});a(6540);var s=a(4164),n=a(7936),r=a(7634),o=a(4365),i=a(5242),c=a(9979),l=a(4861);const d={cardContainer:"cardContainer_fWXF",cardTitle:"cardTitle_rnsV",cardDescription:"cardDescription_PWke"};var h=a(4848);function u(e){let{href:t,children:a}=e;return(0,h.jsx)(r.A,{href:t,className:(0,s.A)("card padding--lg",d.cardContainer),children:a})}function p(e){let{href:t,icon:a,title:n,description:r}=e;return(0,h.jsxs)(u,{href:t,children:[(0,h.jsxs)(l.A,{as:"h2",className:(0,s.A)("text--truncate",d.cardTitle),title:n,children:[a," ",n]}),r&&(0,h.jsx)("p",{className:(0,s.A)("text--truncate",d.cardDescription),title:r,children:r})]})}function m(e){let{item:t}=e;const a=(0,n.Nr)(t),s=function(){const{selectMessage:e}=(0,o.W)();return t=>e(t,(0,c.T)({message:"1 item|{count} items",id:"theme.docs.DocCard.categoryDescription.plurals",description:"The default description for a category card in the generated index about how many items this category includes"},{count:t}))}();return a?(0,h.jsx)(p,{href:a,icon:"\ud83d\uddc3\ufe0f",title:t.label,description:t.description??s(t.items.length)}):null}function f(e){let{item:t}=e;const a=(0,i.A)(t.href)?"\ud83d\udcc4\ufe0f":"\ud83d\udd17",s=(0,n.cC)(t.docId??void 0);return(0,h.jsx)(p,{href:t.href,icon:a,title:t.label,description:t.description??s?.description})}function g(e){let{item:t}=e;switch(t.type){case"link":return(0,h.jsx)(f,{item:t});case"category":return(0,h.jsx)(m,{item:t});default:throw new Error(`unknown item type ${JSON.stringify(t)}`)}}function x(e){let{className:t}=e;const a=(0,n.$S)();return(0,h.jsx)(j,{items:a.items,className:t})}function j(e){const{items:t,className:a}=e;if(!t)return(0,h.jsx)(x,{...e});const r=(0,n.d1)(t);return(0,h.jsx)("section",{className:(0,s.A)("row",a),children:r.map(((e,t)=>(0,h.jsx)("article",{className:"col col--6 margin-bottom--lg",children:(0,h.jsx)(g,{item:e})},t)))})}},8453:(e,t,a)=>{a.d(t,{R:()=>o,x:()=>i});var s=a(6540);const n={},r=s.createContext(n);function o(e){const t=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),s.createElement(r.Provider,{value:t},e.children)}}}]);