"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[8374],{8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>s});var o=n(6540);const a={},r=o.createContext(a);function i(e){const t=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),o.createElement(r.Provider,{value:t},e.children)}},9691:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>s,default:()=>c,frontMatter:()=>i,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"apache-hadoop/hadoop-using-docker-01","title":"Hadoop using Docker 01","description":"Description","source":"@site/docs/apache-hadoop/hadoop-using-docker-01.mdx","sourceDirName":"apache-hadoop","slug":"/apache-hadoop/hadoop-using-docker-01","permalink":"/docs/apache-hadoop/hadoop-using-docker-01","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":500,"frontMatter":{"sidebar_position":500},"sidebar":"tutorialSidebar","previous":{"title":"HBase in Pseudo-Distributed Mode with Kerberos Authentication","permalink":"/docs/apache-hadoop/hbase-pseudo-distributed-mode-kerberos"},"next":{"title":"Examples","permalink":"/docs/apache-hadoop/examples"}}');var a=n(4848),r=n(8453);const i={sidebar_position:500},s="Hadoop using Docker 01",d={},l=[{value:"Description",id:"description",level:2},{value:"Objectives",id:"objectives",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Set up Cluster",id:"set-up-cluster",level:2},{value:"Explore the hadoop environment",id:"explore-the-hadoop-environment",level:2},{value:"Create a file in the HDFS",id:"create-a-file-in-the-hdfs",level:2},{value:"View the HDFS on GUI",id:"view-the-hdfs-on-gui",level:2},{value:"<em><strong>localhost:9870</strong></em>",id:"localhost9870",level:3}];function h(e){const t={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"hadoop-using-docker-01",children:"Hadoop using Docker 01"})}),"\n",(0,a.jsx)(t.h2,{id:"description",children:"Description"}),"\n",(0,a.jsxs)(t.p,{children:["We will create an instance of Hadoop Cluster within a Docker container.",(0,a.jsx)("br",{}),"\nThis Hadoop Cluster will have:"]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Namenode"}),"\n",(0,a.jsx)(t.li,{children:"Datanode"}),"\n",(0,a.jsx)(t.li,{children:"Node Manager"}),"\n",(0,a.jsx)(t.li,{children:"Resource manager"}),"\n",(0,a.jsx)(t.li,{children:"Hadoop history server"}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"objectives",children:"Objectives"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Run hadoop instance"}),"\n",(0,a.jsx)(t.li,{children:"Create files in the HDFS"}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Ubuntu"}),"\n",(0,a.jsx)(t.li,{children:"Docker"}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"set-up-cluster",children:"Set up Cluster"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Clone the repository"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"\ngit clone https://github.com/ibm-developer-skills-network/ooxwv-docker_hadoop.git\n"})}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Go to"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"cd ooxwv-docker_hadoop\n"})}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Compose the docker application"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"docker-compose up -d\n"})}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Run the namenode as a mounted drive on bash"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"docker exec -it namenode /bin/bash\n"})}),"\n",(0,a.jsx)(t.strong,{children:"Output"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"root@03d7b5c822b7:/#\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"explore-the-hadoop-environment",children:"Explore the hadoop environment"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"ls -1 /opt/hadoop-3.2.1/etc/hadoop/*.xml\n"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Output"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"# root@03d7b5c822b7:/# ls -1 /opt/hadoop-3.2.1/etc/hadoop/*.xml\n\n/opt/hadoop-3.2.1/etc/hadoop/capacity-scheduler.xml\n/opt/hadoop-3.2.1/etc/hadoop/core-site.xml\n/opt/hadoop-3.2.1/etc/hadoop/hadoop-policy.xml\n/opt/hadoop-3.2.1/etc/hadoop/hdfs-site.xml\n/opt/hadoop-3.2.1/etc/hadoop/httpfs-site.xml\n/opt/hadoop-3.2.1/etc/hadoop/kms-acls.xml\n/opt/hadoop-3.2.1/etc/hadoop/kms-site.xml\n/opt/hadoop-3.2.1/etc/hadoop/mapred-site.xml\n/opt/hadoop-3.2.1/etc/hadoop/yarn-site.xml\n"})}),"\n",(0,a.jsx)(t.h2,{id:"create-a-file-in-the-hdfs",children:"Create a file in the HDFS"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:(0,a.jsxs)(t.strong,{children:["In the HDFS, create a directory structure named ",(0,a.jsx)(t.code,{children:"user/root/input"})]})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"hdfs dfs -mkdir -p /user/root/input\n"})}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Copy all the hadoop configuration xml files into the input directory"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml /user/root/input\n"})}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Check if the files have been copied"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"hdfs dfs -ls /user/root/input\n"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Output"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"# root@03d7b5c822b7:/# hdfs dfs -ls /user/root/input\n\nFound 9 items\n-rw-r--r--   3 root supergroup       8260 2025-04-22 12:46 /user/root/input/capacity-scheduler.xml\n-rw-r--r--   3 root supergroup       1204 2025-04-22 12:46 /user/root/input/core-site.xml\n-rw-r--r--   3 root supergroup      11392 2025-04-22 12:46 /user/root/input/hadoop-policy.xml\n-rw-r--r--   3 root supergroup       1643 2025-04-22 12:46 /user/root/input/hdfs-site.xml\n-rw-r--r--   3 root supergroup        620 2025-04-22 12:46 /user/root/input/httpfs-site.xml\n-rw-r--r--   3 root supergroup       3518 2025-04-22 12:46 /user/root/input/kms-acls.xml\n-rw-r--r--   3 root supergroup        682 2025-04-22 12:46 /user/root/input/kms-site.xml\n-rw-r--r--   3 root supergroup       1647 2025-04-22 12:46 /user/root/input/mapred-site.xml\n-rw-r--r--   3 root supergroup       3484 2025-04-22 12:46 /user/root/input/yarn-site.xml\n"})}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:(0,a.jsxs)(t.strong,{children:["Create a ",(0,a.jsx)(t.code,{children:"data.txt"})," file in the current directory"]})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"# data.txt:\n\ncurl https://raw.githubusercontent.com/ibm-developer-skills-network/ooxwv-docker_hadoop/master/SampleMapReduce.txt --output data.txt\n"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Output"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"\n# root@03d7b5c822b7:/# curl https://raw.githubusercontent.com/ibm-developer-skills-network/ooxwv-docker_hadoop/master/SampleMapReduce.txt --output data.txt\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  6858  100  6858    0     0  21643      0 --:--:-- --:--:-- --:--:-- 21702\n"})}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:(0,a.jsxs)(t.strong,{children:["Copy the ",(0,a.jsx)(t.code,{children:"data.txt"})," file into ",(0,a.jsx)(t.code,{children:"/user/root"})]})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"hdfs dfs -put data.txt /user/root/\n"})}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Check if the file has been copied into the HDFS by viewing its content"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"hdfs dfs -cat /user/root/data.txt\n"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Output"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"# root@03d7b5c822b7:/# hdfs dfs -cat /user/root/data.txt\n2025-04-22 13:14:32,763 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\nWhat is big data? More than volume, velocity and variety -\n\nGet your head around the topic of big data\nBy J. Steven Perry\nPublished May 22, 2017\n\nYou\u2019ve heard of Big Data, right? We\u2019re all supposed to say yes, and Big Data is one of those topics I thought I understood until I tried explaining it. I realized that I needed to get my head around it at a high level. If you\u2019re like I was, then this blog post is for you.\n\nThe Problem(s)\nAny technology is only useful if it solves a problem (or problems). So what problem(s) does Big Data solve?\n\nAs we all know, there is data, lots of it: historical data, sure, but also new data generated from social media apps, click stream data from web applications, IoT sensor data, and on and on. The amount of data is larger than ever, coming in at ever-increasing rates, and in many different formats.\n\nThe business value in the data comes from the meaning we can harvest from it. And deriving business value from all that data is a big problem. Why? Let\u2019s break it down.\n\nData Volume\nPeople are more connected than ever before, and this interconnection leads to more and more data sources, resulting in an amount of data that is larger than ever before (and constantly growing). The increased volume of data requires ever increasing computing power in order to derive value (meaning) from the data. Traditional computing methods simply don\u2019t work on the volume of data accumulating today!\n\nData Velocity\nThe speed and directions from which data come into the enterprise is increasing due to interconnection and advances in network technology, so it is coming in faster than we can make sense out of it [2]. And the faster the data come in and more varied the sources, the harder it is to derive value (meaning) from the data. Traditional computing methods don\u2019t work on data coming in at today\u2019s speeds!\n\nData Variety\nMore sources of data means more varieties of data in different formats: from traditional documents and databases, to semi-structured and unstructured data from click streams, GPS location data, social media apps, and IoT (to name a few). Different data formats means it\u2019s tougher to derive value (meaning) from the data because it must all be extracted for processing in different ways. Traditional computing methods don\u2019t work on all these different varieties of data!\n\nWhat Big Data is NOT\nTraditional data like documents and databases\nIt\u2019s true, there are LOTS of documents and databases in the world, and while these sources contribute to Big Data, they themselves are not Big Data. The varieties of data that are being collected today is changing, and this is driving Big Data. Some of the data are structured, like traditional documents and databases but most are semi-structured, or unstructured.\n\nJust a synonym for \u201clots of data\u201d\nBig Data is much more than just a \u201clot of data\u201d.\n\nLots of data is driving Big Data, but to associate the volume of data with the term Big Data and stop there is a mistake.\n\nIt\u2019s not about the data\nBig Data is not about the data [1], any more than philosophy is about words. Big Data is about the value that can be extracted from the data, or, the MEANING contained in the data.\n\nA single technology \u2013 rather it\u2019s an entire technology ecosystem\nBig Data is a way of harvesting raw data from multiple, disparate data sources, storing the data for use by analytics programs, and using the raw data to derive value (meaning) from the data in a whole new ways. We\u2019re talking data from traditional business applications like CRM and web applications, combined with data from a growing number of sensors (IoT), and social media like Facebook, Twitter, and LinkedIn.\n\nThis means that no single technology can be called Big Data, which requires a tightly coordinated ecosystem of data acquisition, storage, and application technologies to make it work.\n\nA trend\nBig Data is the natural evolution of the way to cope with the vast quantities, types, and volume of data from today\u2019s applications. The volume, velocity and variety of data coming into today\u2019s enterprise means that these problems can only be solved by a solution that is equally organic, and capable of continued evolution.\n\nIn other words, it\u2019s the ways we are using software and creating the data that are driving Big Data.\n\nUnless we change the ways we use software (like apps), platforms (like social media), and core infrastructure technologies (like the internet), Big Data is here to stay. Case in point: Give up Snapchat? LinkedIn? Facebook? Twitter? Not gonna happen.\n\n\nThe Solution\nIn my opinion, Big Data is really a misnomer. As I mentioned earlier, Big Data is no more about the data than philosophy is about words. Big Data is about MEANING derived from the data. Maybe we should call it \u201cBig Meaning\u201d (granted, that\u2019s not as catchy, but it makes more sense to me).\n\nSo how does Big Meaning, um, I mean Big Data, solve the problems of data volume, velocity and variety?\n\nVolume\nWell, first, the data has to be stored somewhere, because without somewhere to store the data, it cannot be made available for analysis.\n\nFortunately, storage is cheaper, more reliable, and \u2013 thanks to the cloud \u2013 more accessible than ever.\n\nVelocity\nWe first need to deal with the speed at which the data comes in, and automated, intelligent systems that run lights-out, 24 x 7 x 365 help harvest patterns (meaning) in the data that would be impossible to detect through manual analysis. Advances in machine learning techniques help deal with the Velocity problem. Artificial neural networks, for example, can be trained to detect patterns, apply that knowledge to make predictions, and even adapt to the changing data on the fly.\n\nVariety\nThen there are the variety of directions (sources) from which the data come in. Patterns in the data are only good if we can look at what has happened before (historical data) and use them to predict something helpful or interesting about the future.\n\nHowever, as the variety of data sources continues to grow, so does the complexity of harvesting meaning from the data. Human beings simply cannot handle the load, which is where techniques like deep learning come into play. Deep learning networks can figure out how to make sense of the data\u2019s various input formats and feed that into other networks to harvest meaning from the data.\n\nConclusion\nThe term Big Data really means \u201charvesting meaning from data\u201d that is coming in faster, from more sources, and in more varied formats than ever before. We should probably call it Big Meaning. Because Big Data is really about the value in the data, rather than the data itself.\n\nRather than being a single technology, Big Data is an ecosystem of coordinated techniques and technologies that derive business value from the mountains of data produced in today\u2019s world.\nroot@03d7b5c822b7:/#\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"view-the-hdfs-on-gui",children:"View the HDFS on GUI"}),"\n",(0,a.jsx)(t.h3,{id:"localhost9870",children:(0,a.jsx)(t.em,{children:(0,a.jsx)(t.strong,{children:"localhost:9870"})})}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Overview"}),"\n",(0,a.jsx)("img",{src:"/img/apache-hadoop/01-hadoop-using-docker-01.png",alt:"01-hadoop-using-docker-01.png"}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsx)("br",{}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.code,{children:"/user/root/input"})}),"\n",(0,a.jsx)("img",{src:"/img/apache-hadoop/01-hadoop-using-docker-02.png",alt:"01-hadoop-using-docker-02.png"}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsx)("br",{}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.code,{children:"/user/root"})}),"\n",(0,a.jsx)("img",{src:"/img/apache-hadoop/01-hadoop-using-docker-03.png",alt:"01-hadoop-using-docker-03.png"}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsx)("br",{}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.p,{children:"Based on:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://skills.network/",children:(0,a.jsx)(t.em,{children:(0,a.jsx)(t.strong,{children:"IBM Skill Network"})})})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.em,{children:(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.a,{href:"https://github.com/ibm-developer-skills-network/ooxwv-docker_hadoop/blob/master/docker-compose.yml",children:"https://github.com/ibm-developer-skills-network/ooxwv-docker_hadoop/blob/master/docker-compose.yml"})})})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://medium.com/@garin.sanny07/hadoop-cluster-55477505d0ff",children:(0,a.jsx)(t.em,{children:(0,a.jsx)(t.strong,{children:"Sanny Garin Jr"})})})}),"\n"]})]})}function c(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}}}]);