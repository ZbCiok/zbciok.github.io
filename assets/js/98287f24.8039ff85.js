"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[5171],{5122:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"apache-hadoop/Examples/mapreduce-second-program","title":"MapReduce: the Second Program","description":"Setting up a Single Node Cluster - install and start Hadoop.","source":"@site/docs/apache-hadoop/Examples/mapreduce-second-program.mdx","sourceDirName":"apache-hadoop/Examples","slug":"/apache-hadoop/Examples/mapreduce-second-program","permalink":"/docs/apache-hadoop/Examples/mapreduce-second-program","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":200,"frontMatter":{"sidebar_position":200},"sidebar":"tutorialSidebar","previous":{"title":"MapReduce: the First Program","permalink":"/docs/apache-hadoop/Examples/mapreduce-first-program"},"next":{"title":"References","permalink":"/docs/apache-hadoop/references"}}');var r=t(4848),o=t(8453);const i={sidebar_position:200},s="MapReduce: the Second Program",c={},d=[{value:"Change user to <em><strong>hadoop</strong></em>:",id:"change-user-to-hadoop",level:4},{value:"Start Hadoop:",id:"start-hadoop",level:4},{value:"Create a new directory with name MapReduceTutorial:",id:"create-a-new-directory-with-name-mapreducetutorial",level:4},{value:"Give permissions:",id:"give-permissions",level:4},{value:"Create a new directory with name inputMapReduce:",id:"create-a-new-directory-with-name-inputmapreduce",level:4},{value:"Verify whether a file is actually copied",id:"verify-whether-a-file-is-actually-copied",level:4},{value:"Use <em><strong>copyFromLocal</strong></em> command to copy <em><strong>SalesJan2009.csv</strong></em> to HDFS <em><strong>/inputMapReduce</strong></em> directory.",id:"use-copyfromlocal-command-to-copy-salesjan2009csv-to-hdfs-inputmapreduce-directory",level:4},{value:"Verify:",id:"verify",level:4},{value:"Output:",id:"output",level:4},{value:"Java Application",id:"java-application",level:2},{value:"Program Structure",id:"program-structure",level:3},{value:"pom.xml",id:"pomxml",level:4},{value:"SalesCountryDriver.java",id:"salescountrydriverjava",level:4},{value:"SalesMapper.java",id:"salesmapperjava",level:3},{value:"SalesCountryReducer.java",id:"salescountryreducerjava",level:3},{value:"Running <em><strong>SalesCountryDriver</strong></em>",id:"running-salescountrydriver",level:2},{value:"Copy:",id:"copy",level:4},{value:"Verify:",id:"verify-1",level:4},{value:"Run MapReduce job:",id:"run-mapreduce-job",level:4},{value:"Output:",id:"output-1",level:4},{value:"The result can be seen through command interface as:",id:"the-result-can-be-seen-through-command-interface-as",level:4},{value:"Output:",id:"output-2",level:4},{value:"Results can also be seen via a web interface",id:"results-can-also-be-seen-via-a-web-interface",level:3}];function l(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"mapreduce-the-second-program",children:"MapReduce: the Second Program"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/docs/apache-hadoop/install",children:(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"Setting up a Single Node Cluster"})})})," - ",(0,r.jsx)(n.em,{children:"install and start Hadoop."}),(0,r.jsx)("br",{})]}),"\n",(0,r.jsxs)(n.h4,{id:"change-user-to-hadoop",children:["Change user to ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"hadoop"})}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"su - hadoop\n"})}),"\n",(0,r.jsx)(n.h4,{id:"start-hadoop",children:"Start Hadoop:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"sbin/start-dfs.sh\n"})}),"\n",(0,r.jsx)(n.h4,{id:"create-a-new-directory-with-name-mapreducetutorial",children:"Create a new directory with name MapReduceTutorial:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"mkdir MapReduceTutorial\n"})}),"\n",(0,r.jsx)(n.h4,{id:"give-permissions",children:"Give permissions:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"chmod -R 777 MapReduceTutorial\n"})}),"\n",(0,r.jsx)(n.h4,{id:"create-a-new-directory-with-name-inputmapreduce",children:"Create a new directory with name inputMapReduce:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hdfs dfs -mkdir /inputMapReduce\n"})}),"\n",(0,r.jsx)(n.h4,{id:"verify-whether-a-file-is-actually-copied",children:"Verify whether a file is actually copied"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hdfs dfs -ls /inputMapReduce\n"})}),"\n",(0,r.jsxs)(n.h4,{id:"use-copyfromlocal-command-to-copy-salesjan2009csv-to-hdfs-inputmapreduce-directory",children:["Use ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"copyFromLocal"})})," command to copy ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"SalesJan2009.csv"})})," to HDFS ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"/inputMapReduce"})})," directory."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hdfs dfs -copyFromLocal /home/hadoop/MapReduceTutorial/SalesJan2009.csv /inputMapReduce\n"})}),"\n",(0,r.jsx)(n.h4,{id:"verify",children:"Verify:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hdfs dfs -ls /inputMapReduce\n"})}),"\n",(0,r.jsx)(n.h4,{id:"output",children:"Output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\n-rw-r--r--   1 hadoop supergroup     123637 2025-03-05 18:35 /inputMapReduce/SalesJan2009.csv\n"})}),"\n",(0,r.jsx)(n.h2,{id:"java-application",children:"Java Application"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/ZbCiok/zjc-examples/tree/main/streams/hadoop/mapreduce-second-program",children:(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"download"})})}),(0,r.jsx)("br",{})]}),"\n",(0,r.jsx)(n.h3,{id:"program-structure",children:"Program Structure"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u251c\u2500\u2500 dependency-reduced-pom.xml\n\u251c\u2500\u2500 pom.xml\n\u251c\u2500\u2500 SalesJan2009.csv\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 main\n    \u2502\xa0\xa0 \u251c\u2500\u2500 java\n    \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 SalesCountry\n    \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 SalesCountryDriver.java\n    \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 SalesCountryReducer.java\n    \u2502\xa0\xa0 \u2502\xa0\xa0     \u2514\u2500\u2500 SalesMapper.java\n    \u2502\xa0\xa0 \u2514\u2500\u2500 resources\n    \u2514\u2500\u2500 test\n        \u2514\u2500\u2500 java\n"})}),"\n",(0,r.jsx)(n.h4,{id:"pomxml",children:"pom.xml"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>org.example</groupId>\n    <artifactId>mapreduce-first-program</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <target.java.version>11</target.java.version>\n        <maven.compiler.source>${target.java.version}</maven.compiler.source>\n        <maven.compiler.target>${target.java.version}</maven.compiler.target>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    </properties>\n\n    <dependencies>\n        \x3c!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core --\x3e\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-mapreduce-client-core</artifactId>\n            <version>3.4.1</version>\n        </dependency>\n\n        \x3c!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core --\x3e\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-core</artifactId>\n            <version>1.2.1</version>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n\n            \x3c!-- Java Compiler --\x3e\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.1</version>\n                <configuration>\n                    <source>${target.java.version}</source>\n                    <target>${target.java.version}</target>\n                </configuration>\n            </plugin>\n\n            \x3c!-- We use the maven-shade plugin to create a fat jar that contains all necessary dependencies. --\x3e\n            \x3c!-- Change the value of <mainClass>...</mainClass> if your program entry point changes. --\x3e\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>3.0.0</version>\n                <executions>\n                    \x3c!-- Run shade goal on package phase --\x3e\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <artifactSet>\n                                <excludes>\n                                    <exclude>org.apache.flink:flink-shaded-force-shading</exclude>\n                                    <exclude>com.google.code.findbugs:jsr305</exclude>\n                                    <exclude>org.slf4j:*</exclude>\n                                    <exclude>org.apache.logging.log4j:*</exclude>\n                                </excludes>\n                            </artifactSet>\n                            <filters>\n                                <filter>\n                                    \x3c!-- Do not copy the signatures in the META-INF folder.\n                                    Otherwise, this might cause SecurityExceptions when using the JAR. --\x3e\n                                    <artifact>*:*</artifact>\n                                    <excludes>\n                                        <exclude>META-INF/*.SF</exclude>\n                                        <exclude>META-INF/*.DSA</exclude>\n                                        <exclude>META-INF/*.RSA</exclude>\n                                    </excludes>\n                                </filter>\n                            </filters>\n                            <transformers>\n                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">\n                                    <mainClass>SalesCountry.SalesCountryDriver</mainClass>\n                                </transformer>\n                            </transformers>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n\n    </build>\n</project>\n'})}),"\n",(0,r.jsx)(n.h4,{id:"salescountrydriverjava",children:"SalesCountryDriver.java"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'\npackage SalesCountry;\n\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapred.*;\n\npublic class SalesCountryDriver {\n\tpublic static void main(String[] args) {\n\t\tJobClient my_client = new JobClient();\n\t\t// Create a configuration object for the job\n\t\tJobConf job_conf = new JobConf(SalesCountryDriver.class);\n\n\t\t// Set a name of the Job\n\t\tjob_conf.setJobName("SalePerCountry");\n\n\t\t// Specify data type of output key and value\n\t\tjob_conf.setOutputKeyClass(Text.class);\n\t\tjob_conf.setOutputValueClass(IntWritable.class);\n\n\t\t// Specify names of Mapper and Reducer Class\n\t\tjob_conf.setMapperClass(SalesCountry.SalesMapper.class);\n\t\tjob_conf.setReducerClass(SalesCountry.SalesCountryReducer.class);\n\n\t\t// Specify formats of the data type of Input and output\n\t\tjob_conf.setInputFormat(TextInputFormat.class);\n\t\tjob_conf.setOutputFormat(TextOutputFormat.class);\n\n\t\t// Set input and output directories using command line arguments,\n\t\t//arg[0] = name of input directory on HDFS, and arg[1] =  name of output directory to be created to store the output file.\n\n\t\tFileInputFormat.setInputPaths(job_conf, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job_conf, new Path(args[1]));\n\n\t\tmy_client.setConf(job_conf);\n\t\ttry {\n\t\t\t// Run the job\n\t\t\tJobClient.runJob(job_conf);\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"salesmapperjava",children:"SalesMapper.java"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'package SalesCountry;\n\nimport java.io.IOException;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapred.*;\n\npublic class SalesMapper extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\n\tprivate final static IntWritable one = new IntWritable(1);\n\n\tpublic void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n\n\t\tString valueString = value.toString();\n\t\tString[] SingleCountryData = valueString.split(",");\n\t\toutput.collect(new Text(SingleCountryData[7]), one);\n\t}\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"salescountryreducerjava",children:"SalesCountryReducer.java"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"package SalesCountry;\n\nimport java.io.IOException;\nimport java.util.*;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapred.*;\n\npublic class SalesCountryReducer extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\n\n\tpublic void reduce(Text t_key, Iterator<IntWritable> values, OutputCollector<Text,IntWritable> output, Reporter reporter) throws IOException {\n\t\tText key = t_key;\n\t\tint frequencyForCountry = 0;\n\t\twhile (values.hasNext()) {\n\t\t\t// replace type of value with the actual type of our value\n\t\t\tIntWritable value = (IntWritable) values.next();\n\t\t\tfrequencyForCountry += value.get();\n\n\t\t}\n\t\toutput.collect(key, new IntWritable(frequencyForCountry));\n\t}\n}\n"})}),"\n",(0,r.jsxs)(n.h2,{id:"running-salescountrydriver",children:["Running ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"SalesCountryDriver"})})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"mvn clean package\n"})}),"\n",(0,r.jsx)(n.h4,{id:"copy",children:"Copy:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"/target/mapreduce-first-program-1.0-SNAPSHOT.jar"})}),(0,r.jsx)("br",{}),"\nto the ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"hadoop"})}),(0,r.jsx)("br",{}),"\n",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"/MapReduceTutorial"})})]}),"\n",(0,r.jsx)(n.h4,{id:"verify-1",children:"Verify:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hdfs dfs -ls /inputMapReduce\n"})}),"\n",(0,r.jsx)(n.h4,{id:"run-mapreduce-job",children:"Run MapReduce job:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\nhadoop jar /home/hadoop/MapReduceTutorial/mapreduce-first-program-1.0-SNAPSHOT.jar /inputMapReduce /mapreduce_output_sales\n"})}),"\n",(0,r.jsx)(n.h4,{id:"output-1",children:"Output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\n./hadoop jar /home/hadoop/MapReduceTutorial/mapreduce-first-program-1-1.0-SNAPSHOT.jar /inputMapReduce /mapreduce_output_sales\n2025-03-05 18:43:07,175 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n2025-03-05 18:43:07,215 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n2025-03-05 18:43:07,215 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n2025-03-05 18:43:07,223 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n\n...\n...\n\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters\n\t\tBytes Read=123637\n\tFile Output Format Counters\n\t\tBytes Written=661\n"})}),"\n",(0,r.jsx)(n.h4,{id:"the-result-can-be-seen-through-command-interface-as",children:"The result can be seen through command interface as:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"hdfs dfs -cat /mapreduce_output_sales/part-00000\n"})}),"\n",(0,r.jsx)(n.h4,{id:"output-2",children:"Output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Argentina\t1\nAustralia\t38\nAustria\t7\nBahrain\t1\nBelgium\t8\nBermuda\t1\nBrazil\t5\nBulgaria\t1\nCO\t1\nCanada\t76\nCayman Isls\t1\nChina\t1\nCosta Rica\t1\nCountry\t1\nCzech Republic\t3\nDenmark\t15\nDominican Republic\t1\nFinland\t2\nFrance\t27\nGermany\t25\nGreece\t1\nGuatemala\t1\nHong Kong\t1\nHungary\t3\nIceland\t1\nIndia\t2\nIreland\t49\nIsrael\t1\nItaly\t15\nJapan\t2\nJersey\t1\nKuwait\t1\nLatvia\t1\nLuxembourg\t1\nMalaysia\t1\nMalta\t2\nMauritius\t1\nMoldova\t1\nMonaco\t2\nNetherlands\t22\nNew Zealand\t6\nNorway\t16\nPhilippines\t2\nPoland\t2\nRomania\t1\nRussia\t1\nSouth Africa\t5\nSouth Korea\t1\nSpain\t12\nSweden\t13\nSwitzerland\t36\nThailand\t2\nThe Bahamas\t2\nTurkey\t6\nUkraine\t1\nUnited Arab Emirates\t6\nUnited Kingdom\t100\nUnited States\t462\n"})}),"\n",(0,r.jsx)(n.h3,{id:"results-can-also-be-seen-via-a-web-interface",children:"Results can also be seen via a web interface"}),"\n",(0,r.jsx)("img",{src:"\\img\\apache-hadoop\\browse-directory-01.png",alt:"browse-directory-01.png"})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>s});var a=t(6540);const r={},o=a.createContext(r);function i(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);