"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[7873],{1181:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>a,default:()=>c,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"apache-hadoop/install","title":"Setting up a Single Node Cluster","description":"https://hadoop.apache.org/","source":"@site/docs/apache-hadoop/install.mdx","sourceDirName":"apache-hadoop","slug":"/apache-hadoop/install","permalink":"/docs/apache-hadoop/install","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":100,"frontMatter":{"sidebar_position":100},"sidebar":"tutorialSidebar","previous":{"title":"Apache Hadoop","permalink":"/docs/apache-hadoop"},"next":{"title":"Examples","permalink":"/docs/apache-hadoop/examples"}}');var o=t(4848),d=t(8453);const r={sidebar_position:100},a="Setting up a Single Node Cluster",i={},l=[{value:"Supported Platforms",id:"supported-platforms",level:3},{value:"Required Software",id:"required-software",level:3},{value:"Download and Install",id:"download-and-install",level:3},{value:"After Installing <em><strong>(as hadoop user)</strong></em>",id:"after-installing-as-hadoop-user",level:3},{value:"Format the filesystem:",id:"format-the-filesystem",level:4},{value:"Start NameNode daemon and DataNode daemon:",id:"start-namenode-daemon-and-datanode-daemon",level:4},{value:"Browse",id:"browse",level:4},{value:"sidebar_position: 100",id:"sidebar_position-100",level:2},{value:"Make the HDFS directories required to execute MapReduce jobs:",id:"make-the-hdfs-directories-required-to-execute-mapreduce-jobs",level:4},{value:"Copy the input files into the distributed filesystem:",id:"copy-the-input-files-into-the-distributed-filesystem",level:4},{value:"Run some of the examples provided:",id:"run-some-of-the-examples-provided",level:4},{value:"Examine the output files:",id:"examine-the-output-files",level:4},{value:"View the output files on the distributed filesystem:",id:"view-the-output-files-on-the-distributed-filesystem",level:4},{value:"When you\u2019re done, stop the daemons with:",id:"when-youre-done-stop-the-daemons-with",level:4}];function h(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,d.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"setting-up-a-single-node-cluster",children:"Setting up a Single Node Cluster"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.em,{children:(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.a,{href:"https://hadoop.apache.org/",children:"https://hadoop.apache.org/"})})})," ",(0,o.jsx)("br",{}),"\n",(0,o.jsx)(n.a,{href:"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Purpose",children:(0,o.jsx)(n.em,{children:(0,o.jsx)(n.strong,{children:"Hadoop: Setting up a Single Node Cluster."})})})," ",(0,o.jsx)("br",{}),"\n",(0,o.jsx)(n.a,{href:"https://hadoop.apache.org/docs/r3.4.1/hadoop-project-dist/hadoop-common/SecureMode.html",children:(0,o.jsx)(n.em,{children:(0,o.jsx)(n.strong,{children:"Hadoop in Secure Mode"})})})," ",(0,o.jsx)("br",{}),"\n",(0,o.jsx)(n.a,{href:"https://medium.com/@nsidana123/installing-latest-hadoop-3-4-on-ubuntu-2024-easy-installation-guide-874f889fede7",children:(0,o.jsx)(n.em,{children:(0,o.jsx)(n.strong,{children:"Installing latest Hadoop 3.4 on Ubuntu 2024: Easy Installation Guide"})})})," ",(0,o.jsx)("br",{})]}),"\n",(0,o.jsx)(n.admonition,{type:"important",children:(0,o.jsx)(n.p,{children:"All production Hadoop clusters use Kerberos to authenticate callers and secure access to HDFS data as well as restriction access to computation services (YARN etc.)."})}),"\n",(0,o.jsx)(n.h3,{id:"supported-platforms",children:"Supported Platforms"}),"\n",(0,o.jsx)(n.p,{children:"GNU/Linux is supported as a development and production platform. Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes."}),"\n",(0,o.jsx)(n.h3,{id:"required-software",children:"Required Software"}),"\n",(0,o.jsx)(n.p,{children:"Required software for Linux include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Java\u2122 must be installed. Recommended Java versions are described at ",(0,o.jsx)(n.a,{href:"https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions",children:(0,o.jsx)(n.em,{children:(0,o.jsx)(n.strong,{children:"HadoopJavaVersions."})})})]}),"\n",(0,o.jsx)(n.li,{children:"ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons if the optional start and stop scripts are to be used. Additionally, it is recommmended that pdsh also be installed for better ssh resource management."}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"download-and-install",children:"Download and Install"}),"\n",(0,o.jsxs)(n.p,{children:["To get a Hadoop distribution, download a recent stable release from one of the ",(0,o.jsx)(n.a,{href:"http://www.apache.org/dyn/closer.cgi/hadoop/common/",children:(0,o.jsx)(n.em,{children:(0,o.jsx)(n.strong,{children:"Apache Download Mirrors"})})}),"."]}),"\n",(0,o.jsxs)(n.h3,{id:"after-installing-as-hadoop-user",children:["After Installing ",(0,o.jsx)(n.em,{children:(0,o.jsx)(n.strong,{children:"(as hadoop user)"})})]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:".\n\u251c\u2500\u2500 hadoop\n\u2502\xa0\xa0 \u251c\u2500\u2500 bin\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 container-executor\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 hadoop\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 hadoop.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 hdfs\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 hdfs.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 mapred\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 mapred.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 oom-listener\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 test-container-executor\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 yarn\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 yarn.cmd\n\u2502\xa0\xa0 \u251c\u2500\u2500 etc\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 hadoop\n\u2502\xa0\xa0 \u2502\xa0\xa0     \u2514\u2500\u2500 shellprofile.d\n\u2502\xa0\xa0 \u251c\u2500\u2500 include\n\u2502\xa0\xa0 \u251c\u2500\u2500 lib\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 native\n\u2502\xa0\xa0 \u2502\xa0\xa0     \u2514\u2500\u2500 examples\n\u2502\xa0\xa0 \u251c\u2500\u2500 libexec\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 shellprofile.d\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 tools\n\u2502\xa0\xa0 \u251c\u2500\u2500 licenses-binary\n\u2502\xa0\xa0 \u251c\u2500\u2500 logs\n\u2502\xa0\xa0 \u251c\u2500\u2500 sbin\n\u2502\xa0\xa0 \u251c\u2500\u2500 sbin\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 distribute-exclude.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 FederationStateStore\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 MySQL\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dropDatabase.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dropStoreProcedures.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dropTables.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dropUser.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 FederationStateStoreDatabase.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 FederationStateStoreStoredProcs.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 FederationStateStoreTables.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 FederationStateStoreUser.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 SQLServer\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 dropDatabase.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 dropStoreProcedures.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 dropTables.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 dropUser.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 FederationStateStoreDatabase.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 FederationStateStoreStoredProcs.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 FederationStateStoreTables.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u2514\u2500\u2500 FederationStateStoreUser.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 hadoop-daemon.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 hadoop-daemons.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 httpfs.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 kms.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 mr-jobhistory-daemon.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 refresh-namenodes.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-all.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-all.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-balancer.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-dfs.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-dfs.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-secure-dns.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-yarn.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-yarn.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-all.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-all.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-balancer.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-dfs.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-dfs.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-secure-dns.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-yarn.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-yarn.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 workers.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 yarn-daemon.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 yarn-daemons.sh\n...\n...\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 FederationStateStore\n\u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 MySQL\n\u2502\xa0\xa0 \u2502\xa0\xa0     \u2514\u2500\u2500 SQLServer\n\u2502\xa0\xa0 \u2514\u2500\u2500 share\n\u2502\xa0\xa0     \u251c\u2500\u2500 doc\n...\n...\n\u2502\xa0\xa0                 \u2514\u2500\u2500 sleeper\n\u2514\u2500\u2500 hadoopdata\n    \u2514\u2500\u2500 hdfs\n        \u251c\u2500\u2500 datanode\n        \u2502\xa0\xa0 \u2514\u2500\u2500 current\n        \u2502\xa0\xa0     \u2514\u2500\u2500 BP-737539575-192.168.1.19-1741088952847\n        \u2502\xa0\xa0         \u251c\u2500\u2500 current\n        \u2502\xa0\xa0         \u2502\xa0\xa0 \u251c\u2500\u2500 finalized\n        \u2502\xa0\xa0         \u2502\xa0\xa0 \u2514\u2500\u2500 rbw\n        \u2502\xa0\xa0         \u2514\u2500\u2500 tmp\n        \u2514\u2500\u2500 namenode\n            \u2514\u2500\u2500 current\n"})}),"\n",(0,o.jsx)(n.h4,{id:"format-the-filesystem",children:"Format the filesystem:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"bin/hdfs namenode -format\n"})}),"\n",(0,o.jsx)(n.h4,{id:"start-namenode-daemon-and-datanode-daemon",children:"Start NameNode daemon and DataNode daemon:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"sbin/start-dfs.sh\n"})}),"\n",(0,o.jsx)(n.h4,{id:"browse",children:"Browse"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"http://localhost:9870/\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"sidebar_position-100",children:"sidebar_position: 100"}),"\n",(0,o.jsx)(n.h4,{id:"make-the-hdfs-directories-required-to-execute-mapreduce-jobs",children:"Make the HDFS directories required to execute MapReduce jobs:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"bin/hdfs dfs -mkdir -p /user/<username>\n"})}),"\n",(0,o.jsx)(n.h4,{id:"copy-the-input-files-into-the-distributed-filesystem",children:"Copy the input files into the distributed filesystem:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"bin/hdfs dfs -mkdir input\nbin/hdfs dfs -put etc/hadoop/*.xml input\n"})}),"\n",(0,o.jsx)(n.h4,{id:"run-some-of-the-examples-provided",children:"Run some of the examples provided:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.0.jar grep input output 'dfs[a-z.]+'\n"})}),"\n",(0,o.jsx)(n.h4,{id:"examine-the-output-files",children:"Examine the output files:"}),"\n",(0,o.jsx)(n.p,{children:"Copy the output files from the distributed filesystem to the local filesystem and examine them:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"bin/hdfs dfs -get output output $ cat output/*\n"})}),"\n",(0,o.jsx)(n.h4,{id:"view-the-output-files-on-the-distributed-filesystem",children:"View the output files on the distributed filesystem:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"bin/hdfs dfs -cat output/*\n"})}),"\n",(0,o.jsx)(n.h4,{id:"when-youre-done-stop-the-daemons-with",children:"When you\u2019re done, stop the daemons with:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"stop-dfs.sh\n"})})]})}function c(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const o={},d=s.createContext(o);function r(e){const n=s.useContext(d);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(d.Provider,{value:n},e.children)}}}]);