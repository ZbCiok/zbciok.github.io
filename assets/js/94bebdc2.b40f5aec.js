"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[7478],{2121:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var r=t(4848),i=t(8453);const a={sidebar_position:80},o="Flink Kafka Word Count",s={id:"streams/apache-flink/Examples/flink-kafka-word-count",title:"Flink Kafka Word Count",description:"Source, operator and sink in DataStream API",source:"@site/docs/streams/apache-flink/Examples/flink-kafka-word-count.mdx",sourceDirName:"streams/apache-flink/Examples",slug:"/streams/apache-flink/Examples/flink-kafka-word-count",permalink:"/docs/streams/apache-flink/Examples/flink-kafka-word-count",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:80,frontMatter:{sidebar_position:80},sidebar:"tutorialSidebar",previous:{title:"Taxi Ride Generator",permalink:"/docs/streams/apache-flink/Examples/taxi-ride-generator"},next:{title:"Stock Price Streams",permalink:"/docs/streams/apache-flink/Examples/stock-price-streams"}},c={},l=[{value:"Description",id:"description",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Data Setup",id:"data-setup",level:2},{value:"Create topics:",id:"create-topics",level:4},{value:"Project Structure",id:"project-structure",level:2},{value:"WordCount.java",id:"wordcountjava",level:4},{value:"Run",id:"run",level:2},{value:"Input",id:"input",level:4},{value:"Output",id:"output",level:4}];function u(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"flink-kafka-word-count",children:"Flink Kafka Word Count"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://docs.cloudera.com/csa/1.11.0/how-to-flink/topics/csa-datastream-dev.html",children:(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"Source, operator and sink in DataStream API"})})})," ",(0,r.jsx)("br",{})]}),"\n",(0,r.jsx)(n.h2,{id:"description",children:"Description"}),"\n",(0,r.jsx)(n.p,{children:"This is a re-write of the Apache Flink WordCount example using Kafka connectors."}),"\n",(0,r.jsx)("img",{src:"/img/streams/apache-flink/flink-kafka-word-count-01.png",alt:"flink-kafka-word-count-01.png"}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"JDK 11"}),"\n",(0,r.jsx)(n.li,{children:"Maven"}),"\n",(0,r.jsx)(n.li,{children:"IDE (IntelliJ IDEA)"}),"\n",(0,r.jsx)(n.li,{children:"Flink 1.17.1"}),"\n",(0,r.jsx)(n.li,{children:"Docker / Docker Desktop"}),"\n",(0,r.jsx)(n.li,{children:"Docker apache/kafka:3.9.0"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"data-setup",children:"Data Setup"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"docker pull apache/kafka:3.9.0\ndocker run -p 9092:9092 apache/kafka:3.9.0\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'docker ps\nCONTAINER ID   IMAGE                COMMAND                  CREATED       STATUS       PORTS                    NAMES\n5076e71c54cd   apache/kafka:3.9.0   "/__cacert_entrypoin\u2026"   5 hours ago   Up 5 hours   0.0.0.0:9092->9092/tcp   fervent_dijkstra\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"In our case (above):"})," ",(0,r.jsx)("br",{}),"\nContainer name: ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"fervent_dijkstra"})})]}),"\n",(0,r.jsx)(n.h4,{id:"create-topics",children:"Create topics:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"docker exec -it fervent_dijkstra /bin/bash\ncd /opt/kafka\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"bin/kafka-topics.sh --create --topic inputTopic --bootstrap-server localhost:9092\nbin/kafka-topics.sh --create --topic outputTopic --bootstrap-server localhost:9092\n"})}),"\n",(0,r.jsx)(n.h2,{id:"project-structure",children:"Project Structure"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:".\n\u251c\u2500\u2500 pom.xml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src\n\u2502\xa0\xa0 \u2514\u2500\u2500 main\n\u2502\xa0\xa0     \u251c\u2500\u2500 java\n\u2502\xa0\xa0     \u2502\xa0\xa0 \u2514\u2500\u2500 io\n\u2502\xa0\xa0     \u2502\xa0\xa0     \u2514\u2500\u2500 redpanda\n\u2502\xa0\xa0     \u2502\xa0\xa0         \u2514\u2500\u2500 examples\n\u2502\xa0\xa0     \u2502\xa0\xa0             \u2514\u2500\u2500 WordCount.java\n\u2502\xa0\xa0     \u2514\u2500\u2500 resources\n\u2502\xa0\xa0         \u2514\u2500\u2500 log4j2.properties\n\u2514\u2500\u2500 target\n\n"})}),"\n",(0,r.jsx)(n.h4,{id:"wordcountjava",children:"WordCount.java"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'public class WordCount {\n\n\tfinal static String inputTopic = "inputTopic";\n\tfinal static String outputTopic = "outputTopic";\n\tfinal static String jobTitle = "WordCount";\n\n\tpublic static void main(String[] args) throws Exception {\n\t    final String bootstrapServers = args.length > 0 ? args[0] : "localhost:9092";\n\n\t\t// Set up the streaming execution environment\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n\t\tKafkaSource<String> source = KafkaSource.<String>builder()\n\t\t    .setBootstrapServers(bootstrapServers)\n\t\t    .setTopics(inputTopic)\n\t\t    .setGroupId("my-group")\n\t\t    .setStartingOffsets(OffsetsInitializer.earliest())\n\t\t    .setValueOnlyDeserializer(new SimpleStringSchema())\n\t\t    .build();\n\n\t\tKafkaRecordSerializationSchema<String> serializer = KafkaRecordSerializationSchema.builder()\n\t\t\t.setValueSerializationSchema(new SimpleStringSchema())\n\t\t\t.setTopic(outputTopic)\n\t\t\t.build();\n\n\t\tKafkaSink<String> sink = KafkaSink.<String>builder()\n\t\t\t.setBootstrapServers(bootstrapServers)\n\t\t\t.setRecordSerializer(serializer)\n\t\t\t.build();\n\n\t\tDataStream<String> text = env.fromSource(source, WatermarkStrategy.noWatermarks(), "Kafka Source");\n\n\n\t\t// Split up the lines in pairs (2-tuples) containing: (word,1)\n        DataStream<String> counts = text.flatMap(new Tokenizer())\n\t\t// Group by the tuple field "0" and sum up tuple field "1"\n\t\t.keyBy(value -> value.f0)\n\t\t.sum(1)\n\t\t.flatMap(new Reducer());\n\n\t\t// Add the sink to so results\n\t\t// are written to the outputTopic\n        counts.sinkTo(sink);\n\n\t\t// Execute program\n\t\tenv.execute(jobTitle);\n\t}\n\n    /**\n     * Implements the string tokenizer that splits sentences into words as a user-defined\n     * FlatMapFunction. The function takes a line (String) and splits it into multiple pairs in the\n     * form of "(word,1)" ({@code Tuple2<String, Integer>}).\n     */\n    public static final class Tokenizer\n            implements FlatMapFunction<String, Tuple2<String, Integer>> {\n\n        @Override\n        public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {\n            // Normalize and split the line\n            String[] tokens = value.toLowerCase().split("\\\\W+");\n\n            // Emit the pairs\n            for (String token : tokens) {\n                if (token.length() > 0) {\n                    out.collect(new Tuple2<>(token, 1));\n                }\n            }\n        }\n    }\n\n    // Implements a simple reducer using FlatMap to\n    // reduce the Tuple2 into a single string for\n    // writing to kafka topics\n    public static final class Reducer\n            implements FlatMapFunction<Tuple2<String, Integer>, String> {\n\n        @Override\n        public void flatMap(Tuple2<String, Integer> value, Collector<String> out) {\n        \t// Convert the pairs to a string\n        \t// for easy writing to Kafka Topic\n        \tString count = value.f0 + " " + value.f1;\n        \tout.collect(count);\n        }\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h2,{id:"run",children:"Run"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"# Run\nWordCount.java\n"})}),"\n",(0,r.jsx)(n.h4,{id:"input",children:"Input"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\nbin/kafka-console-producer.sh --topic inputTopic --bootstrap-server localhost:9092\n\n>a\n>a b\n>a b c\n>a b c d\n>\n"})}),"\n",(0,r.jsx)(n.h4,{id:"output",children:"Output"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"# Next terminal\nbin/kafka-console-consumer.sh --topic outputTopic --from-beginning --bootstrap-server localhost:9092\n\na 1\na 2\nb 1\na 3\nb 2\nc 1\nc 2\nb 3\na 4\nd 1\n\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Source code:"}),(0,r.jsx)("br",{})," ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://github.com/ZbCiok/zjc-examples/tree/main/streams/flink/flink-kafka-word-count",children:"https://github.com/ZbCiok/zjc-examples/tree/main/streams/flink/flink-kafka-word-count"})})})]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var r=t(6540);const i={},a=r.createContext(i);function o(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);