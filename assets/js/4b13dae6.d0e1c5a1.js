"use strict";(self.webpackChunkjreact_com_docsaurus_01=self.webpackChunkjreact_com_docsaurus_01||[]).push([[7873],{1181:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>i,contentTitle:()=>t,default:()=>c,frontMatter:()=>d,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"apache-hadoop/install","title":"Setting up a Single Node Cluster","description":"How to Install Hadoop on Ubuntu","source":"@site/docs/apache-hadoop/install.mdx","sourceDirName":"apache-hadoop","slug":"/apache-hadoop/install","permalink":"/docs/apache-hadoop/install","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":100,"frontMatter":{"sidebar_position":100},"sidebar":"tutorialSidebar","previous":{"title":"Apache Hadoop","permalink":"/docs/apache-hadoop"},"next":{"title":"SQOOP","permalink":"/docs/apache-hadoop/mapreduce-sqoop"}}');var a=s(4848),r=s(8453);const d={sidebar_position:100},t="Setting up a Single Node Cluster",i={},l=[{value:"Supported Platforms",id:"supported-platforms",level:3},{value:"Required Software",id:"required-software",level:3},{value:"Download and Install",id:"download-and-install",level:2},{value:"At the moment, Apache Hadoop 3.x fully supports Java 8 and 11",id:"at-the-moment-apache-hadoop-3x-fully-supports-java-8-and-11",level:3},{value:"In our case:",id:"in-our-case",level:4},{value:"Set Up Hadoop User and Configure SSH",id:"set-up-hadoop-user-and-configure-ssh",level:3},{value:"Create Hadoop User",id:"create-hadoop-user",level:4},{value:"Install OpenSSH",id:"install-openssh",level:4},{value:"Switch to the newly created user",id:"switch-to-the-newly-created-user",level:4},{value:"Enable Passwordless SSH for Hadoop User",id:"enable-passwordless-ssh-for-hadoop-user",level:4},{value:"Download and Install Hadoop on Ubuntu",id:"download-and-install-hadoop-on-ubuntu",level:3},{value:"Configure Single Node Hadoop",id:"configure-single-node-hadoop",level:3},{value:".bashrc",id:"bashrc",level:4},{value:"hadoop-env.sh",id:"hadoop-envsh",level:4},{value:"core-site.xml",id:"core-sitexml",level:4},{value:"hdfs-site.xml",id:"hdfs-sitexml",level:4},{value:"mapred-site.xml",id:"mapred-sitexml",level:4},{value:"yarn-site.xml",id:"yarn-sitexml",level:4},{value:"Format HDFS NameNode",id:"format-hdfs-namenode",level:3},{value:"Start Hadoop Cluster",id:"start-hadoop-cluster",level:3},{value:"Start YARN",id:"start-yarn",level:3},{value:"Run the following command to check if all the daemons are active and running as Java processes:",id:"run-the-following-command-to-check-if-all-the-daemons-are-active-and-running-as-java-processes",level:3},{value:"Access Hadoop from Browser",id:"access-hadoop-from-browser",level:3},{value:"Selected Commands",id:"selected-commands",level:3},{value:"~/hadoop-3.4.1/sbin",id:"hadoop-341sbin",level:4},{value:"~/hadoop-3.4.1/bin",id:"hadoop-341bin",level:4},{value:"After Installing <em><strong>(as hadoop user)</strong></em>",id:"after-installing-as-hadoop-user",level:3},{value:"sidebar_position: 100",id:"sidebar_position-100",level:2},{value:"Make the HDFS directories required to execute MapReduce jobs:",id:"make-the-hdfs-directories-required-to-execute-mapreduce-jobs",level:4},{value:"Copy the input files into the distributed filesystem:",id:"copy-the-input-files-into-the-distributed-filesystem",level:4},{value:"Run some of the examples provided:",id:"run-some-of-the-examples-provided",level:4},{value:"Examine the output files:",id:"examine-the-output-files",level:4},{value:"View the output files on the distributed filesystem:",id:"view-the-output-files-on-the-distributed-filesystem",level:4},{value:"When you\u2019re done, stop the daemons with:",id:"when-youre-done-stop-the-daemons-with",level:4}];function h(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"setting-up-a-single-node-cluster",children:"Setting up a Single Node Cluster"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://phoenixnap.com/kb/install-hadoop-ubuntu",children:(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"How to Install Hadoop on Ubuntu"})})})," ",(0,a.jsx)("br",{}),"\n",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"https://hadoop.apache.org/",children:"https://hadoop.apache.org/"})})})," ",(0,a.jsx)("br",{}),"\n",(0,a.jsx)(n.a,{href:"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Purpose",children:(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"Hadoop: Setting up a Single Node Cluster."})})})," ",(0,a.jsx)("br",{}),"\n",(0,a.jsx)(n.a,{href:"https://hadoop.apache.org/docs/r3.4.1/hadoop-project-dist/hadoop-common/SecureMode.html",children:(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"Hadoop in Secure Mode"})})})," ",(0,a.jsx)("br",{})]}),"\n",(0,a.jsx)(n.admonition,{type:"important",children:(0,a.jsx)(n.p,{children:"All production Hadoop clusters use Kerberos to authenticate callers and secure access to HDFS data as well as restriction access to computation services (YARN etc.)."})}),"\n",(0,a.jsx)(n.h3,{id:"supported-platforms",children:"Supported Platforms"}),"\n",(0,a.jsx)(n.p,{children:"GNU/Linux is supported as a development and production platform. Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes."}),"\n",(0,a.jsx)(n.h3,{id:"required-software",children:"Required Software"}),"\n",(0,a.jsx)(n.p,{children:"Required software for Linux include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Java\u2122 must be installed. Recommended Java versions are described at ",(0,a.jsx)(n.a,{href:"https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions",children:(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"HadoopJavaVersions."})})})]}),"\n",(0,a.jsx)(n.li,{children:"ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons if the optional start and stop scripts are to be used. Additionally, it is recommmended that pdsh also be installed for better ssh resource management."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"download-and-install",children:"Download and Install"}),"\n",(0,a.jsx)(n.h3,{id:"at-the-moment-apache-hadoop-3x-fully-supports-java-8-and-11",children:"At the moment, Apache Hadoop 3.x fully supports Java 8 and 11"}),"\n",(0,a.jsx)(n.h4,{id:"in-our-case",children:"In our case:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"sudo update-java-alternatives --list\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"java-1.11.0-openjdk-amd64      1111       /usr/lib/jvm/java-1.11.0-openjdk-amd64\njava-1.17.0-openjdk-amd64      1711       /usr/lib/jvm/java-1.17.0-openjdk-amd64\njava-1.21.0-openjdk-amd64      2111       /usr/lib/jvm/java-1.21.0-openjdk-amd64\njava-1.8.0-openjdk-amd64       1081       /usr/lib/jvm/java-1.8.0-openjdk-amd64\n"})}),"\n",(0,a.jsxs)(n.p,{children:["We set ",(0,a.jsx)(n.code,{children:"java-1.11.0-openjdk-amd64"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"sudo update-java-alternatives --set java-1.11.0-openjdk-amd64\n"})}),"\n",(0,a.jsx)(n.p,{children:"Verify:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"java -version\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'openjdk version "11.0.26" 2025-01-21\nOpenJDK Runtime Environment (build 11.0.26+4-post-Ubuntu-1ubuntu122.04)\nOpenJDK 64-Bit Server VM (build 11.0.26+4-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"set-up-hadoop-user-and-configure-ssh",children:"Set Up Hadoop User and Configure SSH"}),"\n",(0,a.jsx)(n.h4,{id:"create-hadoop-user",children:"Create Hadoop User"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"sudo adduser hadoop\n"})}),"\n",(0,a.jsx)(n.h4,{id:"install-openssh",children:"Install OpenSSH"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"sudo apt install openssh-server openssh-client -y\n"})}),"\n",(0,a.jsx)(n.h4,{id:"switch-to-the-newly-created-user",children:"Switch to the newly created user"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"su - hadoop\n"})}),"\n",(0,a.jsx)(n.h4,{id:"enable-passwordless-ssh-for-hadoop-user",children:"Enable Passwordless SSH for Hadoop User"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"chmod 0600 ~/.ssh/authorized_keys\n"})}),"\n",(0,a.jsx)(n.p,{children:"The new user can now SSH without entering a password every time. Verify everything is set up correctly by using the hadoop user to SSH to localhost:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"ssh localhost\n"})}),"\n",(0,a.jsx)(n.h3,{id:"download-and-install-hadoop-on-ubuntu",children:"Download and Install Hadoop on Ubuntu"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"wget https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"tar xzf hadoop-3.4.1.tar.gz\n"})}),"\n",(0,a.jsx)(n.h3,{id:"configure-single-node-hadoop",children:"Configure Single Node Hadoop"}),"\n",(0,a.jsx)(n.h4,{id:"bashrc",children:".bashrc"}),"\n",(0,a.jsxs)(n.p,{children:["Go to ",(0,a.jsx)(n.code,{children:"/home/hadoop"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"nano .bashrc\n"})}),"\n",(0,a.jsx)(n.p,{children:"and add:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'export HADOOP_HOME=/home/hadoop/hadoop-3.4.1\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nexport HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Run the command below to apply the changes to the current running environment (",(0,a.jsx)(n.code,{children:"/home/hadoop/"}),"):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"source ./.bashrc\n"})}),"\n",(0,a.jsx)(n.h4,{id:"hadoop-envsh",children:"hadoop-env.sh"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n"})}),"\n",(0,a.jsxs)(n.p,{children:["add",(0,a.jsx)("br",{}),"\n",(0,a.jsx)(n.code,{children:"export JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64"})," - in our case.",(0,a.jsx)("br",{})]}),"\n",(0,a.jsx)(n.h4,{id:"core-sitexml",children:"core-site.xml"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"nano $HADOOP_HOME/etc/hadoop/core-site.xml\n"})}),"\n",(0,a.jsxs)(n.p,{children:["add",(0,a.jsx)("br",{})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"<configuration>\n<property>\n  <name>hadoop.tmp.dir</name>\n  <value>/home/hadoop/tmpdata</value>\n</property>\n<property>\n  <name>fs.default.name</name>\n  <value>hdfs://127.0.0.1:9000</value>\n</property>\n</configuration>\n"})}),"\n",(0,a.jsx)(n.h4,{id:"hdfs-sitexml",children:"hdfs-site.xml"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml\n"})}),"\n",(0,a.jsxs)(n.p,{children:["add",(0,a.jsx)("br",{})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"<configuration>\n<property>\n  <name>dfs.data.dir</name>\n  <value>/home/hadoop/dfsdata/namenode</value>\n</property>\n<property>\n  <name>dfs.data.dir</name>\n  <value>/home/hadoop/dfsdata/datanode</value>\n</property>\n<property>\n  <name>dfs.replication</name>\n  <value>1</value>\n</property>\n</configuration>\n"})}),"\n",(0,a.jsx)(n.h4,{id:"mapred-sitexml",children:"mapred-site.xml"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"nano $HADOOP_HOME/etc/hadoop/mapred-site.xml\n"})}),"\n",(0,a.jsxs)(n.p,{children:["add",(0,a.jsx)("br",{})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"<configuration>\n<property>\n  <name>mapreduce.framework.name</name>\n  <value>yarn</value>\n</property>\n</configuration>\n"})}),"\n",(0,a.jsx)(n.h4,{id:"yarn-sitexml",children:"yarn-site.xml"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"nano $HADOOP_HOME/etc/hadoop/yarn-site.xml\n"})}),"\n",(0,a.jsxs)(n.p,{children:["add",(0,a.jsx)("br",{})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"<configuration>\n<property>\n  <name>yarn.nodemanager.aux-services</name>\n  <value>mapreduce_shuffle</value>\n</property>\n<property>\n  <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n  <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n</property>\n<property>\n  <name>yarn.resourcemanager.hostname</name>\n  <value>127.0.0.1</value>\n</property>\n<property>\n  <name>yarn.acl.enable</name>\n  <value>0</value>\n</property>\n<property>\n  <name>yarn.nodemanager.env-whitelist</name>\n  <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\n</property>\n</configuration>\n"})}),"\n",(0,a.jsx)(n.h3,{id:"format-hdfs-namenode",children:"Format HDFS NameNode"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"hdfs namenode -format\n"})}),"\n",(0,a.jsx)(n.h3,{id:"start-hadoop-cluster",children:"Start Hadoop Cluster"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"start-dfs.sh\n"})}),"\n",(0,a.jsx)(n.p,{children:"Output:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Starting namenodes on [localhost]\nStarting datanodes\nStarting secondary namenodes\n"})}),"\n",(0,a.jsx)(n.h3,{id:"start-yarn",children:"Start YARN"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"start-yarn.sh\n"})}),"\n",(0,a.jsx)(n.p,{children:"Output:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Starting resourcemanager\nStarting nodemanagers\n"})}),"\n",(0,a.jsx)(n.h3,{id:"run-the-following-command-to-check-if-all-the-daemons-are-active-and-running-as-java-processes",children:"Run the following command to check if all the daemons are active and running as Java processes:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"jps\n"})}),"\n",(0,a.jsx)(n.p,{children:"Sample output:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"15584 Jps\n14916 ResourceManager\n15252 NodeManager\n"})}),"\n",(0,a.jsx)(n.h3,{id:"access-hadoop-from-browser",children:"Access Hadoop from Browser"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"http://localhost:9870\nhttp://localhost:9864\nhttp://localhost:8088\n"})}),"\n",(0,a.jsx)(n.h3,{id:"selected-commands",children:"Selected Commands"}),"\n",(0,a.jsx)(n.h4,{id:"hadoop-341sbin",children:"~/hadoop-3.4.1/sbin"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"hadoop-daemon.sh\nhadoop-daemons.sh\nhttpfs.sh\nkms.sh\nmr-jobhistory-daemon.sh\nrefresh-namenodes.sh\nstart-all.cmd\nstart-all.sh\nstart-balancer.sh\nstart-dfs.cmd\nstart-dfs.sh\nstart-secure-dns.sh\nstart-yarn.cmd\nstart-yarn.sh\nstop-all.cmd\nstop-all.sh\nstop-balancer.sh\nstop-dfs.cmd\nstop-dfs.sh\nstop-secure-dns.sh\nstop-yarn.cmd\nstop-yarn.sh\nworkers.sh\nyarn-daemon.sh\nyarn-daemons.sh\n"})}),"\n",(0,a.jsx)(n.h4,{id:"hadoop-341bin",children:"~/hadoop-3.4.1/bin"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"container-executor\nhadoop\nhadoop.cmd\nhdfs\nhdfs.cmd\nmapred\nmapred.cmd\noom-listener\ntest-container-executor\nyarn\nyarn.cmd\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsxs)(n.h3,{id:"after-installing-as-hadoop-user",children:["After Installing ",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"(as hadoop user)"})})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:".\n\u251c\u2500\u2500 hadoop\n\u2502\xa0\xa0 \u251c\u2500\u2500 bin\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 container-executor\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 hadoop\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 hadoop.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 hdfs\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 hdfs.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 mapred\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 mapred.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 oom-listener\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 test-container-executor\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 yarn\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 yarn.cmd\n\u2502\xa0\xa0 \u251c\u2500\u2500 etc\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 hadoop\n\u2502\xa0\xa0 \u2502\xa0\xa0     \u2514\u2500\u2500 shellprofile.d\n\u2502\xa0\xa0 \u251c\u2500\u2500 include\n\u2502\xa0\xa0 \u251c\u2500\u2500 lib\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 native\n\u2502\xa0\xa0 \u2502\xa0\xa0     \u2514\u2500\u2500 examples\n\u2502\xa0\xa0 \u251c\u2500\u2500 libexec\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 shellprofile.d\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 tools\n\u2502\xa0\xa0 \u251c\u2500\u2500 licenses-binary\n\u2502\xa0\xa0 \u251c\u2500\u2500 logs\n\u2502\xa0\xa0 \u251c\u2500\u2500 sbin\n\u2502\xa0\xa0 \u251c\u2500\u2500 sbin\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 distribute-exclude.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 FederationStateStore\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 MySQL\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dropDatabase.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dropStoreProcedures.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dropTables.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dropUser.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 FederationStateStoreDatabase.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 FederationStateStoreStoredProcs.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 FederationStateStoreTables.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 FederationStateStoreUser.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 SQLServer\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 dropDatabase.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 dropStoreProcedures.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 dropTables.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 dropUser.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 FederationStateStoreDatabase.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 FederationStateStoreStoredProcs.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 FederationStateStoreTables.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2502\xa0\xa0     \u2514\u2500\u2500 FederationStateStoreUser.sql\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 hadoop-daemon.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 hadoop-daemons.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 httpfs.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 kms.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 mr-jobhistory-daemon.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 refresh-namenodes.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-all.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-all.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-balancer.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-dfs.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-dfs.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-secure-dns.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-yarn.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start-yarn.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-all.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-all.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-balancer.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-dfs.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-dfs.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-secure-dns.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-yarn.cmd\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 stop-yarn.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 workers.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 yarn-daemon.sh\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 yarn-daemons.sh\n...\n...\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 FederationStateStore\n\u2502\xa0\xa0 \u2502\xa0\xa0     \u251c\u2500\u2500 MySQL\n\u2502\xa0\xa0 \u2502\xa0\xa0     \u2514\u2500\u2500 SQLServer\n\u2502\xa0\xa0 \u2514\u2500\u2500 share\n\u2502\xa0\xa0     \u251c\u2500\u2500 doc\n...\n...\n\u2502\xa0\xa0                 \u2514\u2500\u2500 sleeper\n\u2514\u2500\u2500 hadoopdata\n    \u2514\u2500\u2500 hdfs\n        \u251c\u2500\u2500 datanode\n        \u2502\xa0\xa0 \u2514\u2500\u2500 current\n        \u2502\xa0\xa0     \u2514\u2500\u2500 BP-737539575-192.168.1.19-1741088952847\n        \u2502\xa0\xa0         \u251c\u2500\u2500 current\n        \u2502\xa0\xa0         \u2502\xa0\xa0 \u251c\u2500\u2500 finalized\n        \u2502\xa0\xa0         \u2502\xa0\xa0 \u2514\u2500\u2500 rbw\n        \u2502\xa0\xa0         \u2514\u2500\u2500 tmp\n        \u2514\u2500\u2500 namenode\n            \u2514\u2500\u2500 current\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"sidebar_position-100",children:"sidebar_position: 100"}),"\n",(0,a.jsx)(n.h4,{id:"make-the-hdfs-directories-required-to-execute-mapreduce-jobs",children:"Make the HDFS directories required to execute MapReduce jobs:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"bin/hdfs dfs -mkdir -p /user/<username>\n"})}),"\n",(0,a.jsx)(n.h4,{id:"copy-the-input-files-into-the-distributed-filesystem",children:"Copy the input files into the distributed filesystem:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"bin/hdfs dfs -mkdir input\nbin/hdfs dfs -put etc/hadoop/*.xml input\n"})}),"\n",(0,a.jsx)(n.h4,{id:"run-some-of-the-examples-provided",children:"Run some of the examples provided:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar grep input output 'dfs[a-z.]+'\n"})}),"\n",(0,a.jsx)(n.h4,{id:"examine-the-output-files",children:"Examine the output files:"}),"\n",(0,a.jsx)(n.p,{children:"Copy the output files from the distributed filesystem to the local filesystem and examine them:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"bin/hdfs dfs -get output output $ cat output/*\n"})}),"\n",(0,a.jsx)(n.h4,{id:"view-the-output-files-on-the-distributed-filesystem",children:"View the output files on the distributed filesystem:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"bin/hdfs dfs -cat output/*\n"})}),"\n",(0,a.jsx)(n.h4,{id:"when-youre-done-stop-the-daemons-with",children:"When you\u2019re done, stop the daemons with:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"stop-dfs.sh\n"})})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>d,x:()=>t});var o=s(6540);const a={},r=o.createContext(a);function d(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:d(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);